{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEX2RpwKBmAq"
   },
   "source": [
    "**Mount google drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faamYjbWBkgV"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShKg7xVQBx7s"
   },
   "source": [
    "**Download U2Net repo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0wYh-jhxQwzP",
    "outputId": "72c39ff3-cebb-40f7-c829-491bac886034"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive\n",
      "/content/drive/MyDrive/U-2-Net\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive\n",
    "import os\n",
    "if not os.path.exists(\"/content/drive/MyDrive/U-2-Net/\"):\n",
    "    !git clone https://github.com/xuebinqin/U-2-Net.git\n",
    "\n",
    "%cd /content/drive/MyDrive/U-2-Net/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCpGZj89B35C"
   },
   "source": [
    "**Generate training images from labelled json file and augment the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQZOxTw-CAzD"
   },
   "outputs": [],
   "source": [
    "!python3 generate_training_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70iQ-fwPCGn5"
   },
   "outputs": [],
   "source": [
    "!python3 augment_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VL33AaPEQnD5",
    "outputId": "67f6e16d-8b91-4620-edbb-f06d945e9392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/U-2-Net\n",
      "mkdir: cannot create directory ‘saved_models/u2net/’: File exists\n",
      "/content/drive/MyDrive/U-2-Net/saved_models/u2net\n",
      "'uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ'\n",
      "Downloading...\n",
      "From: https://drive.google.com/u/0/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ\n",
      "To: /content/drive/MyDrive/U-2-Net/saved_models/u2net/u2net.pth\n",
      "100% 176M/176M [00:05<00:00, 32.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/U-2-Net/\n",
    "! mkdir saved_models/u2net/\n",
    "%cd /content/drive/MyDrive/U-2-Net/saved_models/u2net/\n",
    "!gdown https://drive.google.com/u/0/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ&export=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nX7lGLXVCTPE"
   },
   "source": [
    "**Train U2NET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRcc6wfHTTSf",
    "outputId": "82a119b2-6e9f-431d-c268-dc76dc5d5e51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "\n",
      "[epoch: 21518/100000, batch:    24/   30, ite: 172142] train loss: 0.310622, tar: 0.015455 \n",
      "It 172142\n",
      "l0: 0.011394, l1: 0.011396, l2: 0.012401, l3: 0.013156, l4: 0.016660, l5: 0.050542, l6: 0.200185\n",
      "\n",
      "[epoch: 21518/100000, batch:    28/   30, ite: 172143] train loss: 0.310658, tar: 0.015426 \n",
      "It 172143\n",
      "l0: 0.059140, l1: 0.059306, l2: 0.060257, l3: 0.064607, l4: 0.073658, l5: 0.139470, l6: 0.300258\n",
      "\n",
      "[epoch: 21518/100000, batch:    32/   30, ite: 172144] train loss: 0.313755, tar: 0.015730 \n",
      "It 172144\n",
      "l0: 0.007343, l1: 0.007340, l2: 0.008162, l3: 0.009313, l4: 0.011353, l5: 0.023670, l6: 0.121169\n",
      "\n",
      "[epoch: 21519/100000, batch:     4/   30, ite: 172145] train loss: 0.312890, tar: 0.015672 \n",
      "It 172145\n",
      "l0: 0.007669, l1: 0.007667, l2: 0.008376, l3: 0.008836, l4: 0.010336, l5: 0.030820, l6: 0.140726\n",
      "\n",
      "[epoch: 21519/100000, batch:     8/   30, ite: 172146] train loss: 0.312216, tar: 0.015617 \n",
      "It 172146\n",
      "l0: 0.010274, l1: 0.010271, l2: 0.011085, l3: 0.011834, l4: 0.016261, l5: 0.064354, l6: 0.189659\n",
      "\n",
      "[epoch: 21519/100000, batch:    12/   30, ite: 172147] train loss: 0.312226, tar: 0.015581 \n",
      "It 172147\n",
      "l0: 0.036158, l1: 0.036064, l2: 0.037826, l3: 0.040446, l4: 0.045782, l5: 0.094327, l6: 0.227780\n",
      "\n",
      "[epoch: 21519/100000, batch:    16/   30, ite: 172148] train loss: 0.313619, tar: 0.015720 \n",
      "It 172148\n",
      "l0: 0.011428, l1: 0.011426, l2: 0.012384, l3: 0.013327, l4: 0.015909, l5: 0.033626, l6: 0.223580\n",
      "\n",
      "[epoch: 21519/100000, batch:    20/   30, ite: 172149] train loss: 0.313673, tar: 0.015691 \n",
      "It 172149\n",
      "l0: 0.008545, l1: 0.008545, l2: 0.009057, l3: 0.009690, l4: 0.012294, l5: 0.048867, l6: 0.145389\n",
      "\n",
      "[epoch: 21519/100000, batch:    24/   30, ite: 172150] train loss: 0.313198, tar: 0.015644 \n",
      "It 172150\n",
      "l0: 0.035792, l1: 0.035871, l2: 0.037100, l3: 0.038968, l4: 0.046740, l5: 0.074968, l6: 0.183132\n",
      "\n",
      "[epoch: 21519/100000, batch:    28/   30, ite: 172151] train loss: 0.314121, tar: 0.015777 \n",
      "It 172151\n",
      "l0: 0.010610, l1: 0.010613, l2: 0.011596, l3: 0.012307, l4: 0.015655, l5: 0.028910, l6: 0.198538\n",
      "\n",
      "[epoch: 21519/100000, batch:    32/   30, ite: 172152] train loss: 0.313951, tar: 0.015743 \n",
      "It 172152\n",
      "l0: 0.012833, l1: 0.012837, l2: 0.013893, l3: 0.014698, l4: 0.017358, l5: 0.052018, l6: 0.232357\n",
      "\n",
      "[epoch: 21520/100000, batch:     4/   30, ite: 172153] train loss: 0.314225, tar: 0.015724 \n",
      "It 172153\n",
      "l0: 0.007825, l1: 0.007826, l2: 0.008640, l3: 0.009238, l4: 0.011800, l5: 0.025108, l6: 0.129322\n",
      "\n",
      "[epoch: 21520/100000, batch:     8/   30, ite: 172154] train loss: 0.313482, tar: 0.015673 \n",
      "It 172154\n",
      "l0: 0.011089, l1: 0.011090, l2: 0.012023, l3: 0.012844, l4: 0.016838, l5: 0.045245, l6: 0.202003\n",
      "\n",
      "[epoch: 21520/100000, batch:    12/   30, ite: 172155] train loss: 0.313467, tar: 0.015643 \n",
      "It 172155\n",
      "l0: 0.034874, l1: 0.034959, l2: 0.036170, l3: 0.038804, l4: 0.047450, l5: 0.111459, l6: 0.281388\n",
      "\n",
      "[epoch: 21520/100000, batch:    16/   30, ite: 172156] train loss: 0.315208, tar: 0.015766 \n",
      "It 172156\n",
      "l0: 0.011653, l1: 0.011649, l2: 0.012628, l3: 0.013481, l4: 0.018209, l5: 0.051339, l6: 0.210340\n",
      "\n",
      "[epoch: 21520/100000, batch:    20/   30, ite: 172157] train loss: 0.315298, tar: 0.015740 \n",
      "It 172157\n",
      "l0: 0.006939, l1: 0.006935, l2: 0.007620, l3: 0.008142, l4: 0.010245, l5: 0.050210, l6: 0.099408\n",
      "\n",
      "[epoch: 21520/100000, batch:    24/   30, ite: 172158] train loss: 0.314502, tar: 0.015684 \n",
      "It 172158\n",
      "l0: 0.004512, l1: 0.004512, l2: 0.004952, l3: 0.005242, l4: 0.006032, l5: 0.010398, l6: 0.060736\n",
      "\n",
      "[epoch: 21520/100000, batch:    28/   30, ite: 172159] train loss: 0.313130, tar: 0.015614 \n",
      "It 172159\n",
      "l0: 0.058400, l1: 0.058172, l2: 0.060598, l3: 0.066366, l4: 0.070628, l5: 0.121339, l6: 0.236616\n",
      "\n",
      "[epoch: 21520/100000, batch:    32/   30, ite: 172160] train loss: 0.315374, tar: 0.015882 \n",
      "It 172160\n",
      "l0: 0.012233, l1: 0.012226, l2: 0.013236, l3: 0.014436, l4: 0.021494, l5: 0.072501, l6: 0.205657\n",
      "\n",
      "[epoch: 21521/100000, batch:     4/   30, ite: 172161] train loss: 0.315600, tar: 0.015859 \n",
      "It 172161\n",
      "l0: 0.032359, l1: 0.032264, l2: 0.033491, l3: 0.034497, l4: 0.037970, l5: 0.062157, l6: 0.190642\n",
      "\n",
      "[epoch: 21521/100000, batch:     8/   30, ite: 172162] train loss: 0.316265, tar: 0.015961 \n",
      "It 172162\n",
      "l0: 0.010183, l1: 0.010185, l2: 0.011035, l3: 0.011815, l4: 0.015504, l5: 0.060170, l6: 0.181417\n",
      "\n",
      "[epoch: 21521/100000, batch:    12/   30, ite: 172163] train loss: 0.316167, tar: 0.015925 \n",
      "It 172163\n",
      "l0: 0.008490, l1: 0.008492, l2: 0.009440, l3: 0.010229, l4: 0.012749, l5: 0.034847, l6: 0.120028\n",
      "\n",
      "[epoch: 21521/100000, batch:    16/   30, ite: 172164] train loss: 0.315485, tar: 0.015880 \n",
      "It 172164\n",
      "l0: 0.030371, l1: 0.030445, l2: 0.031654, l3: 0.034724, l4: 0.042962, l5: 0.065204, l6: 0.214939\n",
      "\n",
      "[epoch: 21521/100000, batch:    20/   30, ite: 172165] train loss: 0.316302, tar: 0.015968 \n",
      "It 172165\n",
      "l0: 0.009725, l1: 0.009731, l2: 0.010562, l3: 0.011340, l4: 0.013832, l5: 0.037642, l6: 0.200076\n",
      "\n",
      "[epoch: 21521/100000, batch:    24/   30, ite: 172166] train loss: 0.316161, tar: 0.015930 \n",
      "It 172166\n",
      "l0: 0.006675, l1: 0.006678, l2: 0.007411, l3: 0.007936, l4: 0.010278, l5: 0.023939, l6: 0.101372\n",
      "\n",
      "[epoch: 21521/100000, batch:    28/   30, ite: 172167] train loss: 0.315252, tar: 0.015875 \n",
      "It 172167\n",
      "l0: 0.009373, l1: 0.009373, l2: 0.010037, l3: 0.010519, l4: 0.012430, l5: 0.048541, l6: 0.153943\n",
      "\n",
      "[epoch: 21521/100000, batch:    32/   30, ite: 172168] train loss: 0.314888, tar: 0.015836 \n",
      "It 172168\n",
      "l0: 0.029653, l1: 0.029739, l2: 0.030868, l3: 0.033959, l4: 0.037059, l5: 0.068562, l6: 0.176467\n",
      "\n",
      "[epoch: 21522/100000, batch:     4/   30, ite: 172169] train loss: 0.315429, tar: 0.015918 \n",
      "It 172169\n",
      "l0: 0.007500, l1: 0.007497, l2: 0.008049, l3: 0.008442, l4: 0.010329, l5: 0.031784, l6: 0.111149\n",
      "\n",
      "[epoch: 21522/100000, batch:     8/   30, ite: 172170] train loss: 0.314661, tar: 0.015868 \n",
      "It 172170\n",
      "l0: 0.007790, l1: 0.007786, l2: 0.008465, l3: 0.008937, l4: 0.010641, l5: 0.025026, l6: 0.131206\n",
      "\n",
      "[epoch: 21522/100000, batch:    12/   30, ite: 172171] train loss: 0.313989, tar: 0.015821 \n",
      "It 172171\n",
      "l0: 0.012544, l1: 0.012535, l2: 0.013749, l3: 0.014772, l4: 0.018160, l5: 0.039673, l6: 0.213038\n",
      "\n",
      "[epoch: 21522/100000, batch:    16/   30, ite: 172172] train loss: 0.314050, tar: 0.015802 \n",
      "It 172172\n",
      "l0: 0.011988, l1: 0.011982, l2: 0.012975, l3: 0.014086, l4: 0.019044, l5: 0.096844, l6: 0.222669\n",
      "\n",
      "[epoch: 21522/100000, batch:    20/   30, ite: 172173] train loss: 0.314487, tar: 0.015780 \n",
      "It 172173\n",
      "l0: 0.008842, l1: 0.008838, l2: 0.009683, l3: 0.010376, l4: 0.013444, l5: 0.038523, l6: 0.118662\n",
      "\n",
      "[epoch: 21522/100000, batch:    24/   30, ite: 172174] train loss: 0.313877, tar: 0.015740 \n",
      "It 172174\n",
      "l0: 0.030752, l1: 0.030641, l2: 0.031994, l3: 0.033551, l4: 0.041238, l5: 0.092128, l6: 0.204897\n",
      "\n",
      "[epoch: 21522/100000, batch:    28/   30, ite: 172175] train loss: 0.314742, tar: 0.015826 \n",
      "It 172175\n",
      "l0: 0.010781, l1: 0.010782, l2: 0.011620, l3: 0.012531, l4: 0.017083, l5: 0.069952, l6: 0.192251\n",
      "\n",
      "[epoch: 21522/100000, batch:    32/   30, ite: 172176] train loss: 0.314800, tar: 0.015797 \n",
      "It 172176\n",
      "l0: 0.010756, l1: 0.010757, l2: 0.011583, l3: 0.012224, l4: 0.015999, l5: 0.083837, l6: 0.196889\n",
      "\n",
      "[epoch: 21523/100000, batch:     4/   30, ite: 172177] train loss: 0.314954, tar: 0.015769 \n",
      "It 172177\n",
      "l0: 0.009028, l1: 0.009032, l2: 0.009825, l3: 0.010496, l4: 0.014016, l5: 0.055454, l6: 0.130824\n",
      "\n",
      "[epoch: 21523/100000, batch:     8/   30, ite: 172178] train loss: 0.314525, tar: 0.015731 \n",
      "It 172178\n",
      "l0: 0.007088, l1: 0.007088, l2: 0.007683, l3: 0.008028, l4: 0.009796, l5: 0.029898, l6: 0.106492\n",
      "\n",
      "[epoch: 21523/100000, batch:    12/   30, ite: 172179] train loss: 0.313752, tar: 0.015683 \n",
      "It 172179\n",
      "l0: 0.036851, l1: 0.036959, l2: 0.038264, l3: 0.041362, l4: 0.048178, l5: 0.103925, l6: 0.285592\n",
      "\n",
      "[epoch: 21523/100000, batch:    16/   30, ite: 172180] train loss: 0.315293, tar: 0.015800 \n",
      "It 172180\n",
      "l0: 0.008269, l1: 0.008272, l2: 0.008943, l3: 0.009488, l4: 0.011801, l5: 0.033295, l6: 0.163819\n",
      "\n",
      "[epoch: 21523/100000, batch:    20/   30, ite: 172181] train loss: 0.314898, tar: 0.015759 \n",
      "It 172181\n",
      "l0: 0.007859, l1: 0.007861, l2: 0.008731, l3: 0.009197, l4: 0.010522, l5: 0.028878, l6: 0.101042\n",
      "\n",
      "[epoch: 21523/100000, batch:    24/   30, ite: 172182] train loss: 0.314125, tar: 0.015715 \n",
      "It 172182\n",
      "l0: 0.007890, l1: 0.007889, l2: 0.008591, l3: 0.009101, l4: 0.013545, l5: 0.022811, l6: 0.119362\n",
      "\n",
      "[epoch: 21523/100000, batch:    28/   30, ite: 172183] train loss: 0.313442, tar: 0.015672 \n",
      "It 172183\n",
      "l0: 0.068441, l1: 0.068198, l2: 0.070514, l3: 0.074787, l4: 0.088286, l5: 0.132406, l6: 0.373893\n",
      "\n",
      "[epoch: 21523/100000, batch:    32/   30, ite: 172184] train loss: 0.316502, tar: 0.015959 \n",
      "It 172184\n",
      "l0: 0.031567, l1: 0.031647, l2: 0.032952, l3: 0.035988, l4: 0.042133, l5: 0.059306, l6: 0.137333\n",
      "\n",
      "[epoch: 21524/100000, batch:     4/   30, ite: 172185] train loss: 0.316796, tar: 0.016044 \n",
      "It 172185\n",
      "l0: 0.010951, l1: 0.010948, l2: 0.011918, l3: 0.012731, l4: 0.017217, l5: 0.072500, l6: 0.177294\n",
      "\n",
      "[epoch: 21524/100000, batch:     8/   30, ite: 172186] train loss: 0.316779, tar: 0.016016 \n",
      "It 172186\n",
      "l0: 0.011199, l1: 0.011196, l2: 0.012116, l3: 0.012897, l4: 0.016346, l5: 0.051355, l6: 0.219767\n",
      "\n",
      "[epoch: 21524/100000, batch:    12/   30, ite: 172187] train loss: 0.316876, tar: 0.015990 \n",
      "It 172187\n",
      "l0: 0.006975, l1: 0.006974, l2: 0.007609, l3: 0.008068, l4: 0.010012, l5: 0.018193, l6: 0.081616\n",
      "\n",
      "[epoch: 21524/100000, batch:    16/   30, ite: 172188] train loss: 0.315932, tar: 0.015943 \n",
      "It 172188\n",
      "l0: 0.037219, l1: 0.037106, l2: 0.038507, l3: 0.039549, l4: 0.045765, l5: 0.122992, l6: 0.312651\n",
      "\n",
      "[epoch: 21524/100000, batch:    20/   30, ite: 172189] train loss: 0.317614, tar: 0.016055 \n",
      "It 172189\n",
      "l0: 0.010899, l1: 0.010896, l2: 0.011731, l3: 0.012452, l4: 0.015787, l5: 0.052623, l6: 0.209563\n",
      "\n",
      "[epoch: 21524/100000, batch:    24/   30, ite: 172190] train loss: 0.317647, tar: 0.016028 \n",
      "It 172190\n",
      "l0: 0.009290, l1: 0.009291, l2: 0.010133, l3: 0.010752, l4: 0.012440, l5: 0.049170, l6: 0.148033\n",
      "\n",
      "[epoch: 21524/100000, batch:    28/   30, ite: 172191] train loss: 0.317288, tar: 0.015993 \n",
      "It 172191\n",
      "l0: 0.007163, l1: 0.007161, l2: 0.008128, l3: 0.008917, l4: 0.010594, l5: 0.019026, l6: 0.059727\n",
      "\n",
      "[epoch: 21524/100000, batch:    32/   30, ite: 172192] train loss: 0.316264, tar: 0.015947 \n",
      "It 172192\n",
      "l0: 0.013213, l1: 0.013215, l2: 0.014363, l3: 0.015538, l4: 0.020668, l5: 0.094250, l6: 0.209190\n",
      "\n",
      "[epoch: 21525/100000, batch:     4/   30, ite: 172193] train loss: 0.316597, tar: 0.015933 \n",
      "It 172193\n",
      "l0: 0.033541, l1: 0.033611, l2: 0.034967, l3: 0.038110, l4: 0.044272, l5: 0.077821, l6: 0.213027\n",
      "\n",
      "[epoch: 21525/100000, batch:     8/   30, ite: 172194] train loss: 0.317415, tar: 0.016023 \n",
      "It 172194\n",
      "l0: 0.008541, l1: 0.008538, l2: 0.009364, l3: 0.009841, l4: 0.011365, l5: 0.036505, l6: 0.122548\n",
      "\n",
      "[epoch: 21525/100000, batch:    12/   30, ite: 172195] train loss: 0.316847, tar: 0.015985 \n",
      "It 172195\n",
      "l0: 0.006259, l1: 0.006260, l2: 0.006869, l3: 0.007263, l4: 0.009025, l5: 0.017561, l6: 0.104954\n",
      "\n",
      "[epoch: 21525/100000, batch:    16/   30, ite: 172196] train loss: 0.316038, tar: 0.015935 \n",
      "It 172196\n",
      "l0: 0.036785, l1: 0.036693, l2: 0.037548, l3: 0.038039, l4: 0.042430, l5: 0.112094, l6: 0.276439\n",
      "\n",
      "[epoch: 21525/100000, batch:    20/   30, ite: 172197] train loss: 0.317378, tar: 0.016041 \n",
      "It 172197\n",
      "l0: 0.004818, l1: 0.004816, l2: 0.005435, l3: 0.005724, l4: 0.006613, l5: 0.009358, l6: 0.034963\n",
      "\n",
      "[epoch: 21525/100000, batch:    24/   30, ite: 172198] train loss: 0.316137, tar: 0.015984 \n",
      "It 172198\n",
      "l0: 0.014247, l1: 0.014244, l2: 0.015353, l3: 0.016329, l4: 0.020358, l5: 0.073726, l6: 0.263137\n",
      "\n",
      "[epoch: 21525/100000, batch:    28/   30, ite: 172199] train loss: 0.316646, tar: 0.015976 \n",
      "It 172199\n",
      "l0: 0.006478, l1: 0.006476, l2: 0.007129, l3: 0.007751, l4: 0.010416, l5: 0.029730, l6: 0.108594\n",
      "\n",
      "[epoch: 21525/100000, batch:    32/   30, ite: 172200] train loss: 0.315946, tar: 0.015928 \n",
      "It 172200\n",
      "l0: 0.031850, l1: 0.031914, l2: 0.033070, l3: 0.035942, l4: 0.041798, l5: 0.083563, l6: 0.180418\n",
      "\n",
      "[epoch: 21526/100000, batch:     4/   30, ite: 172201] train loss: 0.316556, tar: 0.016007 \n",
      "It 172201\n",
      "l0: 0.029666, l1: 0.029594, l2: 0.030477, l3: 0.032173, l4: 0.038949, l5: 0.083618, l6: 0.202554\n",
      "\n",
      "[epoch: 21526/100000, batch:     8/   30, ite: 172202] train loss: 0.317202, tar: 0.016075 \n",
      "It 172202\n",
      "l0: 0.012671, l1: 0.012670, l2: 0.013686, l3: 0.014657, l4: 0.017820, l5: 0.033894, l6: 0.233469\n",
      "\n",
      "[epoch: 21526/100000, batch:    12/   30, ite: 172203] train loss: 0.317308, tar: 0.016058 \n",
      "It 172203\n",
      "l0: 0.009878, l1: 0.009878, l2: 0.010705, l3: 0.011215, l4: 0.013696, l5: 0.056314, l6: 0.173469\n",
      "\n",
      "[epoch: 21526/100000, batch:    16/   30, ite: 172204] train loss: 0.317151, tar: 0.016028 \n",
      "It 172204\n",
      "l0: 0.011883, l1: 0.011885, l2: 0.012813, l3: 0.013542, l4: 0.018744, l5: 0.057244, l6: 0.212309\n",
      "\n",
      "[epoch: 21526/100000, batch:    20/   30, ite: 172205] train loss: 0.317255, tar: 0.016008 \n",
      "It 172205\n",
      "l0: 0.007780, l1: 0.007778, l2: 0.008588, l3: 0.009159, l4: 0.012152, l5: 0.031724, l6: 0.123817\n",
      "\n",
      "[epoch: 21526/100000, batch:    24/   30, ite: 172206] train loss: 0.316690, tar: 0.015968 \n",
      "It 172206\n",
      "l0: 0.008824, l1: 0.008824, l2: 0.009750, l3: 0.010310, l4: 0.012409, l5: 0.034630, l6: 0.136346\n",
      "\n",
      "[epoch: 21526/100000, batch:    28/   30, ite: 172207] train loss: 0.316228, tar: 0.015933 \n",
      "It 172207\n",
      "l0: 0.004334, l1: 0.004335, l2: 0.004881, l3: 0.005157, l4: 0.006805, l5: 0.010178, l6: 0.043152\n",
      "\n",
      "[epoch: 21526/100000, batch:    32/   30, ite: 172208] train loss: 0.315087, tar: 0.015878 \n",
      "It 172208\n",
      "l0: 0.038134, l1: 0.038152, l2: 0.039846, l3: 0.044040, l4: 0.052957, l5: 0.128690, l6: 0.322833\n",
      "\n",
      "[epoch: 21527/100000, batch:     4/   30, ite: 172209] train loss: 0.316760, tar: 0.015984 \n",
      "It 172209\n",
      "l0: 0.008327, l1: 0.008327, l2: 0.009194, l3: 0.009777, l4: 0.011970, l5: 0.032701, l6: 0.138122\n",
      "\n",
      "[epoch: 21527/100000, batch:     8/   30, ite: 172210] train loss: 0.316291, tar: 0.015948 \n",
      "It 172210\n",
      "l0: 0.006243, l1: 0.006242, l2: 0.006800, l3: 0.007392, l4: 0.008793, l5: 0.018628, l6: 0.076633\n",
      "\n",
      "[epoch: 21527/100000, batch:    12/   30, ite: 172211] train loss: 0.315412, tar: 0.015902 \n",
      "It 172211\n",
      "l0: 0.010064, l1: 0.010063, l2: 0.010927, l3: 0.011748, l4: 0.015862, l5: 0.054448, l6: 0.190193\n",
      "\n",
      "[epoch: 21527/100000, batch:    16/   30, ite: 172212] train loss: 0.315355, tar: 0.015874 \n",
      "It 172212\n",
      "l0: 0.008868, l1: 0.008869, l2: 0.009602, l3: 0.010207, l4: 0.013446, l5: 0.040144, l6: 0.161811\n",
      "\n",
      "[epoch: 21527/100000, batch:    20/   30, ite: 172213] train loss: 0.315062, tar: 0.015841 \n",
      "It 172213\n",
      "l0: 0.007812, l1: 0.007809, l2: 0.008659, l3: 0.009209, l4: 0.011112, l5: 0.021046, l6: 0.123322\n",
      "\n",
      "[epoch: 21527/100000, batch:    24/   30, ite: 172214] train loss: 0.314473, tar: 0.015804 \n",
      "It 172214\n",
      "l0: 0.027711, l1: 0.027670, l2: 0.028640, l3: 0.030483, l4: 0.033943, l5: 0.051357, l6: 0.134285\n",
      "\n",
      "[epoch: 21527/100000, batch:    28/   30, ite: 172215] train loss: 0.314564, tar: 0.015859 \n",
      "It 172215\n",
      "l0: 0.013706, l1: 0.013710, l2: 0.014745, l3: 0.015853, l4: 0.022998, l5: 0.110912, l6: 0.238295\n",
      "\n",
      "[epoch: 21527/100000, batch:    32/   30, ite: 172216] train loss: 0.315099, tar: 0.015849 \n",
      "It 172216\n",
      "l0: 0.038046, l1: 0.038101, l2: 0.039802, l3: 0.042390, l4: 0.051186, l5: 0.148312, l6: 0.341312\n",
      "\n",
      "[epoch: 21528/100000, batch:     4/   30, ite: 172217] train loss: 0.316869, tar: 0.015951 \n",
      "It 172217\n",
      "l0: 0.009568, l1: 0.009567, l2: 0.010409, l3: 0.010968, l4: 0.013448, l5: 0.039328, l6: 0.173581\n",
      "\n",
      "[epoch: 21528/100000, batch:     8/   30, ite: 172218] train loss: 0.316640, tar: 0.015922 \n",
      "It 172218\n",
      "l0: 0.010056, l1: 0.010057, l2: 0.010815, l3: 0.011600, l4: 0.016028, l5: 0.046982, l6: 0.196369\n",
      "\n",
      "[epoch: 21528/100000, batch:    12/   30, ite: 172219] train loss: 0.316573, tar: 0.015895 \n",
      "It 172219\n",
      "l0: 0.005460, l1: 0.005460, l2: 0.006129, l3: 0.006499, l4: 0.007530, l5: 0.011429, l6: 0.054678\n",
      "\n",
      "[epoch: 21528/100000, batch:    16/   30, ite: 172220] train loss: 0.315575, tar: 0.015848 \n",
      "It 172220\n",
      "l0: 0.008974, l1: 0.008971, l2: 0.009848, l3: 0.010391, l4: 0.012804, l5: 0.043777, l6: 0.128685\n",
      "\n",
      "[epoch: 21528/100000, batch:    20/   30, ite: 172221] train loss: 0.315158, tar: 0.015817 \n",
      "It 172221\n",
      "l0: 0.009618, l1: 0.009617, l2: 0.010563, l3: 0.011411, l4: 0.014155, l5: 0.028869, l6: 0.121431\n",
      "\n",
      "[epoch: 21528/100000, batch:    24/   30, ite: 172222] train loss: 0.314665, tar: 0.015789 \n",
      "It 172222\n",
      "l0: 0.031603, l1: 0.031548, l2: 0.032615, l3: 0.034318, l4: 0.037655, l5: 0.075640, l6: 0.175132\n",
      "\n",
      "[epoch: 21528/100000, batch:    28/   30, ite: 172223] train loss: 0.315131, tar: 0.015860 \n",
      "It 172223\n",
      "l0: 0.011302, l1: 0.011302, l2: 0.012350, l3: 0.013100, l4: 0.015973, l5: 0.037455, l6: 0.207843\n",
      "\n",
      "[epoch: 21528/100000, batch:    32/   30, ite: 172224] train loss: 0.315105, tar: 0.015839 \n",
      "It 172224\n",
      "l0: 0.013962, l1: 0.013958, l2: 0.015007, l3: 0.016023, l4: 0.020753, l5: 0.072991, l6: 0.233420\n",
      "\n",
      "[epoch: 21529/100000, batch:     4/   30, ite: 172225] train loss: 0.315421, tar: 0.015831 \n",
      "It 172225\n",
      "l0: 0.011250, l1: 0.011249, l2: 0.012197, l3: 0.012955, l4: 0.016799, l5: 0.084373, l6: 0.191046\n",
      "\n",
      "[epoch: 21529/100000, batch:     8/   30, ite: 172226] train loss: 0.315529, tar: 0.015811 \n",
      "It 172226\n",
      "l0: 0.029573, l1: 0.029616, l2: 0.030857, l3: 0.033558, l4: 0.038732, l5: 0.073702, l6: 0.190887\n",
      "\n",
      "[epoch: 21529/100000, batch:    12/   30, ite: 172227] train loss: 0.316020, tar: 0.015871 \n",
      "It 172227\n",
      "l0: 0.008182, l1: 0.008183, l2: 0.008966, l3: 0.009594, l4: 0.012034, l5: 0.036463, l6: 0.124358\n",
      "\n",
      "[epoch: 21529/100000, batch:    16/   30, ite: 172228] train loss: 0.315545, tar: 0.015838 \n",
      "It 172228\n",
      "l0: 0.007896, l1: 0.007901, l2: 0.008689, l3: 0.009478, l4: 0.012320, l5: 0.037978, l6: 0.109107\n",
      "\n",
      "[epoch: 21529/100000, batch:    20/   30, ite: 172229] train loss: 0.315011, tar: 0.015803 \n",
      "It 172229\n",
      "l0: 0.010332, l1: 0.010332, l2: 0.011162, l3: 0.011989, l4: 0.014446, l5: 0.031696, l6: 0.187503\n",
      "\n",
      "[epoch: 21529/100000, batch:    24/   30, ite: 172230] train loss: 0.314848, tar: 0.015779 \n",
      "It 172230\n",
      "l0: 0.030168, l1: 0.030115, l2: 0.031417, l3: 0.032563, l4: 0.037407, l5: 0.063740, l6: 0.219923\n",
      "\n",
      "[epoch: 21529/100000, batch:    28/   30, ite: 172231] train loss: 0.315413, tar: 0.015842 \n",
      "It 172231\n",
      "l0: 0.003569, l1: 0.003570, l2: 0.004053, l3: 0.004269, l4: 0.004844, l5: 0.007087, l6: 0.015903\n",
      "\n",
      "[epoch: 21529/100000, batch:    32/   30, ite: 172232] train loss: 0.314240, tar: 0.015789 \n",
      "It 172232\n",
      "l0: 0.027821, l1: 0.027865, l2: 0.029280, l3: 0.032157, l4: 0.038209, l5: 0.057383, l6: 0.152693\n",
      "\n",
      "[epoch: 21530/100000, batch:     4/   30, ite: 172233] train loss: 0.314460, tar: 0.015840 \n",
      "It 172233\n",
      "l0: 0.014805, l1: 0.014806, l2: 0.016018, l3: 0.017021, l4: 0.022452, l5: 0.089423, l6: 0.257180\n",
      "\n",
      "[epoch: 21530/100000, batch:     8/   30, ite: 172234] train loss: 0.314961, tar: 0.015836 \n",
      "It 172234\n",
      "l0: 0.012512, l1: 0.012509, l2: 0.013409, l3: 0.014121, l4: 0.018519, l5: 0.083914, l6: 0.221837\n",
      "\n",
      "[epoch: 21530/100000, batch:    12/   30, ite: 172235] train loss: 0.315224, tar: 0.015822 \n",
      "It 172235\n",
      "l0: 0.007876, l1: 0.007875, l2: 0.008555, l3: 0.009011, l4: 0.010443, l5: 0.024156, l6: 0.153220\n",
      "\n",
      "[epoch: 21530/100000, batch:    16/   30, ite: 172236] train loss: 0.314825, tar: 0.015788 \n",
      "It 172236\n",
      "l0: 0.013578, l1: 0.013579, l2: 0.014632, l3: 0.015641, l4: 0.020340, l5: 0.061031, l6: 0.270711\n",
      "\n",
      "[epoch: 21530/100000, batch:    20/   30, ite: 172237] train loss: 0.315225, tar: 0.015779 \n",
      "It 172237\n",
      "l0: 0.003345, l1: 0.003345, l2: 0.003778, l3: 0.004020, l4: 0.004834, l5: 0.008672, l6: 0.032880\n",
      "\n",
      "[epoch: 21530/100000, batch:    24/   30, ite: 172238] train loss: 0.314156, tar: 0.015726 \n",
      "It 172238\n",
      "l0: 0.033091, l1: 0.033030, l2: 0.034333, l3: 0.036548, l4: 0.042935, l5: 0.066844, l6: 0.163293\n",
      "\n",
      "[epoch: 21530/100000, batch:    28/   30, ite: 172239] train loss: 0.314557, tar: 0.015799 \n",
      "It 172239\n",
      "l0: 0.008042, l1: 0.008040, l2: 0.008882, l3: 0.009527, l4: 0.013219, l5: 0.040040, l6: 0.095344\n",
      "\n",
      "[epoch: 21530/100000, batch:    32/   30, ite: 172240] train loss: 0.314010, tar: 0.015767 \n",
      "It 172240\n",
      "l0: 0.006819, l1: 0.006820, l2: 0.007483, l3: 0.008061, l4: 0.010097, l5: 0.017705, l6: 0.111829\n",
      "\n",
      "[epoch: 21531/100000, batch:     4/   30, ite: 172241] train loss: 0.313407, tar: 0.015730 \n",
      "It 172241\n",
      "l0: 0.006709, l1: 0.006708, l2: 0.007262, l3: 0.007683, l4: 0.009922, l5: 0.060490, l6: 0.109855\n",
      "\n",
      "[epoch: 21531/100000, batch:     8/   30, ite: 172242] train loss: 0.312974, tar: 0.015692 \n",
      "It 172242\n",
      "l0: 0.059355, l1: 0.059366, l2: 0.061264, l3: 0.065587, l4: 0.075876, l5: 0.144435, l6: 0.329163\n",
      "\n",
      "[epoch: 21531/100000, batch:    12/   30, ite: 172243] train loss: 0.314958, tar: 0.015872 \n",
      "It 172243\n",
      "l0: 0.010645, l1: 0.010645, l2: 0.011734, l3: 0.012663, l4: 0.015115, l5: 0.033328, l6: 0.189734\n",
      "\n",
      "[epoch: 21531/100000, batch:    16/   30, ite: 172244] train loss: 0.314830, tar: 0.015851 \n",
      "It 172244\n",
      "l0: 0.008939, l1: 0.008939, l2: 0.009820, l3: 0.010277, l4: 0.012170, l5: 0.028103, l6: 0.144287\n",
      "\n",
      "[epoch: 21531/100000, batch:    20/   30, ite: 172245] train loss: 0.314454, tar: 0.015822 \n",
      "It 172245\n",
      "l0: 0.005629, l1: 0.005630, l2: 0.006333, l3: 0.006709, l4: 0.007942, l5: 0.012873, l6: 0.062842\n",
      "\n",
      "[epoch: 21531/100000, batch:    24/   30, ite: 172246] train loss: 0.313614, tar: 0.015781 \n",
      "It 172246\n",
      "l0: 0.014495, l1: 0.014496, l2: 0.015661, l3: 0.016566, l4: 0.019740, l5: 0.075799, l6: 0.253084\n",
      "\n",
      "[epoch: 21531/100000, batch:    28/   30, ite: 172247] train loss: 0.314004, tar: 0.015776 \n",
      "It 172247\n",
      "l0: 0.009483, l1: 0.009484, l2: 0.010162, l3: 0.010773, l4: 0.016166, l5: 0.048233, l6: 0.153523\n",
      "\n",
      "[epoch: 21531/100000, batch:    32/   30, ite: 172248] train loss: 0.313777, tar: 0.015750 \n",
      "It 172248\n",
      "l0: 0.013584, l1: 0.013583, l2: 0.014589, l3: 0.015442, l4: 0.020031, l5: 0.098965, l6: 0.253170\n",
      "\n",
      "[epoch: 21532/100000, batch:     4/   30, ite: 172249] train loss: 0.314242, tar: 0.015742 \n",
      "It 172249\n",
      "l0: 0.008304, l1: 0.008304, l2: 0.009032, l3: 0.009464, l4: 0.013912, l5: 0.032631, l6: 0.121964\n",
      "\n",
      "[epoch: 21532/100000, batch:     8/   30, ite: 172250] train loss: 0.313799, tar: 0.015712 \n",
      "It 172250\n",
      "l0: 0.009981, l1: 0.009978, l2: 0.010754, l3: 0.011306, l4: 0.013945, l5: 0.057662, l6: 0.169190\n",
      "\n",
      "[epoch: 21532/100000, batch:    12/   30, ite: 172251] train loss: 0.313676, tar: 0.015689 \n",
      "It 172251\n",
      "l0: 0.034098, l1: 0.034166, l2: 0.035728, l3: 0.039293, l4: 0.044832, l5: 0.067155, l6: 0.169678\n",
      "\n",
      "[epoch: 21532/100000, batch:    16/   30, ite: 172252] train loss: 0.314117, tar: 0.015762 \n",
      "It 172252\n",
      "l0: 0.030582, l1: 0.030553, l2: 0.031529, l3: 0.032518, l4: 0.035244, l5: 0.072316, l6: 0.211120\n",
      "\n",
      "[epoch: 21532/100000, batch:    20/   30, ite: 172253] train loss: 0.314630, tar: 0.015821 \n",
      "It 172253\n",
      "l0: 0.010554, l1: 0.010554, l2: 0.011487, l3: 0.012327, l4: 0.015930, l5: 0.047155, l6: 0.184032\n",
      "\n",
      "[epoch: 21532/100000, batch:    24/   30, ite: 172254] train loss: 0.314541, tar: 0.015800 \n",
      "It 172254\n",
      "l0: 0.007951, l1: 0.007951, l2: 0.008856, l3: 0.009524, l4: 0.011591, l5: 0.027279, l6: 0.114825\n",
      "\n",
      "[epoch: 21532/100000, batch:    28/   30, ite: 172255] train loss: 0.314045, tar: 0.015769 \n",
      "It 172255\n",
      "l0: 0.008983, l1: 0.008981, l2: 0.009670, l3: 0.010368, l4: 0.012403, l5: 0.037032, l6: 0.176712\n",
      "\n",
      "[epoch: 21532/100000, batch:    32/   30, ite: 172256] train loss: 0.313850, tar: 0.015743 \n",
      "It 172256\n",
      "l0: 0.036418, l1: 0.036452, l2: 0.038275, l3: 0.040543, l4: 0.046481, l5: 0.076747, l6: 0.226966\n",
      "\n",
      "[epoch: 21533/100000, batch:     4/   30, ite: 172257] train loss: 0.314582, tar: 0.015823 \n",
      "It 172257\n",
      "l0: 0.014021, l1: 0.014020, l2: 0.014925, l3: 0.015653, l4: 0.020470, l5: 0.122425, l6: 0.263585\n",
      "\n",
      "[epoch: 21533/100000, batch:     8/   30, ite: 172258] train loss: 0.315165, tar: 0.015816 \n",
      "It 172258\n",
      "l0: 0.007329, l1: 0.007330, l2: 0.008153, l3: 0.008779, l4: 0.010451, l5: 0.023275, l6: 0.092725\n",
      "\n",
      "[epoch: 21533/100000, batch:    12/   30, ite: 172259] train loss: 0.314558, tar: 0.015783 \n",
      "It 172259\n",
      "l0: 0.012368, l1: 0.012369, l2: 0.013346, l3: 0.014100, l4: 0.018205, l5: 0.072394, l6: 0.210342\n",
      "\n",
      "[epoch: 21533/100000, batch:    16/   30, ite: 172260] train loss: 0.314707, tar: 0.015770 \n",
      "It 172260\n",
      "l0: 0.011816, l1: 0.011815, l2: 0.012681, l3: 0.013520, l4: 0.016853, l5: 0.058290, l6: 0.214702\n",
      "\n",
      "[epoch: 21533/100000, batch:    20/   30, ite: 172261] train loss: 0.314802, tar: 0.015755 \n",
      "It 172261\n",
      "l0: 0.031926, l1: 0.031859, l2: 0.032816, l3: 0.034907, l4: 0.038078, l5: 0.079299, l6: 0.181622\n",
      "\n",
      "[epoch: 21533/100000, batch:    24/   30, ite: 172262] train loss: 0.315244, tar: 0.015817 \n",
      "It 172262\n",
      "l0: 0.005484, l1: 0.005486, l2: 0.006054, l3: 0.006363, l4: 0.007090, l5: 0.013564, l6: 0.082689\n",
      "\n",
      "[epoch: 21533/100000, batch:    28/   30, ite: 172263] train loss: 0.314527, tar: 0.015778 \n",
      "It 172263\n",
      "l0: 0.002386, l1: 0.002386, l2: 0.002767, l3: 0.002901, l4: 0.003329, l5: 0.004044, l6: 0.009583\n",
      "\n",
      "[epoch: 21533/100000, batch:    32/   30, ite: 172264] train loss: 0.313439, tar: 0.015727 \n",
      "It 172264\n",
      "l0: 0.008805, l1: 0.008807, l2: 0.009663, l3: 0.010417, l4: 0.013987, l5: 0.044409, l6: 0.156886\n",
      "\n",
      "[epoch: 21534/100000, batch:     4/   30, ite: 172265] train loss: 0.313211, tar: 0.015701 \n",
      "It 172265\n",
      "l0: 0.032638, l1: 0.032586, l2: 0.033861, l3: 0.035996, l4: 0.042016, l5: 0.078840, l6: 0.206797\n",
      "\n",
      "[epoch: 21534/100000, batch:     8/   30, ite: 172266] train loss: 0.313773, tar: 0.015764 \n",
      "It 172266\n",
      "l0: 0.004798, l1: 0.004798, l2: 0.005436, l3: 0.006001, l4: 0.007621, l5: 0.014526, l6: 0.044622\n",
      "\n",
      "[epoch: 21534/100000, batch:    12/   30, ite: 172267] train loss: 0.312927, tar: 0.015723 \n",
      "It 172267\n",
      "l0: 0.012671, l1: 0.012670, l2: 0.013653, l3: 0.014628, l4: 0.018254, l5: 0.074432, l6: 0.232932\n",
      "\n",
      "[epoch: 21534/100000, batch:    16/   30, ite: 172268] train loss: 0.313175, tar: 0.015712 \n",
      "It 172268\n",
      "l0: 0.036977, l1: 0.036995, l2: 0.038531, l3: 0.040968, l4: 0.046980, l5: 0.097164, l6: 0.239145\n",
      "\n",
      "[epoch: 21534/100000, batch:    20/   30, ite: 172269] train loss: 0.314006, tar: 0.015791 \n",
      "It 172269\n",
      "l0: 0.007227, l1: 0.007227, l2: 0.007839, l3: 0.008307, l4: 0.010616, l5: 0.036172, l6: 0.104186\n",
      "\n",
      "[epoch: 21534/100000, batch:    24/   30, ite: 172270] train loss: 0.313515, tar: 0.015759 \n",
      "It 172270\n",
      "l0: 0.009194, l1: 0.009193, l2: 0.009978, l3: 0.010645, l4: 0.015007, l5: 0.054923, l6: 0.145846\n",
      "\n",
      "[epoch: 21534/100000, batch:    28/   30, ite: 172271] train loss: 0.313298, tar: 0.015735 \n",
      "It 172271\n",
      "l0: 0.013984, l1: 0.013980, l2: 0.014795, l3: 0.015417, l4: 0.017835, l5: 0.058010, l6: 0.305501\n",
      "\n",
      "[epoch: 21534/100000, batch:    32/   30, ite: 172272] train loss: 0.313763, tar: 0.015729 \n",
      "It 172272\n",
      "l0: 0.031643, l1: 0.031599, l2: 0.032764, l3: 0.034936, l4: 0.040218, l5: 0.089258, l6: 0.209429\n",
      "\n",
      "[epoch: 21535/100000, batch:     4/   30, ite: 172273] train loss: 0.314334, tar: 0.015787 \n",
      "It 172273\n",
      "l0: 0.008125, l1: 0.008123, l2: 0.008956, l3: 0.009763, l4: 0.012815, l5: 0.021396, l6: 0.127672\n",
      "\n",
      "[epoch: 21535/100000, batch:     8/   30, ite: 172274] train loss: 0.313906, tar: 0.015759 \n",
      "It 172274\n",
      "l0: 0.028498, l1: 0.028507, l2: 0.029922, l3: 0.032498, l4: 0.036701, l5: 0.063768, l6: 0.122961\n",
      "\n",
      "[epoch: 21535/100000, batch:    12/   30, ite: 172275] train loss: 0.314011, tar: 0.015805 \n",
      "It 172275\n",
      "l0: 0.008839, l1: 0.008839, l2: 0.009619, l3: 0.010253, l4: 0.012960, l5: 0.037381, l6: 0.143204\n",
      "\n",
      "[epoch: 21535/100000, batch:    16/   30, ite: 172276] train loss: 0.313710, tar: 0.015780 \n",
      "It 172276\n",
      "l0: 0.013250, l1: 0.013251, l2: 0.014087, l3: 0.014864, l4: 0.018372, l5: 0.061420, l6: 0.254339\n",
      "\n",
      "[epoch: 21535/100000, batch:    20/   30, ite: 172277] train loss: 0.313984, tar: 0.015771 \n",
      "It 172277\n",
      "l0: 0.009356, l1: 0.009357, l2: 0.010096, l3: 0.010654, l4: 0.013283, l5: 0.063708, l6: 0.156633\n",
      "\n",
      "[epoch: 21535/100000, batch:    24/   30, ite: 172278] train loss: 0.313837, tar: 0.015748 \n",
      "It 172278\n",
      "l0: 0.008287, l1: 0.008287, l2: 0.009103, l3: 0.009820, l4: 0.012709, l5: 0.027663, l6: 0.132199\n",
      "\n",
      "[epoch: 21535/100000, batch:    28/   30, ite: 172279] train loss: 0.313458, tar: 0.015721 \n",
      "It 172279\n",
      "l0: 0.012709, l1: 0.012707, l2: 0.013909, l3: 0.014601, l4: 0.018595, l5: 0.067478, l6: 0.236352\n",
      "\n",
      "[epoch: 21535/100000, batch:    32/   30, ite: 172280] train loss: 0.313683, tar: 0.015710 \n",
      "It 172280\n",
      "l0: 0.036403, l1: 0.036432, l2: 0.038000, l3: 0.040185, l4: 0.047068, l5: 0.114787, l6: 0.268627\n",
      "\n",
      "[epoch: 21536/100000, batch:     4/   30, ite: 172281] train loss: 0.314636, tar: 0.015784 \n",
      "It 172281\n",
      "l0: 0.009010, l1: 0.009011, l2: 0.009927, l3: 0.010607, l4: 0.013000, l5: 0.043958, l6: 0.157781\n",
      "\n",
      "[epoch: 21536/100000, batch:     8/   30, ite: 172282] train loss: 0.314418, tar: 0.015760 \n",
      "It 172282\n",
      "l0: 0.008741, l1: 0.008740, l2: 0.009385, l3: 0.009797, l4: 0.011795, l5: 0.037474, l6: 0.134478\n",
      "\n",
      "[epoch: 21536/100000, batch:    12/   30, ite: 172283] train loss: 0.314086, tar: 0.015735 \n",
      "It 172283\n",
      "l0: 0.011888, l1: 0.011888, l2: 0.012903, l3: 0.013661, l4: 0.017316, l5: 0.034955, l6: 0.226038\n",
      "\n",
      "[epoch: 21536/100000, batch:    16/   30, ite: 172284] train loss: 0.314137, tar: 0.015722 \n",
      "It 172284\n",
      "l0: 0.005687, l1: 0.005685, l2: 0.006479, l3: 0.007034, l4: 0.008606, l5: 0.013113, l6: 0.039077\n",
      "\n",
      "[epoch: 21536/100000, batch:    20/   30, ite: 172285] train loss: 0.313336, tar: 0.015686 \n",
      "It 172285\n",
      "l0: 0.007143, l1: 0.007141, l2: 0.007836, l3: 0.008315, l4: 0.009971, l5: 0.030516, l6: 0.119373\n",
      "\n",
      "[epoch: 21536/100000, batch:    24/   30, ite: 172286] train loss: 0.312906, tar: 0.015656 \n",
      "It 172286\n",
      "l0: 0.029472, l1: 0.029424, l2: 0.030530, l3: 0.032379, l4: 0.036352, l5: 0.065126, l6: 0.191240\n",
      "\n",
      "[epoch: 21536/100000, batch:    28/   30, ite: 172287] train loss: 0.313260, tar: 0.015705 \n",
      "It 172287\n",
      "l0: 0.015773, l1: 0.015770, l2: 0.016988, l3: 0.018076, l4: 0.023473, l5: 0.140015, l6: 0.294988\n",
      "\n",
      "[epoch: 21536/100000, batch:    32/   30, ite: 172288] train loss: 0.313995, tar: 0.015705 \n",
      "It 172288\n",
      "l0: 0.011449, l1: 0.011452, l2: 0.012281, l3: 0.012900, l4: 0.016620, l5: 0.077522, l6: 0.213574\n",
      "\n",
      "[epoch: 21537/100000, batch:     4/   30, ite: 172289] train loss: 0.314140, tar: 0.015690 \n",
      "It 172289\n",
      "l0: 0.008269, l1: 0.008271, l2: 0.009257, l3: 0.009888, l4: 0.013027, l5: 0.021360, l6: 0.092145\n",
      "\n",
      "[epoch: 21537/100000, batch:     8/   30, ite: 172290] train loss: 0.313616, tar: 0.015665 \n",
      "It 172290\n",
      "l0: 0.034087, l1: 0.034017, l2: 0.035183, l3: 0.036773, l4: 0.041324, l5: 0.088643, l6: 0.213276\n",
      "\n",
      "[epoch: 21537/100000, batch:    12/   30, ite: 172291] train loss: 0.314199, tar: 0.015728 \n",
      "It 172291\n",
      "l0: 0.013707, l1: 0.013707, l2: 0.014861, l3: 0.015939, l4: 0.020163, l5: 0.051401, l6: 0.264223\n",
      "\n",
      "[epoch: 21537/100000, batch:    16/   30, ite: 172292] train loss: 0.314472, tar: 0.015721 \n",
      "It 172292\n",
      "l0: 0.008117, l1: 0.008118, l2: 0.008745, l3: 0.009451, l4: 0.013221, l5: 0.055251, l6: 0.156289\n",
      "\n",
      "[epoch: 21537/100000, batch:    20/   30, ite: 172293] train loss: 0.314284, tar: 0.015695 \n",
      "It 172293\n",
      "l0: 0.035609, l1: 0.035657, l2: 0.036936, l3: 0.039379, l4: 0.044485, l5: 0.082614, l6: 0.235277\n",
      "\n",
      "[epoch: 21537/100000, batch:    24/   30, ite: 172294] train loss: 0.314949, tar: 0.015763 \n",
      "It 172294\n",
      "l0: 0.007140, l1: 0.007141, l2: 0.007802, l3: 0.008418, l4: 0.010554, l5: 0.042804, l6: 0.100533\n",
      "\n",
      "[epoch: 21537/100000, batch:    28/   30, ite: 172295] train loss: 0.314507, tar: 0.015733 \n",
      "It 172295\n",
      "l0: 0.005485, l1: 0.005484, l2: 0.005955, l3: 0.006212, l4: 0.008048, l5: 0.016804, l6: 0.057861\n",
      "\n",
      "[epoch: 21537/100000, batch:    32/   30, ite: 172296] train loss: 0.313802, tar: 0.015699 \n",
      "It 172296\n",
      "l0: 0.010167, l1: 0.010165, l2: 0.010991, l3: 0.011612, l4: 0.016566, l5: 0.070242, l6: 0.171572\n",
      "\n",
      "[epoch: 21538/100000, batch:     4/   30, ite: 172297] train loss: 0.313760, tar: 0.015680 \n",
      "It 172297\n",
      "l0: 0.008276, l1: 0.008274, l2: 0.008934, l3: 0.009609, l4: 0.012072, l5: 0.025624, l6: 0.149753\n",
      "\n",
      "[epoch: 21538/100000, batch:     8/   30, ite: 172298] train loss: 0.313454, tar: 0.015655 \n",
      "It 172298\n",
      "l0: 0.010593, l1: 0.010590, l2: 0.011626, l3: 0.012472, l4: 0.017480, l5: 0.060346, l6: 0.152035\n",
      "\n",
      "[epoch: 21538/100000, batch:    12/   30, ite: 172299] train loss: 0.313325, tar: 0.015638 \n",
      "It 172299\n",
      "l0: 0.007715, l1: 0.007714, l2: 0.008426, l3: 0.009138, l4: 0.010949, l5: 0.032655, l6: 0.108726\n",
      "\n",
      "[epoch: 21538/100000, batch:    16/   30, ite: 172300] train loss: 0.312899, tar: 0.015612 \n",
      "It 172300\n",
      "l0: 0.009275, l1: 0.009273, l2: 0.010019, l3: 0.010646, l4: 0.013183, l5: 0.036139, l6: 0.163264\n",
      "\n",
      "[epoch: 21538/100000, batch:    20/   30, ite: 172301] train loss: 0.312696, tar: 0.015591 \n",
      "It 172301\n",
      "l0: 0.009190, l1: 0.009189, l2: 0.010115, l3: 0.010686, l4: 0.013177, l5: 0.035683, l6: 0.144173\n",
      "\n",
      "[epoch: 21538/100000, batch:    24/   30, ite: 172302] train loss: 0.312429, tar: 0.015570 \n",
      "It 172302\n",
      "l0: 0.037085, l1: 0.037048, l2: 0.038461, l3: 0.041142, l4: 0.045851, l5: 0.105125, l6: 0.302644\n",
      "\n",
      "[epoch: 21538/100000, batch:    28/   30, ite: 172303] train loss: 0.313403, tar: 0.015641 \n",
      "It 172303\n",
      "l0: 0.053691, l1: 0.053754, l2: 0.055612, l3: 0.059139, l4: 0.069259, l5: 0.111958, l6: 0.180277\n",
      "\n",
      "[epoch: 21538/100000, batch:    32/   30, ite: 172304] train loss: 0.314292, tar: 0.015766 \n",
      "It 172304\n",
      "l0: 0.012336, l1: 0.012338, l2: 0.013319, l3: 0.014109, l4: 0.018086, l5: 0.057163, l6: 0.241896\n",
      "\n",
      "[epoch: 21539/100000, batch:     4/   30, ite: 172305] train loss: 0.314472, tar: 0.015755 \n",
      "It 172305\n",
      "l0: 0.009678, l1: 0.009679, l2: 0.010658, l3: 0.011398, l4: 0.014872, l5: 0.036800, l6: 0.139925\n",
      "\n",
      "[epoch: 21539/100000, batch:     8/   30, ite: 172306] train loss: 0.314206, tar: 0.015735 \n",
      "It 172306\n",
      "l0: 0.030732, l1: 0.030694, l2: 0.032126, l3: 0.033856, l4: 0.038425, l5: 0.092813, l6: 0.201637\n",
      "\n",
      "[epoch: 21539/100000, batch:    12/   30, ite: 172307] train loss: 0.314681, tar: 0.015784 \n",
      "It 172307\n",
      "l0: 0.029529, l1: 0.029576, l2: 0.030698, l3: 0.033205, l4: 0.038828, l5: 0.071808, l6: 0.174415\n",
      "\n",
      "[epoch: 21539/100000, batch:    16/   30, ite: 172308] train loss: 0.314985, tar: 0.015828 \n",
      "It 172308\n",
      "l0: 0.010518, l1: 0.010519, l2: 0.011323, l3: 0.012037, l4: 0.014710, l5: 0.046738, l6: 0.198218\n",
      "\n",
      "[epoch: 21539/100000, batch:    20/   30, ite: 172309] train loss: 0.314949, tar: 0.015811 \n",
      "It 172309\n",
      "l0: 0.010365, l1: 0.010364, l2: 0.011190, l3: 0.012022, l4: 0.015581, l5: 0.074380, l6: 0.163071\n",
      "\n",
      "[epoch: 21539/100000, batch:    24/   30, ite: 172310] train loss: 0.314891, tar: 0.015794 \n",
      "It 172310\n",
      "l0: 0.007193, l1: 0.007193, l2: 0.007909, l3: 0.008478, l4: 0.010830, l5: 0.018566, l6: 0.101597\n",
      "\n",
      "[epoch: 21539/100000, batch:    28/   30, ite: 172311] train loss: 0.314399, tar: 0.015766 \n",
      "It 172311\n",
      "l0: 0.006271, l1: 0.006272, l2: 0.006996, l3: 0.007655, l4: 0.008726, l5: 0.022767, l6: 0.101722\n",
      "\n",
      "[epoch: 21539/100000, batch:    32/   30, ite: 172312] train loss: 0.313905, tar: 0.015736 \n",
      "It 172312\n",
      "l0: 0.034095, l1: 0.034029, l2: 0.035466, l3: 0.038604, l4: 0.042795, l5: 0.076657, l6: 0.192273\n",
      "\n",
      "[epoch: 21540/100000, batch:     4/   30, ite: 172313] train loss: 0.314353, tar: 0.015794 \n",
      "It 172313\n",
      "l0: 0.007878, l1: 0.007877, l2: 0.008638, l3: 0.009169, l4: 0.012025, l5: 0.031242, l6: 0.130497\n",
      "\n",
      "[epoch: 21540/100000, batch:     8/   30, ite: 172314] train loss: 0.314012, tar: 0.015769 \n",
      "It 172314\n",
      "l0: 0.009731, l1: 0.009731, l2: 0.010489, l3: 0.011087, l4: 0.013773, l5: 0.053788, l6: 0.178557\n",
      "\n",
      "[epoch: 21540/100000, batch:    12/   30, ite: 172315] train loss: 0.313927, tar: 0.015750 \n",
      "It 172315\n",
      "l0: 0.009778, l1: 0.009780, l2: 0.010517, l3: 0.011078, l4: 0.014309, l5: 0.053461, l6: 0.184970\n",
      "\n",
      "[epoch: 21540/100000, batch:    16/   30, ite: 172316] train loss: 0.313863, tar: 0.015731 \n",
      "It 172316\n",
      "l0: 0.009060, l1: 0.009059, l2: 0.009885, l3: 0.010481, l4: 0.013452, l5: 0.042066, l6: 0.156098\n",
      "\n",
      "[epoch: 21540/100000, batch:    20/   30, ite: 172317] train loss: 0.313662, tar: 0.015710 \n",
      "It 172317\n",
      "l0: 0.033634, l1: 0.033696, l2: 0.035190, l3: 0.038426, l4: 0.045335, l5: 0.101012, l6: 0.264380\n",
      "\n",
      "[epoch: 21540/100000, batch:    24/   30, ite: 172318] train loss: 0.314411, tar: 0.015766 \n",
      "It 172318\n",
      "l0: 0.007551, l1: 0.007551, l2: 0.008388, l3: 0.008885, l4: 0.010987, l5: 0.020288, l6: 0.101219\n",
      "\n",
      "[epoch: 21540/100000, batch:    28/   30, ite: 172319] train loss: 0.313942, tar: 0.015740 \n",
      "It 172319\n",
      "l0: 0.011721, l1: 0.011721, l2: 0.012796, l3: 0.013739, l4: 0.016756, l5: 0.033614, l6: 0.184770\n",
      "\n",
      "[epoch: 21540/100000, batch:    32/   30, ite: 172320] train loss: 0.313852, tar: 0.015728 \n",
      "It 172320\n",
      "l0: 0.009638, l1: 0.009636, l2: 0.010382, l3: 0.010944, l4: 0.013507, l5: 0.048913, l6: 0.169577\n",
      "\n",
      "[epoch: 21541/100000, batch:     4/   30, ite: 172321] train loss: 0.313723, tar: 0.015709 \n",
      "It 172321\n",
      "l0: 0.010542, l1: 0.010540, l2: 0.011308, l3: 0.012078, l4: 0.016736, l5: 0.100187, l6: 0.196815\n",
      "\n",
      "[epoch: 21541/100000, batch:     8/   30, ite: 172322] train loss: 0.313861, tar: 0.015693 \n",
      "It 172322\n",
      "l0: 0.007961, l1: 0.007958, l2: 0.008902, l3: 0.009529, l4: 0.012040, l5: 0.021982, l6: 0.111528\n",
      "\n",
      "[epoch: 21541/100000, batch:    12/   30, ite: 172323] train loss: 0.313447, tar: 0.015669 \n",
      "It 172323\n",
      "l0: 0.026350, l1: 0.026398, l2: 0.026968, l3: 0.028026, l4: 0.031329, l5: 0.056714, l6: 0.114466\n",
      "\n",
      "[epoch: 21541/100000, batch:    16/   30, ite: 172324] train loss: 0.313437, tar: 0.015702 \n",
      "It 172324\n",
      "l0: 0.008879, l1: 0.008877, l2: 0.009724, l3: 0.010336, l4: 0.012830, l5: 0.038599, l6: 0.122324\n",
      "\n",
      "[epoch: 21541/100000, batch:    20/   30, ite: 172325] train loss: 0.313123, tar: 0.015681 \n",
      "It 172325\n",
      "l0: 0.012611, l1: 0.012609, l2: 0.013571, l3: 0.014439, l4: 0.017421, l5: 0.050042, l6: 0.248834\n",
      "\n",
      "[epoch: 21541/100000, batch:    24/   30, ite: 172326] train loss: 0.313296, tar: 0.015672 \n",
      "It 172326\n",
      "l0: 0.032955, l1: 0.032893, l2: 0.034870, l3: 0.038347, l4: 0.044251, l5: 0.068210, l6: 0.216181\n",
      "\n",
      "[epoch: 21541/100000, batch:    28/   30, ite: 172327] train loss: 0.313768, tar: 0.015724 \n",
      "It 172327\n",
      "l0: 0.010602, l1: 0.010602, l2: 0.011428, l3: 0.012267, l4: 0.016062, l5: 0.059277, l6: 0.174199\n",
      "\n",
      "[epoch: 21541/100000, batch:    32/   30, ite: 172328] train loss: 0.313710, tar: 0.015709 \n",
      "It 172328\n",
      "l0: 0.010650, l1: 0.010651, l2: 0.011628, l3: 0.012992, l4: 0.017158, l5: 0.040245, l6: 0.176626\n",
      "\n",
      "[epoch: 21542/100000, batch:     4/   30, ite: 172329] train loss: 0.313607, tar: 0.015693 \n",
      "It 172329\n",
      "l0: 0.035309, l1: 0.035236, l2: 0.036807, l3: 0.039550, l4: 0.044886, l5: 0.077242, l6: 0.211644\n",
      "\n",
      "[epoch: 21542/100000, batch:     8/   30, ite: 172330] train loss: 0.314113, tar: 0.015753 \n",
      "It 172330\n",
      "l0: 0.009966, l1: 0.009970, l2: 0.010885, l3: 0.011685, l4: 0.015207, l5: 0.045926, l6: 0.145085\n",
      "\n",
      "[epoch: 21542/100000, batch:    12/   30, ite: 172331] train loss: 0.313916, tar: 0.015735 \n",
      "It 172331\n",
      "l0: 0.012646, l1: 0.012654, l2: 0.013555, l3: 0.014547, l4: 0.018352, l5: 0.045408, l6: 0.235243\n",
      "\n",
      "[epoch: 21542/100000, batch:    16/   30, ite: 172332] train loss: 0.314032, tar: 0.015726 \n",
      "It 172332\n",
      "l0: 0.010016, l1: 0.010021, l2: 0.010789, l3: 0.011350, l4: 0.013617, l5: 0.040946, l6: 0.189515\n",
      "\n",
      "[epoch: 21542/100000, batch:    20/   30, ite: 172333] train loss: 0.313948, tar: 0.015709 \n",
      "It 172333\n",
      "l0: 0.009625, l1: 0.009626, l2: 0.010576, l3: 0.011312, l4: 0.015116, l5: 0.055777, l6: 0.134679\n",
      "\n",
      "[epoch: 21542/100000, batch:    24/   30, ite: 172334] train loss: 0.313747, tar: 0.015691 \n",
      "It 172334\n",
      "l0: 0.028832, l1: 0.028920, l2: 0.029978, l3: 0.033950, l4: 0.040674, l5: 0.064069, l6: 0.129355\n",
      "\n",
      "[epoch: 21542/100000, batch:    28/   30, ite: 172335] train loss: 0.313872, tar: 0.015730 \n",
      "It 172335\n",
      "l0: 0.010641, l1: 0.010635, l2: 0.011465, l3: 0.012587, l4: 0.018342, l5: 0.076958, l6: 0.183357\n",
      "\n",
      "[epoch: 21542/100000, batch:    32/   30, ite: 172336] train loss: 0.313902, tar: 0.015715 \n",
      "It 172336\n",
      "l0: 0.009793, l1: 0.009789, l2: 0.010696, l3: 0.011351, l4: 0.014503, l5: 0.042318, l6: 0.158163\n",
      "\n",
      "[epoch: 21543/100000, batch:     4/   30, ite: 172337] train loss: 0.313732, tar: 0.015697 \n",
      "It 172337\n",
      "l0: 0.009422, l1: 0.009416, l2: 0.010153, l3: 0.010678, l4: 0.015024, l5: 0.054245, l6: 0.146049\n",
      "\n",
      "[epoch: 21543/100000, batch:     8/   30, ite: 172338] train loss: 0.313559, tar: 0.015679 \n",
      "It 172338\n",
      "l0: 0.009854, l1: 0.009849, l2: 0.010727, l3: 0.011318, l4: 0.013645, l5: 0.027274, l6: 0.154119\n",
      "\n",
      "[epoch: 21543/100000, batch:    12/   30, ite: 172339] train loss: 0.313332, tar: 0.015661 \n",
      "It 172339\n",
      "l0: 0.015235, l1: 0.015226, l2: 0.016355, l3: 0.017672, l4: 0.025024, l5: 0.076745, l6: 0.306944\n",
      "\n",
      "[epoch: 21543/100000, batch:    16/   30, ite: 172340] train loss: 0.313802, tar: 0.015660 \n",
      "It 172340\n",
      "l0: 0.031642, l1: 0.031521, l2: 0.032665, l3: 0.033446, l4: 0.035448, l5: 0.067996, l6: 0.199135\n",
      "\n",
      "[epoch: 21543/100000, batch:    20/   30, ite: 172341] train loss: 0.314149, tar: 0.015707 \n",
      "It 172341\n",
      "l0: 0.035404, l1: 0.035452, l2: 0.036695, l3: 0.039034, l4: 0.044711, l5: 0.084524, l6: 0.251016\n",
      "\n",
      "[epoch: 21543/100000, batch:    24/   30, ite: 172342] train loss: 0.314770, tar: 0.015765 \n",
      "It 172342\n",
      "l0: 0.005734, l1: 0.005733, l2: 0.006281, l3: 0.006534, l4: 0.008211, l5: 0.024869, l6: 0.075375\n",
      "\n",
      "[epoch: 21543/100000, batch:    28/   30, ite: 172343] train loss: 0.314240, tar: 0.015735 \n",
      "It 172343\n",
      "l0: 0.004767, l1: 0.004768, l2: 0.005428, l3: 0.005788, l4: 0.008107, l5: 0.014879, l6: 0.052128\n",
      "\n",
      "[epoch: 21543/100000, batch:    32/   30, ite: 172344] train loss: 0.313605, tar: 0.015704 \n",
      "It 172344\n",
      "l0: 0.005717, l1: 0.005720, l2: 0.006239, l3: 0.006551, l4: 0.008110, l5: 0.029089, l6: 0.078518\n",
      "\n",
      "[epoch: 21544/100000, batch:     4/   30, ite: 172345] train loss: 0.313102, tar: 0.015675 \n",
      "It 172345\n",
      "l0: 0.009131, l1: 0.009137, l2: 0.010136, l3: 0.010695, l4: 0.013226, l5: 0.026552, l6: 0.135598\n",
      "\n",
      "[epoch: 21544/100000, batch:     8/   30, ite: 172346] train loss: 0.312817, tar: 0.015656 \n",
      "It 172346\n",
      "l0: 0.016576, l1: 0.016583, l2: 0.017826, l3: 0.019143, l4: 0.026414, l5: 0.070832, l6: 0.286827\n",
      "\n",
      "[epoch: 21544/100000, batch:    12/   30, ite: 172347] train loss: 0.313224, tar: 0.015658 \n",
      "It 172347\n",
      "l0: 0.033251, l1: 0.033337, l2: 0.034539, l3: 0.037916, l4: 0.044223, l5: 0.078985, l6: 0.194541\n",
      "\n",
      "[epoch: 21544/100000, batch:    16/   30, ite: 172348] train loss: 0.313637, tar: 0.015709 \n",
      "It 172348\n",
      "l0: 0.012180, l1: 0.012177, l2: 0.013301, l3: 0.014136, l4: 0.017264, l5: 0.048042, l6: 0.198121\n",
      "\n",
      "[epoch: 21544/100000, batch:    20/   30, ite: 172349] train loss: 0.313641, tar: 0.015699 \n",
      "It 172349\n",
      "l0: 0.029675, l1: 0.029611, l2: 0.031115, l3: 0.033199, l4: 0.037646, l5: 0.066497, l6: 0.176388\n",
      "\n",
      "[epoch: 21544/100000, batch:    24/   30, ite: 172350] train loss: 0.313900, tar: 0.015739 \n",
      "It 172350\n",
      "l0: 0.010832, l1: 0.010831, l2: 0.011772, l3: 0.013016, l4: 0.017606, l5: 0.065853, l6: 0.180062\n",
      "\n",
      "[epoch: 21544/100000, batch:    28/   30, ite: 172351] train loss: 0.313888, tar: 0.015725 \n",
      "It 172351\n",
      "l0: 0.007255, l1: 0.007255, l2: 0.008064, l3: 0.008686, l4: 0.010080, l5: 0.020214, l6: 0.067506\n",
      "\n",
      "[epoch: 21544/100000, batch:    32/   30, ite: 172352] train loss: 0.313363, tar: 0.015701 \n",
      "It 172352\n",
      "l0: 0.012291, l1: 0.012286, l2: 0.013451, l3: 0.014267, l4: 0.016968, l5: 0.059885, l6: 0.202566\n",
      "\n",
      "[epoch: 21545/100000, batch:     4/   30, ite: 172353] train loss: 0.313415, tar: 0.015691 \n",
      "It 172353\n",
      "l0: 0.005911, l1: 0.005909, l2: 0.006494, l3: 0.007263, l4: 0.009910, l5: 0.013887, l6: 0.074465\n",
      "\n",
      "[epoch: 21545/100000, batch:     8/   30, ite: 172354] train loss: 0.312880, tar: 0.015663 \n",
      "It 172354\n",
      "l0: 0.011187, l1: 0.011186, l2: 0.012111, l3: 0.013115, l4: 0.018446, l5: 0.065422, l6: 0.189703\n",
      "\n",
      "[epoch: 21545/100000, batch:    12/   30, ite: 172355] train loss: 0.312903, tar: 0.015651 \n",
      "It 172355\n",
      "l0: 0.006036, l1: 0.006034, l2: 0.006648, l3: 0.007055, l4: 0.008294, l5: 0.026188, l6: 0.087725\n",
      "\n",
      "[epoch: 21545/100000, batch:    16/   30, ite: 172356] train loss: 0.312440, tar: 0.015624 \n",
      "It 172356\n",
      "l0: 0.010926, l1: 0.010924, l2: 0.011782, l3: 0.012515, l4: 0.015700, l5: 0.039091, l6: 0.221746\n",
      "\n",
      "[epoch: 21545/100000, batch:    20/   30, ite: 172357] train loss: 0.312469, tar: 0.015611 \n",
      "It 172357\n",
      "l0: 0.038157, l1: 0.038110, l2: 0.039890, l3: 0.043290, l4: 0.049581, l5: 0.099507, l6: 0.292420\n",
      "\n",
      "[epoch: 21545/100000, batch:    24/   30, ite: 172358] train loss: 0.313274, tar: 0.015674 \n",
      "It 172358\n",
      "l0: 0.036064, l1: 0.036158, l2: 0.037487, l3: 0.041185, l4: 0.048423, l5: 0.111430, l6: 0.235246\n",
      "\n",
      "[epoch: 21545/100000, batch:    28/   30, ite: 172359] train loss: 0.313923, tar: 0.015730 \n",
      "It 172359\n",
      "l0: 0.004848, l1: 0.004853, l2: 0.005472, l3: 0.005738, l4: 0.006530, l5: 0.007888, l6: 0.020098\n",
      "\n",
      "[epoch: 21545/100000, batch:    32/   30, ite: 172360] train loss: 0.313205, tar: 0.015700 \n",
      "It 172360\n",
      "l0: 0.009267, l1: 0.009270, l2: 0.010054, l3: 0.010663, l4: 0.014700, l5: 0.058347, l6: 0.165040\n",
      "\n",
      "[epoch: 21546/100000, batch:     4/   30, ite: 172361] train loss: 0.313105, tar: 0.015682 \n",
      "It 172361\n",
      "l0: 0.031017, l1: 0.030976, l2: 0.032612, l3: 0.035102, l4: 0.039209, l5: 0.064465, l6: 0.184778\n",
      "\n",
      "[epoch: 21546/100000, batch:     8/   30, ite: 172362] train loss: 0.313395, tar: 0.015725 \n",
      "It 172362\n",
      "l0: 0.007463, l1: 0.007463, l2: 0.008093, l3: 0.008521, l4: 0.010610, l5: 0.038353, l6: 0.113743\n",
      "\n",
      "[epoch: 21546/100000, batch:    12/   30, ite: 172363] train loss: 0.313067, tar: 0.015702 \n",
      "It 172363\n",
      "l0: 0.007741, l1: 0.007738, l2: 0.008484, l3: 0.008981, l4: 0.011504, l5: 0.031180, l6: 0.114198\n",
      "\n",
      "[epoch: 21546/100000, batch:    16/   30, ite: 172364] train loss: 0.312729, tar: 0.015680 \n",
      "It 172364\n",
      "l0: 0.011337, l1: 0.011332, l2: 0.012272, l3: 0.013015, l4: 0.016102, l5: 0.049186, l6: 0.207372\n",
      "\n",
      "[epoch: 21546/100000, batch:    20/   30, ite: 172365] train loss: 0.312750, tar: 0.015668 \n",
      "It 172365\n",
      "l0: 0.010109, l1: 0.010106, l2: 0.010840, l3: 0.011684, l4: 0.014840, l5: 0.048958, l6: 0.198481\n",
      "\n",
      "[epoch: 21546/100000, batch:    24/   30, ite: 172366] train loss: 0.312729, tar: 0.015653 \n",
      "It 172366\n",
      "l0: 0.009306, l1: 0.009304, l2: 0.010324, l3: 0.010978, l4: 0.013399, l5: 0.026830, l6: 0.129082\n",
      "\n",
      "[epoch: 21546/100000, batch:    28/   30, ite: 172367] train loss: 0.312447, tar: 0.015636 \n",
      "It 172367\n",
      "l0: 0.058324, l1: 0.058482, l2: 0.060715, l3: 0.066520, l4: 0.075308, l5: 0.152443, l6: 0.347663\n",
      "\n",
      "[epoch: 21546/100000, batch:    32/   30, ite: 172368] train loss: 0.313825, tar: 0.015752 \n",
      "It 172368\n",
      "l0: 0.008640, l1: 0.008634, l2: 0.009564, l3: 0.010201, l4: 0.012541, l5: 0.023240, l6: 0.127681\n",
      "\n",
      "[epoch: 21547/100000, batch:     4/   30, ite: 172369] train loss: 0.313518, tar: 0.015732 \n",
      "It 172369\n",
      "l0: 0.010468, l1: 0.010459, l2: 0.011372, l3: 0.012042, l4: 0.015522, l5: 0.050168, l6: 0.141094\n",
      "\n",
      "[epoch: 21547/100000, batch:     8/   30, ite: 172370] train loss: 0.313349, tar: 0.015718 \n",
      "It 172370\n",
      "l0: 0.010013, l1: 0.010012, l2: 0.010888, l3: 0.012042, l4: 0.015854, l5: 0.055180, l6: 0.154703\n",
      "\n",
      "[epoch: 21547/100000, batch:    12/   30, ite: 172371] train loss: 0.313229, tar: 0.015703 \n",
      "It 172371\n",
      "l0: 0.004914, l1: 0.004912, l2: 0.005461, l3: 0.005908, l4: 0.008111, l5: 0.022296, l6: 0.067010\n",
      "\n",
      "[epoch: 21547/100000, batch:    16/   30, ite: 172372] train loss: 0.312706, tar: 0.015674 \n",
      "It 172372\n",
      "l0: 0.032359, l1: 0.032283, l2: 0.033644, l3: 0.035986, l4: 0.042437, l5: 0.086184, l6: 0.211903\n",
      "\n",
      "[epoch: 21547/100000, batch:    20/   30, ite: 172373] train loss: 0.313140, tar: 0.015718 \n",
      "It 172373\n",
      "l0: 0.014423, l1: 0.014412, l2: 0.015442, l3: 0.016558, l4: 0.021561, l5: 0.068555, l6: 0.279283\n",
      "\n",
      "[epoch: 21547/100000, batch:    24/   30, ite: 172374] train loss: 0.313453, tar: 0.015715 \n",
      "It 172374\n",
      "l0: 0.030472, l1: 0.030533, l2: 0.032006, l3: 0.033994, l4: 0.039547, l5: 0.078545, l6: 0.195728\n",
      "\n",
      "[epoch: 21547/100000, batch:    28/   30, ite: 172375] train loss: 0.313793, tar: 0.015754 \n",
      "It 172375\n",
      "l0: 0.013105, l1: 0.013102, l2: 0.014193, l3: 0.014775, l4: 0.020302, l5: 0.065300, l6: 0.218252\n",
      "\n",
      "[epoch: 21547/100000, batch:    32/   30, ite: 172376] train loss: 0.313913, tar: 0.015747 \n",
      "It 172376\n",
      "l0: 0.011121, l1: 0.011122, l2: 0.012098, l3: 0.013022, l4: 0.017596, l5: 0.056432, l6: 0.198180\n",
      "\n",
      "[epoch: 21548/100000, batch:     4/   30, ite: 172377] train loss: 0.313928, tar: 0.015735 \n",
      "It 172377\n",
      "l0: 0.007471, l1: 0.007478, l2: 0.008162, l3: 0.008977, l4: 0.011614, l5: 0.021797, l6: 0.110494\n",
      "\n",
      "[epoch: 21548/100000, batch:     8/   30, ite: 172378] train loss: 0.313563, tar: 0.015713 \n",
      "It 172378\n",
      "l0: 0.034199, l1: 0.034272, l2: 0.035147, l3: 0.037221, l4: 0.043494, l5: 0.104454, l6: 0.257977\n",
      "\n",
      "[epoch: 21548/100000, batch:    12/   30, ite: 172379] train loss: 0.314179, tar: 0.015762 \n",
      "It 172379\n",
      "l0: 0.010090, l1: 0.010090, l2: 0.011015, l3: 0.011701, l4: 0.014555, l5: 0.036244, l6: 0.159992\n",
      "\n",
      "[epoch: 21548/100000, batch:    16/   30, ite: 172380] train loss: 0.314019, tar: 0.015747 \n",
      "It 172380\n",
      "l0: 0.008193, l1: 0.008197, l2: 0.009061, l3: 0.009677, l4: 0.011775, l5: 0.030373, l6: 0.130451\n",
      "\n",
      "[epoch: 21548/100000, batch:    20/   30, ite: 172381] train loss: 0.313740, tar: 0.015727 \n",
      "It 172381\n",
      "l0: 0.010468, l1: 0.010466, l2: 0.011328, l3: 0.012090, l4: 0.016307, l5: 0.044308, l6: 0.180316\n",
      "\n",
      "[epoch: 21548/100000, batch:    24/   30, ite: 172382] train loss: 0.313666, tar: 0.015713 \n",
      "It 172382\n",
      "l0: 0.034390, l1: 0.034283, l2: 0.035770, l3: 0.037443, l4: 0.039686, l5: 0.084489, l6: 0.201100\n",
      "\n",
      "[epoch: 21548/100000, batch:    28/   30, ite: 172383] train loss: 0.314067, tar: 0.015762 \n",
      "It 172383\n",
      "l0: 0.006683, l1: 0.006678, l2: 0.007269, l3: 0.007588, l4: 0.008682, l5: 0.012157, l6: 0.093523\n",
      "\n",
      "[epoch: 21548/100000, batch:    32/   30, ite: 172384] train loss: 0.313620, tar: 0.015739 \n",
      "It 172384\n",
      "l0: 0.008246, l1: 0.008248, l2: 0.009027, l3: 0.009791, l4: 0.012447, l5: 0.030926, l6: 0.123563\n",
      "\n",
      "[epoch: 21549/100000, batch:     4/   30, ite: 172385] train loss: 0.313331, tar: 0.015719 \n",
      "It 172385\n",
      "l0: 0.038300, l1: 0.038240, l2: 0.040320, l3: 0.044126, l4: 0.049900, l5: 0.124626, l6: 0.235001\n",
      "\n",
      "[epoch: 21549/100000, batch:     8/   30, ite: 172386] train loss: 0.313997, tar: 0.015778 \n",
      "It 172386\n",
      "l0: 0.011566, l1: 0.011548, l2: 0.012500, l3: 0.013316, l4: 0.016023, l5: 0.054555, l6: 0.195034\n",
      "\n",
      "[epoch: 21549/100000, batch:    12/   30, ite: 172387] train loss: 0.313999, tar: 0.015767 \n",
      "It 172387\n",
      "l0: 0.032061, l1: 0.032179, l2: 0.033272, l3: 0.035873, l4: 0.042844, l5: 0.073074, l6: 0.193324\n",
      "\n",
      "[epoch: 21549/100000, batch:    16/   30, ite: 172388] train loss: 0.314330, tar: 0.015809 \n",
      "It 172388\n",
      "l0: 0.007913, l1: 0.007919, l2: 0.008654, l3: 0.009357, l4: 0.011957, l5: 0.052822, l6: 0.122853\n",
      "\n",
      "[epoch: 21549/100000, batch:    20/   30, ite: 172389] train loss: 0.314091, tar: 0.015788 \n",
      "It 172389\n",
      "l0: 0.006429, l1: 0.006429, l2: 0.007139, l3: 0.007553, l4: 0.008929, l5: 0.015760, l6: 0.093191\n",
      "\n",
      "[epoch: 21549/100000, batch:    24/   30, ite: 172390] train loss: 0.313659, tar: 0.015764 \n",
      "It 172390\n",
      "l0: 0.012813, l1: 0.012808, l2: 0.013633, l3: 0.014315, l4: 0.018603, l5: 0.046469, l6: 0.259721\n",
      "\n",
      "[epoch: 21549/100000, batch:    28/   30, ite: 172391] train loss: 0.313824, tar: 0.015757 \n",
      "It 172391\n",
      "l0: 0.010378, l1: 0.010370, l2: 0.011358, l3: 0.012829, l4: 0.016208, l5: 0.038670, l6: 0.194788\n",
      "\n",
      "[epoch: 21549/100000, batch:    32/   30, ite: 172392] train loss: 0.313775, tar: 0.015743 \n",
      "It 172392\n",
      "l0: 0.008896, l1: 0.008893, l2: 0.009791, l3: 0.010582, l4: 0.014347, l5: 0.035727, l6: 0.137031\n",
      "\n",
      "[epoch: 21550/100000, batch:     4/   30, ite: 172393] train loss: 0.313550, tar: 0.015726 \n",
      "It 172393\n",
      "l0: 0.010579, l1: 0.010577, l2: 0.011469, l3: 0.012453, l4: 0.016100, l5: 0.082161, l6: 0.170238\n",
      "\n",
      "[epoch: 21550/100000, batch:     8/   30, ite: 172394] train loss: 0.313550, tar: 0.015713 \n",
      "It 172394\n",
      "l0: 0.010148, l1: 0.010142, l2: 0.011021, l3: 0.011648, l4: 0.014045, l5: 0.042333, l6: 0.176855\n",
      "\n",
      "[epoch: 21550/100000, batch:    12/   30, ite: 172395] train loss: 0.313456, tar: 0.015699 \n",
      "It 172395\n",
      "l0: 0.007067, l1: 0.007065, l2: 0.007812, l3: 0.008235, l4: 0.009642, l5: 0.016013, l6: 0.099784\n",
      "\n",
      "[epoch: 21550/100000, batch:    16/   30, ite: 172396] train loss: 0.313057, tar: 0.015677 \n",
      "It 172396\n",
      "l0: 0.026880, l1: 0.026934, l2: 0.027630, l3: 0.029748, l4: 0.034653, l5: 0.063356, l6: 0.135206\n",
      "\n",
      "[epoch: 21550/100000, batch:    20/   30, ite: 172397] train loss: 0.313136, tar: 0.015705 \n",
      "It 172397\n",
      "l0: 0.007129, l1: 0.007127, l2: 0.007672, l3: 0.008091, l4: 0.010143, l5: 0.035456, l6: 0.123303\n",
      "\n",
      "[epoch: 21550/100000, batch:    24/   30, ite: 172398] train loss: 0.312849, tar: 0.015683 \n",
      "It 172398\n",
      "l0: 0.014168, l1: 0.014160, l2: 0.015326, l3: 0.016280, l4: 0.020656, l5: 0.072610, l6: 0.238679\n",
      "\n",
      "[epoch: 21550/100000, batch:    28/   30, ite: 172399] train loss: 0.313047, tar: 0.015680 \n",
      "It 172399\n",
      "l0: 0.062183, l1: 0.061933, l2: 0.064138, l3: 0.067154, l4: 0.068768, l5: 0.121966, l6: 0.396314\n",
      "\n",
      "[epoch: 21550/100000, batch:    32/   30, ite: 172400] train loss: 0.314371, tar: 0.015796 \n",
      "It 172400\n",
      "l0: 0.009791, l1: 0.009787, l2: 0.010468, l3: 0.010867, l4: 0.012844, l5: 0.042410, l6: 0.189122\n",
      "\n",
      "[epoch: 21551/100000, batch:     4/   30, ite: 172401] train loss: 0.314298, tar: 0.015781 \n",
      "It 172401\n",
      "l0: 0.006148, l1: 0.006147, l2: 0.006724, l3: 0.007099, l4: 0.009702, l5: 0.061909, l6: 0.081199\n",
      "\n",
      "[epoch: 21551/100000, batch:     8/   30, ite: 172402] train loss: 0.313961, tar: 0.015757 \n",
      "It 172402\n",
      "l0: 0.033111, l1: 0.033217, l2: 0.034392, l3: 0.036855, l4: 0.043730, l5: 0.083150, l6: 0.264105\n",
      "\n",
      "[epoch: 21551/100000, batch:    12/   30, ite: 172403] train loss: 0.314494, tar: 0.015800 \n",
      "It 172403\n",
      "l0: 0.008211, l1: 0.008213, l2: 0.008933, l3: 0.009686, l4: 0.012884, l5: 0.052199, l6: 0.135238\n",
      "\n",
      "[epoch: 21551/100000, batch:    16/   30, ite: 172404] train loss: 0.314298, tar: 0.015781 \n",
      "It 172404\n",
      "l0: 0.030100, l1: 0.030006, l2: 0.031142, l3: 0.032422, l4: 0.033117, l5: 0.059674, l6: 0.148415\n",
      "\n",
      "[epoch: 21551/100000, batch:    20/   30, ite: 172405] train loss: 0.314423, tar: 0.015817 \n",
      "It 172405\n",
      "l0: 0.013016, l1: 0.013016, l2: 0.014181, l3: 0.015167, l4: 0.017366, l5: 0.041675, l6: 0.229684\n",
      "\n",
      "[epoch: 21551/100000, batch:    24/   30, ite: 172406] train loss: 0.314496, tar: 0.015810 \n",
      "It 172406\n",
      "l0: 0.010697, l1: 0.010698, l2: 0.011819, l3: 0.012848, l4: 0.016185, l5: 0.043483, l6: 0.144388\n",
      "\n",
      "[epoch: 21551/100000, batch:    28/   30, ite: 172407] train loss: 0.314338, tar: 0.015797 \n",
      "It 172407\n",
      "l0: 0.010058, l1: 0.010056, l2: 0.010767, l3: 0.011505, l4: 0.016632, l5: 0.092176, l6: 0.161020\n",
      "\n",
      "[epoch: 21551/100000, batch:    32/   30, ite: 172408] train loss: 0.314333, tar: 0.015783 \n",
      "It 172408\n",
      "l0: 0.010020, l1: 0.010016, l2: 0.010919, l3: 0.011539, l4: 0.014612, l5: 0.036964, l6: 0.157616\n",
      "\n",
      "[epoch: 21552/100000, batch:     4/   30, ite: 172409] train loss: 0.314179, tar: 0.015769 \n",
      "It 172409\n",
      "l0: 0.010621, l1: 0.010616, l2: 0.011482, l3: 0.012206, l4: 0.016059, l5: 0.053573, l6: 0.199019\n",
      "\n",
      "[epoch: 21552/100000, batch:     8/   30, ite: 172410] train loss: 0.314178, tar: 0.015756 \n",
      "It 172410\n",
      "l0: 0.012589, l1: 0.012584, l2: 0.013558, l3: 0.014875, l4: 0.021524, l5: 0.089721, l6: 0.211709\n",
      "\n",
      "[epoch: 21552/100000, batch:    12/   30, ite: 172411] train loss: 0.314330, tar: 0.015749 \n",
      "It 172411\n",
      "l0: 0.010180, l1: 0.010179, l2: 0.011224, l3: 0.011922, l4: 0.015045, l5: 0.041211, l6: 0.150721\n",
      "\n",
      "[epoch: 21552/100000, batch:    16/   30, ite: 172412] train loss: 0.314175, tar: 0.015735 \n",
      "It 172412\n",
      "l0: 0.007041, l1: 0.007041, l2: 0.007856, l3: 0.008493, l4: 0.009961, l5: 0.028427, l6: 0.083291\n",
      "\n",
      "[epoch: 21552/100000, batch:    20/   30, ite: 172413] train loss: 0.313782, tar: 0.015714 \n",
      "It 172413\n",
      "l0: 0.003671, l1: 0.003671, l2: 0.004098, l3: 0.004366, l4: 0.005900, l5: 0.012093, l6: 0.041330\n",
      "\n",
      "[epoch: 21552/100000, batch:    24/   30, ite: 172414] train loss: 0.313206, tar: 0.015685 \n",
      "It 172414\n",
      "l0: 0.038678, l1: 0.038594, l2: 0.040124, l3: 0.042512, l4: 0.049031, l5: 0.126053, l6: 0.346599\n",
      "\n",
      "[epoch: 21552/100000, batch:    28/   30, ite: 172415] train loss: 0.314094, tar: 0.015740 \n",
      "It 172415\n",
      "l0: 0.056667, l1: 0.056857, l2: 0.058521, l3: 0.065474, l4: 0.074707, l5: 0.115650, l6: 0.193658\n",
      "\n",
      "[epoch: 21552/100000, batch:    32/   30, ite: 172416] train loss: 0.314833, tar: 0.015839 \n",
      "It 172416\n",
      "l0: 0.011211, l1: 0.011212, l2: 0.012228, l3: 0.013118, l4: 0.016506, l5: 0.045127, l6: 0.200641\n",
      "\n",
      "[epoch: 21553/100000, batch:     4/   30, ite: 172417] train loss: 0.314821, tar: 0.015828 \n",
      "It 172417\n",
      "l0: 0.033089, l1: 0.033010, l2: 0.034247, l3: 0.035532, l4: 0.041725, l5: 0.072825, l6: 0.213035\n",
      "\n",
      "[epoch: 21553/100000, batch:     8/   30, ite: 172418] train loss: 0.315177, tar: 0.015869 \n",
      "It 172418\n",
      "l0: 0.008852, l1: 0.008850, l2: 0.009461, l3: 0.009913, l4: 0.011825, l5: 0.056146, l6: 0.154473\n",
      "\n",
      "[epoch: 21553/100000, batch:    12/   30, ite: 172419] train loss: 0.315044, tar: 0.015852 \n",
      "It 172419\n",
      "l0: 0.006913, l1: 0.006908, l2: 0.007697, l3: 0.008098, l4: 0.010093, l5: 0.019457, l6: 0.103591\n",
      "\n",
      "[epoch: 21553/100000, batch:    16/   30, ite: 172420] train loss: 0.314681, tar: 0.015831 \n",
      "It 172420\n",
      "l0: 0.033414, l1: 0.033511, l2: 0.034883, l3: 0.037550, l4: 0.045208, l5: 0.090507, l6: 0.177307\n",
      "\n",
      "[epoch: 21553/100000, batch:    20/   30, ite: 172421] train loss: 0.315008, tar: 0.015873 \n",
      "It 172421\n",
      "l0: 0.013406, l1: 0.013398, l2: 0.014600, l3: 0.015433, l4: 0.018029, l5: 0.045378, l6: 0.245324\n",
      "\n",
      "[epoch: 21553/100000, batch:    24/   30, ite: 172422] train loss: 0.315128, tar: 0.015867 \n",
      "It 172422\n",
      "l0: 0.006259, l1: 0.006256, l2: 0.006826, l3: 0.007371, l4: 0.008689, l5: 0.055087, l6: 0.084150\n",
      "\n",
      "[epoch: 21553/100000, batch:    28/   30, ite: 172423] train loss: 0.314796, tar: 0.015844 \n",
      "It 172423\n",
      "l0: 0.011015, l1: 0.011012, l2: 0.012009, l3: 0.012829, l4: 0.016264, l5: 0.029137, l6: 0.191124\n",
      "\n",
      "[epoch: 21553/100000, batch:    32/   30, ite: 172424] train loss: 0.314722, tar: 0.015833 \n",
      "It 172424\n",
      "l0: 0.008490, l1: 0.008486, l2: 0.009409, l3: 0.010214, l4: 0.013024, l5: 0.036921, l6: 0.126532\n",
      "\n",
      "[epoch: 21554/100000, batch:     4/   30, ite: 172425] train loss: 0.314483, tar: 0.015816 \n",
      "It 172425\n",
      "l0: 0.010288, l1: 0.010287, l2: 0.011160, l3: 0.011982, l4: 0.016489, l5: 0.063873, l6: 0.148992\n",
      "\n",
      "[epoch: 21554/100000, batch:     8/   30, ite: 172426] train loss: 0.314386, tar: 0.015803 \n",
      "It 172426\n",
      "l0: 0.035588, l1: 0.035472, l2: 0.036721, l3: 0.038426, l4: 0.041857, l5: 0.071603, l6: 0.251897\n",
      "\n",
      "[epoch: 21554/100000, batch:    12/   30, ite: 172427] train loss: 0.314847, tar: 0.015849 \n",
      "It 172427\n",
      "l0: 0.010793, l1: 0.010781, l2: 0.011843, l3: 0.012763, l4: 0.016126, l5: 0.044730, l6: 0.166148\n",
      "\n",
      "[epoch: 21554/100000, batch:    16/   30, ite: 172428] train loss: 0.314750, tar: 0.015837 \n",
      "It 172428\n",
      "l0: 0.009546, l1: 0.009543, l2: 0.010287, l3: 0.010960, l4: 0.013879, l5: 0.064389, l6: 0.171807\n",
      "\n",
      "[epoch: 21554/100000, batch:    20/   30, ite: 172429] train loss: 0.314693, tar: 0.015822 \n",
      "It 172429\n",
      "l0: 0.029976, l1: 0.030037, l2: 0.030992, l3: 0.033289, l4: 0.040292, l5: 0.074411, l6: 0.194953\n",
      "\n",
      "[epoch: 21554/100000, batch:    24/   30, ite: 172430] train loss: 0.314971, tar: 0.015855 \n",
      "It 172430\n",
      "l0: 0.007875, l1: 0.007872, l2: 0.008624, l3: 0.009240, l4: 0.011936, l5: 0.031730, l6: 0.133805\n",
      "\n",
      "[epoch: 21554/100000, batch:    28/   30, ite: 172431] train loss: 0.314730, tar: 0.015837 \n",
      "It 172431\n",
      "l0: 0.011709, l1: 0.011706, l2: 0.012586, l3: 0.013195, l4: 0.015791, l5: 0.056479, l6: 0.192852\n",
      "\n",
      "[epoch: 21554/100000, batch:    32/   30, ite: 172432] train loss: 0.314729, tar: 0.015827 \n",
      "It 172432\n",
      "l0: 0.015161, l1: 0.015160, l2: 0.016201, l3: 0.017267, l4: 0.022256, l5: 0.096879, l6: 0.262906\n",
      "\n",
      "[epoch: 21555/100000, batch:     4/   30, ite: 172433] train loss: 0.315031, tar: 0.015826 \n",
      "It 172433\n",
      "l0: 0.008894, l1: 0.008892, l2: 0.009761, l3: 0.010313, l4: 0.012500, l5: 0.024838, l6: 0.151917\n",
      "\n",
      "[epoch: 21555/100000, batch:     8/   30, ite: 172434] train loss: 0.314829, tar: 0.015810 \n",
      "It 172434\n",
      "l0: 0.007620, l1: 0.007614, l2: 0.008467, l3: 0.009255, l4: 0.011677, l5: 0.031751, l6: 0.118327\n",
      "\n",
      "[epoch: 21555/100000, batch:    12/   30, ite: 172435] train loss: 0.314553, tar: 0.015791 \n",
      "It 172435\n",
      "l0: 0.008908, l1: 0.008904, l2: 0.009599, l3: 0.010106, l4: 0.012539, l5: 0.048836, l6: 0.165631\n",
      "\n",
      "[epoch: 21555/100000, batch:    16/   30, ite: 172436] train loss: 0.314438, tar: 0.015775 \n",
      "It 172436\n",
      "l0: 0.007054, l1: 0.007052, l2: 0.007819, l3: 0.008270, l4: 0.009345, l5: 0.022298, l6: 0.089258\n",
      "\n",
      "[epoch: 21555/100000, batch:    20/   30, ite: 172437] train loss: 0.314064, tar: 0.015755 \n",
      "It 172437\n",
      "l0: 0.032188, l1: 0.032245, l2: 0.033165, l3: 0.035207, l4: 0.040301, l5: 0.066090, l6: 0.143775\n",
      "\n",
      "[epoch: 21555/100000, batch:    24/   30, ite: 172438] train loss: 0.314221, tar: 0.015793 \n",
      "It 172438\n",
      "l0: 0.032129, l1: 0.032021, l2: 0.033293, l3: 0.034818, l4: 0.038804, l5: 0.095964, l6: 0.252684\n",
      "\n",
      "[epoch: 21555/100000, batch:    28/   30, ite: 172439] train loss: 0.314690, tar: 0.015830 \n",
      "It 172439\n",
      "l0: 0.010548, l1: 0.010548, l2: 0.011373, l3: 0.011851, l4: 0.015789, l5: 0.112950, l6: 0.170412\n",
      "\n",
      "[epoch: 21555/100000, batch:    32/   30, ite: 172440] train loss: 0.314755, tar: 0.015818 \n",
      "It 172440\n",
      "l0: 0.032918, l1: 0.032861, l2: 0.034296, l3: 0.036990, l4: 0.042468, l5: 0.087297, l6: 0.221971\n",
      "\n",
      "[epoch: 21556/100000, batch:     4/   30, ite: 172441] train loss: 0.315150, tar: 0.015857 \n",
      "It 172441\n",
      "l0: 0.011245, l1: 0.011243, l2: 0.012231, l3: 0.013029, l4: 0.017005, l5: 0.048536, l6: 0.221598\n",
      "\n",
      "[epoch: 21556/100000, batch:     8/   30, ite: 172442] train loss: 0.315194, tar: 0.015846 \n",
      "It 172442\n",
      "l0: 0.011473, l1: 0.011473, l2: 0.012204, l3: 0.012737, l4: 0.014958, l5: 0.053570, l6: 0.225333\n",
      "\n",
      "[epoch: 21556/100000, batch:    12/   30, ite: 172443] train loss: 0.315254, tar: 0.015836 \n",
      "It 172443\n",
      "l0: 0.010945, l1: 0.010944, l2: 0.011745, l3: 0.012483, l4: 0.016801, l5: 0.043874, l6: 0.187424\n",
      "\n",
      "[epoch: 21556/100000, batch:    16/   30, ite: 172444] train loss: 0.315207, tar: 0.015825 \n",
      "It 172444\n",
      "l0: 0.009185, l1: 0.009183, l2: 0.010207, l3: 0.010859, l4: 0.014737, l5: 0.026551, l6: 0.098990\n",
      "\n",
      "[epoch: 21556/100000, batch:    20/   30, ite: 172445] train loss: 0.314902, tar: 0.015810 \n",
      "It 172445\n",
      "l0: 0.009622, l1: 0.009621, l2: 0.010287, l3: 0.011037, l4: 0.015927, l5: 0.084712, l6: 0.172693\n",
      "\n",
      "[epoch: 21556/100000, batch:    24/   30, ite: 172446] train loss: 0.314900, tar: 0.015797 \n",
      "It 172446\n",
      "l0: 0.004700, l1: 0.004699, l2: 0.005292, l3: 0.005594, l4: 0.006602, l5: 0.010736, l6: 0.057264\n",
      "\n",
      "[epoch: 21556/100000, batch:    28/   30, ite: 172447] train loss: 0.314408, tar: 0.015772 \n",
      "It 172447\n",
      "l0: 0.051005, l1: 0.051143, l2: 0.052301, l3: 0.056081, l4: 0.063147, l5: 0.097788, l6: 0.241462\n",
      "\n",
      "[epoch: 21556/100000, batch:    32/   30, ite: 172448] train loss: 0.315074, tar: 0.015850 \n",
      "It 172448\n",
      "l0: 0.011851, l1: 0.011849, l2: 0.013032, l3: 0.013978, l4: 0.016857, l5: 0.048961, l6: 0.198221\n",
      "\n",
      "[epoch: 21557/100000, batch:     4/   30, ite: 172449] train loss: 0.315074, tar: 0.015842 \n",
      "It 172449\n",
      "l0: 0.008466, l1: 0.008462, l2: 0.009230, l3: 0.009775, l4: 0.013509, l5: 0.043878, l6: 0.143366\n",
      "\n",
      "[epoch: 21557/100000, batch:     8/   30, ite: 172450] train loss: 0.314899, tar: 0.015825 \n",
      "It 172450\n",
      "l0: 0.009104, l1: 0.009094, l2: 0.009959, l3: 0.011125, l4: 0.014411, l5: 0.058362, l6: 0.159037\n",
      "\n",
      "[epoch: 21557/100000, batch:    12/   30, ite: 172451] train loss: 0.314802, tar: 0.015810 \n",
      "It 172451\n",
      "l0: 0.014914, l1: 0.014910, l2: 0.016173, l3: 0.017513, l4: 0.022348, l5: 0.083595, l6: 0.267761\n",
      "\n",
      "[epoch: 21557/100000, batch:    16/   30, ite: 172452] train loss: 0.315073, tar: 0.015808 \n",
      "It 172452\n",
      "l0: 0.007650, l1: 0.007646, l2: 0.008305, l3: 0.008879, l4: 0.012114, l5: 0.033600, l6: 0.131116\n",
      "\n",
      "[epoch: 21557/100000, batch:    20/   30, ite: 172453] train loss: 0.314840, tar: 0.015790 \n",
      "It 172453\n",
      "l0: 0.055352, l1: 0.055327, l2: 0.056964, l3: 0.059595, l4: 0.065737, l5: 0.112177, l6: 0.267017\n",
      "\n",
      "[epoch: 21557/100000, batch:    24/   30, ite: 172454] train loss: 0.315627, tar: 0.015877 \n",
      "It 172454\n",
      "l0: 0.005243, l1: 0.005247, l2: 0.005736, l3: 0.006144, l4: 0.007542, l5: 0.013971, l6: 0.068552\n",
      "\n",
      "[epoch: 21557/100000, batch:    28/   30, ite: 172455] train loss: 0.315180, tar: 0.015854 \n",
      "It 172455\n",
      "l0: 0.010207, l1: 0.010206, l2: 0.010983, l3: 0.011457, l4: 0.013216, l5: 0.027599, l6: 0.211028\n",
      "\n",
      "[epoch: 21557/100000, batch:    32/   30, ite: 172456] train loss: 0.315135, tar: 0.015842 \n",
      "It 172456\n",
      "l0: 0.033208, l1: 0.033316, l2: 0.034267, l3: 0.036550, l4: 0.043281, l5: 0.073791, l6: 0.185207\n",
      "\n",
      "[epoch: 21558/100000, batch:     4/   30, ite: 172457] train loss: 0.315408, tar: 0.015880 \n",
      "It 172457\n",
      "l0: 0.005699, l1: 0.005702, l2: 0.006406, l3: 0.006815, l4: 0.007822, l5: 0.017652, l6: 0.061074\n",
      "\n",
      "[epoch: 21558/100000, batch:     8/   30, ite: 172458] train loss: 0.314962, tar: 0.015857 \n",
      "It 172458\n",
      "l0: 0.033740, l1: 0.033639, l2: 0.035345, l3: 0.037660, l4: 0.042095, l5: 0.084357, l6: 0.220786\n",
      "\n",
      "[epoch: 21558/100000, batch:    12/   30, ite: 172459] train loss: 0.315338, tar: 0.015896 \n",
      "It 172459\n",
      "l0: 0.013594, l1: 0.013594, l2: 0.014595, l3: 0.015370, l4: 0.018451, l5: 0.064488, l6: 0.237225\n",
      "\n",
      "[epoch: 21558/100000, batch:    16/   30, ite: 172460] train loss: 0.315472, tar: 0.015891 \n",
      "It 172460\n",
      "l0: 0.004878, l1: 0.004877, l2: 0.005413, l3: 0.005704, l4: 0.007342, l5: 0.012936, l6: 0.069739\n",
      "\n",
      "[epoch: 21558/100000, batch:    20/   30, ite: 172461] train loss: 0.315029, tar: 0.015867 \n",
      "It 172461\n",
      "l0: 0.008348, l1: 0.008348, l2: 0.009175, l3: 0.009813, l4: 0.011700, l5: 0.026545, l6: 0.143571\n",
      "\n",
      "[epoch: 21558/100000, batch:    24/   30, ite: 172462] train loss: 0.314818, tar: 0.015851 \n",
      "It 172462\n",
      "l0: 0.012754, l1: 0.012752, l2: 0.013565, l3: 0.014255, l4: 0.019124, l5: 0.113469, l6: 0.254236\n",
      "\n",
      "[epoch: 21558/100000, batch:    28/   30, ite: 172463] train loss: 0.315088, tar: 0.015844 \n",
      "It 172463\n",
      "l0: 0.015901, l1: 0.015904, l2: 0.017243, l3: 0.018342, l4: 0.022793, l5: 0.099161, l6: 0.311930\n",
      "\n",
      "[epoch: 21558/100000, batch:    32/   30, ite: 172464] train loss: 0.315490, tar: 0.015845 \n",
      "It 172464\n",
      "l0: 0.038315, l1: 0.038408, l2: 0.039547, l3: 0.041504, l4: 0.048881, l5: 0.110270, l6: 0.362258\n",
      "\n",
      "[epoch: 21559/100000, batch:     4/   30, ite: 172465] train loss: 0.316272, tar: 0.015893 \n",
      "It 172465\n",
      "l0: 0.032139, l1: 0.032043, l2: 0.033987, l3: 0.037070, l4: 0.042626, l5: 0.087318, l6: 0.198038\n",
      "\n",
      "[epoch: 21559/100000, batch:     8/   30, ite: 172466] train loss: 0.316587, tar: 0.015928 \n",
      "It 172466\n",
      "l0: 0.006209, l1: 0.006204, l2: 0.006833, l3: 0.007149, l4: 0.008309, l5: 0.017523, l6: 0.088292\n",
      "\n",
      "[epoch: 21559/100000, batch:    12/   30, ite: 172467] train loss: 0.316210, tar: 0.015907 \n",
      "It 172467\n",
      "l0: 0.013124, l1: 0.013119, l2: 0.014193, l3: 0.015365, l4: 0.022524, l5: 0.113781, l6: 0.240381\n",
      "\n",
      "[epoch: 21559/100000, batch:    16/   30, ite: 172468] train loss: 0.316459, tar: 0.015901 \n",
      "It 172468\n",
      "l0: 0.010624, l1: 0.010619, l2: 0.011544, l3: 0.012301, l4: 0.015074, l5: 0.035855, l6: 0.192309\n",
      "\n",
      "[epoch: 21559/100000, batch:    20/   30, ite: 172469] train loss: 0.316399, tar: 0.015890 \n",
      "It 172469\n",
      "l0: 0.007828, l1: 0.007826, l2: 0.008679, l3: 0.009305, l4: 0.011824, l5: 0.024170, l6: 0.093256\n",
      "\n",
      "[epoch: 21559/100000, batch:    24/   30, ite: 172470] train loss: 0.316072, tar: 0.015873 \n",
      "It 172470\n",
      "l0: 0.009097, l1: 0.009099, l2: 0.009873, l3: 0.010521, l4: 0.013923, l5: 0.045183, l6: 0.130824\n",
      "\n",
      "[epoch: 21559/100000, batch:    28/   30, ite: 172471] train loss: 0.315886, tar: 0.015858 \n",
      "It 172471\n",
      "l0: 0.002188, l1: 0.002187, l2: 0.002556, l3: 0.002721, l4: 0.003131, l5: 0.006128, l6: 0.013277\n",
      "\n",
      "[epoch: 21559/100000, batch:    32/   30, ite: 172472] train loss: 0.315285, tar: 0.015829 \n",
      "It 172472\n",
      "l0: 0.010164, l1: 0.010167, l2: 0.011241, l3: 0.012151, l4: 0.014916, l5: 0.037499, l6: 0.161578\n",
      "\n",
      "[epoch: 21560/100000, batch:     4/   30, ite: 172473] train loss: 0.315163, tar: 0.015817 \n",
      "It 172473\n",
      "l0: 0.033317, l1: 0.033248, l2: 0.034968, l3: 0.037793, l4: 0.042521, l5: 0.102750, l6: 0.202648\n",
      "\n",
      "[epoch: 21560/100000, batch:     8/   30, ite: 172474] train loss: 0.315526, tar: 0.015854 \n",
      "It 172474\n",
      "l0: 0.006860, l1: 0.006863, l2: 0.007470, l3: 0.008090, l4: 0.009462, l5: 0.032080, l6: 0.103909\n",
      "\n",
      "[epoch: 21560/100000, batch:    12/   30, ite: 172475] train loss: 0.315230, tar: 0.015835 \n",
      "It 172475\n",
      "l0: 0.014329, l1: 0.014335, l2: 0.015375, l3: 0.016375, l4: 0.021323, l5: 0.095769, l6: 0.274141\n",
      "\n",
      "[epoch: 21560/100000, batch:    16/   30, ite: 172476] train loss: 0.315516, tar: 0.015832 \n",
      "It 172476\n",
      "l0: 0.008718, l1: 0.008722, l2: 0.009505, l3: 0.010022, l4: 0.011695, l5: 0.032425, l6: 0.137297\n",
      "\n",
      "[epoch: 21560/100000, batch:    20/   30, ite: 172477] train loss: 0.315313, tar: 0.015817 \n",
      "It 172477\n",
      "l0: 0.003769, l1: 0.003768, l2: 0.004211, l3: 0.004458, l4: 0.005854, l5: 0.009690, l6: 0.050753\n",
      "\n",
      "[epoch: 21560/100000, batch:    24/   30, ite: 172478] train loss: 0.314826, tar: 0.015792 \n",
      "It 172478\n",
      "l0: 0.038153, l1: 0.038296, l2: 0.039827, l3: 0.043571, l4: 0.052317, l5: 0.094512, l6: 0.253995\n",
      "\n",
      "[epoch: 21560/100000, batch:    28/   30, ite: 172479] train loss: 0.315339, tar: 0.015839 \n",
      "It 172479\n",
      "l0: 0.011432, l1: 0.011430, l2: 0.012497, l3: 0.013692, l4: 0.017574, l5: 0.042709, l6: 0.229979\n",
      "\n",
      "[epoch: 21560/100000, batch:    32/   30, ite: 172480] train loss: 0.315389, tar: 0.015830 \n",
      "It 172480\n",
      "l0: 0.007789, l1: 0.007783, l2: 0.008451, l3: 0.009150, l4: 0.012128, l5: 0.062878, l6: 0.117294\n",
      "\n",
      "[epoch: 21561/100000, batch:     4/   30, ite: 172481] train loss: 0.315202, tar: 0.015813 \n",
      "It 172481\n",
      "l0: 0.008216, l1: 0.008210, l2: 0.008997, l3: 0.009649, l4: 0.012658, l5: 0.025399, l6: 0.124426\n",
      "\n",
      "[epoch: 21561/100000, batch:     8/   30, ite: 172482] train loss: 0.314958, tar: 0.015797 \n",
      "It 172482\n",
      "l0: 0.004474, l1: 0.004472, l2: 0.005062, l3: 0.005576, l4: 0.007074, l5: 0.012227, l6: 0.046985\n",
      "\n",
      "[epoch: 21561/100000, batch:    12/   30, ite: 172483] train loss: 0.314484, tar: 0.015774 \n",
      "It 172483\n",
      "l0: 0.036671, l1: 0.036586, l2: 0.038106, l3: 0.040741, l4: 0.045856, l5: 0.085067, l6: 0.295343\n",
      "\n",
      "[epoch: 21561/100000, batch:    16/   30, ite: 172484] train loss: 0.315029, tar: 0.015817 \n",
      "It 172484\n",
      "l0: 0.006225, l1: 0.006219, l2: 0.007062, l3: 0.007608, l4: 0.009121, l5: 0.016489, l6: 0.071337\n",
      "\n",
      "[epoch: 21561/100000, batch:    20/   30, ite: 172485] train loss: 0.314635, tar: 0.015797 \n",
      "It 172485\n",
      "l0: 0.016950, l1: 0.016936, l2: 0.018124, l3: 0.019680, l4: 0.025106, l5: 0.108353, l6: 0.303162\n",
      "\n",
      "[epoch: 21561/100000, batch:    24/   30, ite: 172486] train loss: 0.315034, tar: 0.015799 \n",
      "It 172486\n",
      "l0: 0.036046, l1: 0.036174, l2: 0.037595, l3: 0.041105, l4: 0.048374, l5: 0.082705, l6: 0.234805\n",
      "\n",
      "[epoch: 21561/100000, batch:    28/   30, ite: 172487] train loss: 0.315448, tar: 0.015841 \n",
      "It 172487\n",
      "l0: 0.012211, l1: 0.012211, l2: 0.013449, l3: 0.014251, l4: 0.017536, l5: 0.030106, l6: 0.182201\n",
      "\n",
      "[epoch: 21561/100000, batch:    32/   30, ite: 172488] train loss: 0.315379, tar: 0.015833 \n",
      "It 172488\n",
      "l0: 0.011212, l1: 0.011208, l2: 0.011954, l3: 0.012574, l4: 0.015894, l5: 0.053624, l6: 0.214314\n",
      "\n",
      "[epoch: 21562/100000, batch:     4/   30, ite: 172489] train loss: 0.315411, tar: 0.015824 \n",
      "It 172489\n",
      "l0: 0.008057, l1: 0.008053, l2: 0.008784, l3: 0.009181, l4: 0.011348, l5: 0.029248, l6: 0.139717\n",
      "\n",
      "[epoch: 21562/100000, batch:     8/   30, ite: 172490] train loss: 0.315205, tar: 0.015808 \n",
      "It 172490\n",
      "l0: 0.009838, l1: 0.009836, l2: 0.010592, l3: 0.011131, l4: 0.014977, l5: 0.089076, l6: 0.163946\n",
      "\n",
      "[epoch: 21562/100000, batch:    12/   30, ite: 172491] train loss: 0.315193, tar: 0.015796 \n",
      "It 172491\n",
      "l0: 0.035690, l1: 0.035758, l2: 0.036895, l3: 0.039249, l4: 0.045487, l5: 0.088529, l6: 0.242613\n",
      "\n",
      "[epoch: 21562/100000, batch:    16/   30, ite: 172492] train loss: 0.315618, tar: 0.015836 \n",
      "It 172492\n",
      "l0: 0.009525, l1: 0.009511, l2: 0.010404, l3: 0.011151, l4: 0.013136, l5: 0.027740, l6: 0.146825\n",
      "\n",
      "[epoch: 21562/100000, batch:    20/   30, ite: 172493] train loss: 0.315441, tar: 0.015824 \n",
      "It 172493\n",
      "l0: 0.028781, l1: 0.028699, l2: 0.030122, l3: 0.032730, l4: 0.035687, l5: 0.061582, l6: 0.149375\n",
      "\n",
      "[epoch: 21562/100000, batch:    24/   30, ite: 172494] train loss: 0.315545, tar: 0.015850 \n",
      "It 172494\n",
      "l0: 0.005978, l1: 0.005976, l2: 0.006712, l3: 0.007157, l4: 0.008751, l5: 0.015726, l6: 0.078054\n",
      "\n",
      "[epoch: 21562/100000, batch:    28/   30, ite: 172495] train loss: 0.315167, tar: 0.015830 \n",
      "It 172495\n",
      "l0: 0.017661, l1: 0.017641, l2: 0.018955, l3: 0.020379, l4: 0.028187, l5: 0.109428, l6: 0.346763\n",
      "\n",
      "[epoch: 21562/100000, batch:    32/   30, ite: 172496] train loss: 0.315658, tar: 0.015834 \n",
      "It 172496\n",
      "l0: 0.012599, l1: 0.012591, l2: 0.013816, l3: 0.014804, l4: 0.017830, l5: 0.043635, l6: 0.191850\n",
      "\n",
      "[epoch: 21563/100000, batch:     4/   30, ite: 172497] train loss: 0.315641, tar: 0.015827 \n",
      "It 172497\n",
      "l0: 0.011651, l1: 0.011647, l2: 0.012618, l3: 0.013421, l4: 0.017226, l5: 0.051387, l6: 0.209337\n",
      "\n",
      "[epoch: 21563/100000, batch:     8/   30, ite: 172498] train loss: 0.315665, tar: 0.015819 \n",
      "It 172498\n",
      "l0: 0.031504, l1: 0.031426, l2: 0.032930, l3: 0.035784, l4: 0.040375, l5: 0.071067, l6: 0.175382\n",
      "\n",
      "[epoch: 21563/100000, batch:    12/   30, ite: 172499] train loss: 0.315871, tar: 0.015850 \n",
      "It 172499\n",
      "l0: 0.005846, l1: 0.005845, l2: 0.006358, l3: 0.006947, l4: 0.008717, l5: 0.032441, l6: 0.086497\n",
      "\n",
      "[epoch: 21563/100000, batch:    16/   30, ite: 172500] train loss: 0.315544, tar: 0.015830 \n",
      "It 172500\n",
      "l0: 0.006676, l1: 0.006681, l2: 0.007294, l3: 0.007799, l4: 0.009386, l5: 0.026399, l6: 0.087588\n",
      "\n",
      "[epoch: 21563/100000, batch:    20/   30, ite: 172501] train loss: 0.315217, tar: 0.015812 \n",
      "It 172501\n",
      "l0: 0.006549, l1: 0.006555, l2: 0.007204, l3: 0.007690, l4: 0.009106, l5: 0.018966, l6: 0.115767\n",
      "\n",
      "[epoch: 21563/100000, batch:    24/   30, ite: 172502] train loss: 0.314932, tar: 0.015793 \n",
      "It 172502\n",
      "l0: 0.012896, l1: 0.012901, l2: 0.013781, l3: 0.014598, l4: 0.019194, l5: 0.075968, l6: 0.235743\n",
      "\n",
      "[epoch: 21563/100000, batch:    28/   30, ite: 172503] train loss: 0.315071, tar: 0.015788 \n",
      "It 172503\n",
      "l0: 0.062198, l1: 0.062413, l2: 0.063990, l3: 0.069301, l4: 0.079691, l5: 0.140043, l6: 0.407359\n",
      "\n",
      "[epoch: 21563/100000, batch:    32/   30, ite: 172504] train loss: 0.316202, tar: 0.015880 \n",
      "It 172504\n",
      "l0: 0.013810, l1: 0.013810, l2: 0.014847, l3: 0.016093, l4: 0.022400, l5: 0.107521, l6: 0.253265\n",
      "\n",
      "[epoch: 21564/100000, batch:     4/   30, ite: 172505] train loss: 0.316451, tar: 0.015876 \n",
      "It 172505\n",
      "l0: 0.035662, l1: 0.035579, l2: 0.037301, l3: 0.040329, l4: 0.046706, l5: 0.080684, l6: 0.218151\n",
      "\n",
      "[epoch: 21564/100000, batch:     8/   30, ite: 172506] train loss: 0.316802, tar: 0.015915 \n",
      "It 172506\n",
      "l0: 0.014114, l1: 0.014109, l2: 0.015213, l3: 0.015973, l4: 0.018616, l5: 0.066190, l6: 0.269583\n",
      "\n",
      "[epoch: 21564/100000, batch:    12/   30, ite: 172507] train loss: 0.316994, tar: 0.015911 \n",
      "It 172507\n",
      "l0: 0.029394, l1: 0.029493, l2: 0.030320, l3: 0.032322, l4: 0.038049, l5: 0.058518, l6: 0.120528\n",
      "\n",
      "[epoch: 21564/100000, batch:    16/   30, ite: 172508] train loss: 0.317036, tar: 0.015938 \n",
      "It 172508\n",
      "l0: 0.009758, l1: 0.009757, l2: 0.010648, l3: 0.011472, l4: 0.013671, l5: 0.028214, l6: 0.182694\n",
      "\n",
      "[epoch: 21564/100000, batch:    20/   30, ite: 172509] train loss: 0.316936, tar: 0.015926 \n",
      "It 172509\n",
      "l0: 0.007753, l1: 0.007753, l2: 0.008592, l3: 0.009170, l4: 0.011587, l5: 0.029714, l6: 0.091843\n",
      "\n",
      "[epoch: 21564/100000, batch:    24/   30, ite: 172510] train loss: 0.316641, tar: 0.015910 \n",
      "It 172510\n",
      "l0: 0.009127, l1: 0.009125, l2: 0.009874, l3: 0.010747, l4: 0.015257, l5: 0.046337, l6: 0.142952\n",
      "\n",
      "[epoch: 21564/100000, batch:    28/   30, ite: 172511] train loss: 0.316498, tar: 0.015896 \n",
      "It 172511\n",
      "l0: 0.003919, l1: 0.003919, l2: 0.004364, l3: 0.004810, l4: 0.006314, l5: 0.009071, l6: 0.037126\n",
      "\n",
      "[epoch: 21564/100000, batch:    32/   30, ite: 172512] train loss: 0.316016, tar: 0.015873 \n",
      "It 172512\n",
      "l0: 0.009907, l1: 0.009909, l2: 0.010839, l3: 0.011628, l4: 0.016177, l5: 0.045651, l6: 0.134610\n",
      "\n",
      "[epoch: 21565/100000, batch:     4/   30, ite: 172513] train loss: 0.315865, tar: 0.015861 \n",
      "It 172513\n",
      "l0: 0.008085, l1: 0.008082, l2: 0.008804, l3: 0.009397, l4: 0.012479, l5: 0.024061, l6: 0.146935\n",
      "\n",
      "[epoch: 21565/100000, batch:     8/   30, ite: 172514] train loss: 0.315674, tar: 0.015846 \n",
      "It 172514\n",
      "l0: 0.054625, l1: 0.054507, l2: 0.056069, l3: 0.058353, l4: 0.062210, l5: 0.124734, l6: 0.248281\n",
      "\n",
      "[epoch: 21565/100000, batch:    12/   30, ite: 172515] train loss: 0.316340, tar: 0.015921 \n",
      "It 172515\n",
      "l0: 0.012770, l1: 0.012774, l2: 0.013737, l3: 0.014662, l4: 0.019499, l5: 0.063481, l6: 0.243888\n",
      "\n",
      "[epoch: 21565/100000, batch:    16/   30, ite: 172516] train loss: 0.316465, tar: 0.015915 \n",
      "It 172516\n",
      "l0: 0.007879, l1: 0.007881, l2: 0.008817, l3: 0.009415, l4: 0.011656, l5: 0.020828, l6: 0.103903\n",
      "\n",
      "[epoch: 21565/100000, batch:    20/   30, ite: 172517] train loss: 0.316183, tar: 0.015900 \n",
      "It 172517\n",
      "l0: 0.009979, l1: 0.009978, l2: 0.010794, l3: 0.011457, l4: 0.015231, l5: 0.071109, l6: 0.184333\n",
      "\n",
      "[epoch: 21565/100000, batch:    24/   30, ite: 172518] train loss: 0.316176, tar: 0.015888 \n",
      "It 172518\n",
      "l0: 0.009598, l1: 0.009595, l2: 0.010613, l3: 0.011345, l4: 0.013764, l5: 0.040348, l6: 0.151100\n",
      "\n",
      "[epoch: 21565/100000, batch:    28/   30, ite: 172519] train loss: 0.316042, tar: 0.015876 \n",
      "It 172519\n",
      "l0: 0.009489, l1: 0.009482, l2: 0.010148, l3: 0.010621, l4: 0.012344, l5: 0.030678, l6: 0.204884\n",
      "\n",
      "[epoch: 21565/100000, batch:    32/   30, ite: 172520] train loss: 0.315987, tar: 0.015864 \n",
      "It 172520\n",
      "l0: 0.010969, l1: 0.010969, l2: 0.011836, l3: 0.012698, l4: 0.016518, l5: 0.050008, l6: 0.162235\n",
      "\n",
      "[epoch: 21566/100000, batch:     4/   30, ite: 172521] train loss: 0.315909, tar: 0.015855 \n",
      "It 172521\n",
      "l0: 0.010953, l1: 0.010949, l2: 0.011825, l3: 0.012361, l4: 0.014681, l5: 0.050971, l6: 0.215521\n",
      "\n",
      "[epoch: 21566/100000, batch:     8/   30, ite: 172522] train loss: 0.315931, tar: 0.015845 \n",
      "It 172522\n",
      "l0: 0.031354, l1: 0.031484, l2: 0.032281, l3: 0.035080, l4: 0.040745, l5: 0.064224, l6: 0.171697\n",
      "\n",
      "[epoch: 21566/100000, batch:    12/   30, ite: 172523] train loss: 0.316105, tar: 0.015875 \n",
      "It 172523\n",
      "l0: 0.010302, l1: 0.010304, l2: 0.011178, l3: 0.012410, l4: 0.016762, l5: 0.059821, l6: 0.187285\n",
      "\n",
      "[epoch: 21566/100000, batch:    16/   30, ite: 172524] train loss: 0.316089, tar: 0.015864 \n",
      "It 172524\n",
      "l0: 0.036490, l1: 0.036401, l2: 0.038476, l3: 0.040902, l4: 0.048244, l5: 0.128114, l6: 0.258068\n",
      "\n",
      "[epoch: 21566/100000, batch:    20/   30, ite: 172525] train loss: 0.316605, tar: 0.015904 \n",
      "It 172525\n",
      "l0: 0.010228, l1: 0.010227, l2: 0.011086, l3: 0.011855, l4: 0.014325, l5: 0.042271, l6: 0.187249\n",
      "\n",
      "[epoch: 21566/100000, batch:    24/   30, ite: 172526] train loss: 0.316549, tar: 0.015893 \n",
      "It 172526\n",
      "l0: 0.006716, l1: 0.006716, l2: 0.007388, l3: 0.007767, l4: 0.009876, l5: 0.015885, l6: 0.094011\n",
      "\n",
      "[epoch: 21566/100000, batch:    28/   30, ite: 172527] train loss: 0.316230, tar: 0.015875 \n",
      "It 172527\n",
      "l0: 0.003072, l1: 0.003070, l2: 0.003496, l3: 0.003734, l4: 0.004587, l5: 0.008414, l6: 0.039781\n",
      "\n",
      "[epoch: 21566/100000, batch:    32/   30, ite: 172528] train loss: 0.315756, tar: 0.015851 \n",
      "It 172528\n",
      "l0: 0.030350, l1: 0.030250, l2: 0.032000, l3: 0.034254, l4: 0.036402, l5: 0.057155, l6: 0.166391\n",
      "\n",
      "[epoch: 21567/100000, batch:     4/   30, ite: 172529] train loss: 0.315890, tar: 0.015878 \n",
      "It 172529\n",
      "l0: 0.010851, l1: 0.010830, l2: 0.011701, l3: 0.012479, l4: 0.015527, l5: 0.062136, l6: 0.205654\n",
      "\n",
      "[epoch: 21567/100000, batch:     8/   30, ite: 172530] train loss: 0.315916, tar: 0.015869 \n",
      "It 172530\n",
      "l0: 0.005240, l1: 0.005241, l2: 0.005693, l3: 0.005941, l4: 0.006887, l5: 0.020131, l6: 0.079806\n",
      "\n",
      "[epoch: 21567/100000, batch:    12/   30, ite: 172531] train loss: 0.315563, tar: 0.015849 \n",
      "It 172531\n",
      "l0: 0.013681, l1: 0.013686, l2: 0.014890, l3: 0.016329, l4: 0.021943, l5: 0.071092, l6: 0.206973\n",
      "\n",
      "[epoch: 21567/100000, batch:    16/   30, ite: 172532] train loss: 0.315644, tar: 0.015845 \n",
      "It 172532\n",
      "l0: 0.008266, l1: 0.008270, l2: 0.009210, l3: 0.009896, l4: 0.012415, l5: 0.024870, l6: 0.114499\n",
      "\n",
      "[epoch: 21567/100000, batch:    20/   30, ite: 172533] train loss: 0.315404, tar: 0.015831 \n",
      "It 172533\n",
      "l0: 0.011048, l1: 0.011047, l2: 0.011930, l3: 0.012569, l4: 0.017683, l5: 0.054528, l6: 0.191392\n",
      "\n",
      "[epoch: 21567/100000, batch:    24/   30, ite: 172534] train loss: 0.315394, tar: 0.015822 \n",
      "It 172534\n",
      "l0: 0.034192, l1: 0.034283, l2: 0.035009, l3: 0.037292, l4: 0.042063, l5: 0.083564, l6: 0.205258\n",
      "\n",
      "[epoch: 21567/100000, batch:    28/   30, ite: 172535] train loss: 0.315686, tar: 0.015856 \n",
      "It 172535\n",
      "l0: 0.015404, l1: 0.015395, l2: 0.016557, l3: 0.017639, l4: 0.021816, l5: 0.059503, l6: 0.312926\n",
      "\n",
      "[epoch: 21567/100000, batch:    32/   30, ite: 172536] train loss: 0.315954, tar: 0.015855 \n",
      "It 172536\n",
      "l0: 0.006982, l1: 0.006979, l2: 0.007656, l3: 0.007985, l4: 0.010094, l5: 0.042603, l6: 0.080334\n",
      "\n",
      "[epoch: 21568/100000, batch:     4/   30, ite: 172537] train loss: 0.315668, tar: 0.015839 \n",
      "It 172537\n",
      "l0: 0.030709, l1: 0.030783, l2: 0.031525, l3: 0.033591, l4: 0.038421, l5: 0.065259, l6: 0.174834\n",
      "\n",
      "[epoch: 21568/100000, batch:     8/   30, ite: 172538] train loss: 0.315835, tar: 0.015866 \n",
      "It 172538\n",
      "l0: 0.037202, l1: 0.037078, l2: 0.038837, l3: 0.041231, l4: 0.044737, l5: 0.084372, l6: 0.237176\n",
      "\n",
      "[epoch: 21568/100000, batch:    12/   30, ite: 172539] train loss: 0.316215, tar: 0.015906 \n",
      "It 172539\n",
      "l0: 0.013076, l1: 0.013070, l2: 0.014067, l3: 0.014955, l4: 0.019671, l5: 0.059398, l6: 0.245456\n",
      "\n",
      "[epoch: 21568/100000, batch:    16/   30, ite: 172540] train loss: 0.316332, tar: 0.015901 \n",
      "It 172540\n",
      "l0: 0.008524, l1: 0.008522, l2: 0.009256, l3: 0.009755, l4: 0.013040, l5: 0.057242, l6: 0.153749\n",
      "\n",
      "[epoch: 21568/100000, batch:    20/   30, ite: 172541] train loss: 0.316228, tar: 0.015887 \n",
      "It 172541\n",
      "l0: 0.010609, l1: 0.010611, l2: 0.011645, l3: 0.012673, l4: 0.016930, l5: 0.037056, l6: 0.167405\n",
      "\n",
      "[epoch: 21568/100000, batch:    24/   30, ite: 172542] train loss: 0.316137, tar: 0.015877 \n",
      "It 172542\n",
      "l0: 0.007612, l1: 0.007612, l2: 0.008348, l3: 0.008879, l4: 0.011051, l5: 0.030577, l6: 0.113054\n",
      "\n",
      "[epoch: 21568/100000, batch:    28/   30, ite: 172543] train loss: 0.315900, tar: 0.015862 \n",
      "It 172543\n",
      "l0: 0.012479, l1: 0.012480, l2: 0.013506, l3: 0.014409, l4: 0.017755, l5: 0.047145, l6: 0.230182\n",
      "\n",
      "[epoch: 21568/100000, batch:    32/   30, ite: 172544] train loss: 0.315959, tar: 0.015856 \n",
      "It 172544\n",
      "l0: 0.010300, l1: 0.010304, l2: 0.011312, l3: 0.012101, l4: 0.015801, l5: 0.054942, l6: 0.157159\n",
      "\n",
      "[epoch: 21569/100000, batch:     4/   30, ite: 172545] train loss: 0.315878, tar: 0.015846 \n",
      "It 172545\n",
      "l0: 0.037460, l1: 0.037348, l2: 0.039083, l3: 0.041187, l4: 0.044873, l5: 0.111800, l6: 0.319252\n",
      "\n",
      "[epoch: 21569/100000, batch:     8/   30, ite: 172546] train loss: 0.316455, tar: 0.015885 \n",
      "It 172546\n",
      "l0: 0.007059, l1: 0.007063, l2: 0.007721, l3: 0.008348, l4: 0.011527, l5: 0.064549, l6: 0.086784\n",
      "\n",
      "[epoch: 21569/100000, batch:    12/   30, ite: 172547] train loss: 0.316229, tar: 0.015869 \n",
      "It 172547\n",
      "l0: 0.028859, l1: 0.028979, l2: 0.029827, l3: 0.032156, l4: 0.038100, l5: 0.066739, l6: 0.156118\n",
      "\n",
      "[epoch: 21569/100000, batch:    16/   30, ite: 172548] train loss: 0.316347, tar: 0.015893 \n",
      "It 172548\n",
      "l0: 0.010449, l1: 0.010445, l2: 0.011336, l3: 0.012129, l4: 0.017457, l5: 0.041972, l6: 0.173099\n",
      "\n",
      "[epoch: 21569/100000, batch:    20/   30, ite: 172549] train loss: 0.316275, tar: 0.015883 \n",
      "It 172549\n",
      "l0: 0.006726, l1: 0.006722, l2: 0.007399, l3: 0.007859, l4: 0.009677, l5: 0.027828, l6: 0.106258\n",
      "\n",
      "[epoch: 21569/100000, batch:    24/   30, ite: 172550] train loss: 0.316014, tar: 0.015866 \n",
      "It 172550\n",
      "l0: 0.011273, l1: 0.011264, l2: 0.011991, l3: 0.012435, l4: 0.014344, l5: 0.059527, l6: 0.207862\n",
      "\n",
      "[epoch: 21569/100000, batch:    28/   30, ite: 172551] train loss: 0.316037, tar: 0.015858 \n",
      "It 172551\n",
      "l0: 0.009737, l1: 0.009732, l2: 0.010804, l3: 0.011604, l4: 0.014621, l5: 0.030857, l6: 0.121605\n",
      "\n",
      "[epoch: 21569/100000, batch:    32/   30, ite: 172552] train loss: 0.315843, tar: 0.015847 \n",
      "It 172552\n",
      "l0: 0.011939, l1: 0.011934, l2: 0.012826, l3: 0.013983, l4: 0.018540, l5: 0.054530, l6: 0.224136\n",
      "\n",
      "[epoch: 21570/100000, batch:     4/   30, ite: 172553] train loss: 0.315901, tar: 0.015840 \n",
      "It 172553\n",
      "l0: 0.006237, l1: 0.006235, l2: 0.006943, l3: 0.007607, l4: 0.010097, l5: 0.026505, l6: 0.094937\n",
      "\n",
      "[epoch: 21570/100000, batch:     8/   30, ite: 172554] train loss: 0.315617, tar: 0.015822 \n",
      "It 172554\n",
      "l0: 0.006301, l1: 0.006300, l2: 0.007159, l3: 0.007680, l4: 0.009175, l5: 0.018817, l6: 0.070112\n",
      "\n",
      "[epoch: 21570/100000, batch:    12/   30, ite: 172555] train loss: 0.315274, tar: 0.015805 \n",
      "It 172555\n",
      "l0: 0.013606, l1: 0.013607, l2: 0.014622, l3: 0.015580, l4: 0.020569, l5: 0.075743, l6: 0.254795\n",
      "\n",
      "[epoch: 21570/100000, batch:    16/   30, ite: 172556] train loss: 0.315442, tar: 0.015801 \n",
      "It 172556\n",
      "l0: 0.007681, l1: 0.007682, l2: 0.008482, l3: 0.008941, l4: 0.010991, l5: 0.019388, l6: 0.109653\n",
      "\n",
      "[epoch: 21570/100000, batch:    20/   30, ite: 172557] train loss: 0.315186, tar: 0.015787 \n",
      "It 172557\n",
      "l0: 0.032956, l1: 0.033066, l2: 0.033818, l3: 0.036319, l4: 0.040991, l5: 0.098569, l6: 0.194983\n",
      "\n",
      "[epoch: 21570/100000, batch:    24/   30, ite: 172558] train loss: 0.315465, tar: 0.015818 \n",
      "It 172558\n",
      "l0: 0.012656, l1: 0.012659, l2: 0.013587, l3: 0.014457, l4: 0.017397, l5: 0.052613, l6: 0.248400\n",
      "\n",
      "[epoch: 21570/100000, batch:    28/   30, ite: 172559] train loss: 0.315565, tar: 0.015812 \n",
      "It 172559\n",
      "l0: 0.063830, l1: 0.063625, l2: 0.066605, l3: 0.072184, l4: 0.084133, l5: 0.118546, l6: 0.288008\n",
      "\n",
      "[epoch: 21570/100000, batch:    32/   30, ite: 172560] train loss: 0.316354, tar: 0.015898 \n",
      "It 172560\n",
      "l0: 0.012195, l1: 0.012198, l2: 0.013134, l3: 0.014075, l4: 0.019052, l5: 0.064236, l6: 0.214332\n",
      "\n",
      "[epoch: 21571/100000, batch:     4/   30, ite: 172561] train loss: 0.316412, tar: 0.015891 \n",
      "It 172561\n",
      "l0: 0.010656, l1: 0.010659, l2: 0.011581, l3: 0.012306, l4: 0.014926, l5: 0.036086, l6: 0.195632\n",
      "\n",
      "[epoch: 21571/100000, batch:     8/   30, ite: 172562] train loss: 0.316368, tar: 0.015882 \n",
      "It 172562\n",
      "l0: 0.013115, l1: 0.013112, l2: 0.014218, l3: 0.015094, l4: 0.018486, l5: 0.070930, l6: 0.226970\n",
      "\n",
      "[epoch: 21571/100000, batch:    12/   30, ite: 172563] train loss: 0.316467, tar: 0.015877 \n",
      "It 172563\n",
      "l0: 0.008360, l1: 0.008358, l2: 0.009088, l3: 0.009892, l4: 0.012728, l5: 0.041549, l6: 0.124427\n",
      "\n",
      "[epoch: 21571/100000, batch:    16/   30, ite: 172564] train loss: 0.316286, tar: 0.015863 \n",
      "It 172564\n",
      "l0: 0.032651, l1: 0.032747, l2: 0.033713, l3: 0.036073, l4: 0.042748, l5: 0.082006, l6: 0.226769\n",
      "\n",
      "[epoch: 21571/100000, batch:    20/   30, ite: 172565] train loss: 0.316588, tar: 0.015893 \n",
      "It 172565\n",
      "l0: 0.024432, l1: 0.024337, l2: 0.025474, l3: 0.027058, l4: 0.029592, l5: 0.040540, l6: 0.092925\n",
      "\n",
      "[epoch: 21571/100000, batch:    24/   30, ite: 172566] train loss: 0.316495, tar: 0.015908 \n",
      "It 172566\n",
      "l0: 0.009163, l1: 0.009162, l2: 0.010013, l3: 0.010562, l4: 0.013209, l5: 0.072332, l6: 0.138226\n",
      "\n",
      "[epoch: 21571/100000, batch:    28/   30, ite: 172567] train loss: 0.316401, tar: 0.015896 \n",
      "It 172567\n",
      "l0: 0.006467, l1: 0.006461, l2: 0.007381, l3: 0.007986, l4: 0.010240, l5: 0.017186, l6: 0.056875\n",
      "\n",
      "[epoch: 21571/100000, batch:    32/   30, ite: 172568] train loss: 0.316042, tar: 0.015880 \n",
      "It 172568\n",
      "l0: 0.012015, l1: 0.012014, l2: 0.013045, l3: 0.013842, l4: 0.017732, l5: 0.053369, l6: 0.205627\n",
      "\n",
      "[epoch: 21572/100000, batch:     4/   30, ite: 172569] train loss: 0.316062, tar: 0.015873 \n",
      "It 172569\n",
      "l0: 0.036236, l1: 0.036328, l2: 0.037650, l3: 0.039589, l4: 0.045770, l5: 0.080041, l6: 0.273472\n",
      "\n",
      "[epoch: 21572/100000, batch:     8/   30, ite: 172570] train loss: 0.316471, tar: 0.015909 \n",
      "It 172570\n",
      "l0: 0.007045, l1: 0.007043, l2: 0.007724, l3: 0.008243, l4: 0.010810, l5: 0.024234, l6: 0.113049\n",
      "\n",
      "[epoch: 21572/100000, batch:    12/   30, ite: 172571] train loss: 0.316229, tar: 0.015893 \n",
      "It 172571\n",
      "l0: 0.011386, l1: 0.011380, l2: 0.012503, l3: 0.013259, l4: 0.015704, l5: 0.040526, l6: 0.195307\n",
      "\n",
      "[epoch: 21572/100000, batch:    16/   30, ite: 172572] train loss: 0.316200, tar: 0.015885 \n",
      "It 172572\n",
      "l0: 0.030374, l1: 0.030240, l2: 0.031641, l3: 0.032544, l4: 0.034974, l5: 0.104902, l6: 0.180279\n",
      "\n",
      "[epoch: 21572/100000, batch:    20/   30, ite: 172573] train loss: 0.316425, tar: 0.015911 \n",
      "It 172573\n",
      "l0: 0.009306, l1: 0.009300, l2: 0.010038, l3: 0.010565, l4: 0.012889, l5: 0.044542, l6: 0.152387\n",
      "\n",
      "[epoch: 21572/100000, batch:    24/   30, ite: 172574] train loss: 0.316308, tar: 0.015899 \n",
      "It 172574\n",
      "l0: 0.006430, l1: 0.006426, l2: 0.007007, l3: 0.007488, l4: 0.009005, l5: 0.030884, l6: 0.107253\n",
      "\n",
      "[epoch: 21572/100000, batch:    28/   30, ite: 172575] train loss: 0.316061, tar: 0.015883 \n",
      "It 172575\n",
      "l0: 0.011940, l1: 0.011941, l2: 0.012873, l3: 0.014015, l4: 0.021196, l5: 0.084923, l6: 0.177156\n",
      "\n",
      "[epoch: 21572/100000, batch:    32/   30, ite: 172576] train loss: 0.316092, tar: 0.015876 \n",
      "It 172576\n",
      "l0: 0.004601, l1: 0.004603, l2: 0.005121, l3: 0.005370, l4: 0.006480, l5: 0.015699, l6: 0.050316\n",
      "\n",
      "[epoch: 21573/100000, batch:     4/   30, ite: 172577] train loss: 0.315704, tar: 0.015856 \n",
      "It 172577\n",
      "l0: 0.014418, l1: 0.014421, l2: 0.015412, l3: 0.016758, l4: 0.021707, l5: 0.100675, l6: 0.276804\n",
      "\n",
      "[epoch: 21573/100000, batch:     8/   30, ite: 172578] train loss: 0.315954, tar: 0.015854 \n",
      "It 172578\n",
      "l0: 0.006380, l1: 0.006379, l2: 0.007027, l3: 0.007570, l4: 0.009121, l5: 0.016269, l6: 0.106523\n",
      "\n",
      "[epoch: 21573/100000, batch:    12/   30, ite: 172579] train loss: 0.315684, tar: 0.015837 \n",
      "It 172579\n",
      "l0: 0.032485, l1: 0.032572, l2: 0.033846, l3: 0.036785, l4: 0.041128, l5: 0.073542, l6: 0.201468\n",
      "\n",
      "[epoch: 21573/100000, batch:    16/   30, ite: 172580] train loss: 0.315918, tar: 0.015866 \n",
      "It 172580\n",
      "l0: 0.036686, l1: 0.036584, l2: 0.038265, l3: 0.039986, l4: 0.047444, l5: 0.091455, l6: 0.252588\n",
      "\n",
      "[epoch: 21573/100000, batch:    20/   30, ite: 172581] train loss: 0.316309, tar: 0.015902 \n",
      "It 172581\n",
      "l0: 0.012810, l1: 0.012808, l2: 0.013712, l3: 0.014676, l4: 0.019656, l5: 0.073962, l6: 0.228037\n",
      "\n",
      "[epoch: 21573/100000, batch:    24/   30, ite: 172582] train loss: 0.316411, tar: 0.015897 \n",
      "It 172582\n",
      "l0: 0.006262, l1: 0.006261, l2: 0.007001, l3: 0.007415, l4: 0.008804, l5: 0.019236, l6: 0.065026\n",
      "\n",
      "[epoch: 21573/100000, batch:    28/   30, ite: 172583] train loss: 0.316074, tar: 0.015880 \n",
      "It 172583\n",
      "l0: 0.008628, l1: 0.008624, l2: 0.009285, l3: 0.009664, l4: 0.011810, l5: 0.030337, l6: 0.151183\n",
      "\n",
      "[epoch: 21573/100000, batch:    32/   30, ite: 172584] train loss: 0.315926, tar: 0.015868 \n",
      "It 172584\n",
      "l0: 0.008878, l1: 0.008875, l2: 0.009773, l3: 0.010610, l4: 0.014149, l5: 0.031152, l6: 0.150058\n",
      "\n",
      "[epoch: 21574/100000, batch:     4/   30, ite: 172585] train loss: 0.315785, tar: 0.015856 \n",
      "It 172585\n",
      "l0: 0.028863, l1: 0.028802, l2: 0.029943, l3: 0.031563, l4: 0.035069, l5: 0.073658, l6: 0.178244\n",
      "\n",
      "[epoch: 21574/100000, batch:     8/   30, ite: 172586] train loss: 0.315939, tar: 0.015878 \n",
      "It 172586\n",
      "l0: 0.012294, l1: 0.012292, l2: 0.013225, l3: 0.014045, l4: 0.019334, l5: 0.068098, l6: 0.228090\n",
      "\n",
      "[epoch: 21574/100000, batch:    12/   30, ite: 172587] train loss: 0.316027, tar: 0.015872 \n",
      "It 172587\n",
      "l0: 0.008248, l1: 0.008247, l2: 0.008888, l3: 0.009492, l4: 0.011068, l5: 0.047733, l6: 0.111402\n",
      "\n",
      "[epoch: 21574/100000, batch:    16/   30, ite: 172588] train loss: 0.315838, tar: 0.015859 \n",
      "It 172588\n",
      "l0: 0.010216, l1: 0.010215, l2: 0.011143, l3: 0.011789, l4: 0.015085, l5: 0.071490, l6: 0.171485\n",
      "\n",
      "[epoch: 21574/100000, batch:    20/   30, ite: 172589] train loss: 0.315814, tar: 0.015849 \n",
      "It 172589\n",
      "l0: 0.035054, l1: 0.035148, l2: 0.036224, l3: 0.039256, l4: 0.044767, l5: 0.082724, l6: 0.215499\n",
      "\n",
      "[epoch: 21574/100000, batch:    24/   30, ite: 172590] train loss: 0.316107, tar: 0.015882 \n",
      "It 172590\n",
      "l0: 0.008315, l1: 0.008315, l2: 0.009166, l3: 0.009762, l4: 0.012138, l5: 0.036708, l6: 0.139789\n",
      "\n",
      "[epoch: 21574/100000, batch:    28/   30, ite: 172591] train loss: 0.315951, tar: 0.015869 \n",
      "It 172591\n",
      "l0: 0.012059, l1: 0.012057, l2: 0.013026, l3: 0.014022, l4: 0.019296, l5: 0.070271, l6: 0.165025\n",
      "\n",
      "[epoch: 21574/100000, batch:    32/   30, ite: 172592] train loss: 0.315934, tar: 0.015863 \n",
      "It 172592\n",
      "l0: 0.011897, l1: 0.011894, l2: 0.012917, l3: 0.013650, l4: 0.017886, l5: 0.056156, l6: 0.222896\n",
      "\n",
      "[epoch: 21575/100000, batch:     4/   30, ite: 172593] train loss: 0.315987, tar: 0.015856 \n",
      "It 172593\n",
      "l0: 0.011002, l1: 0.010998, l2: 0.011815, l3: 0.012589, l4: 0.015498, l5: 0.051691, l6: 0.192939\n",
      "\n",
      "[epoch: 21575/100000, batch:     8/   30, ite: 172594] train loss: 0.315971, tar: 0.015848 \n",
      "It 172594\n",
      "l0: 0.006645, l1: 0.006645, l2: 0.007205, l3: 0.007500, l4: 0.009181, l5: 0.020144, l6: 0.122694\n",
      "\n",
      "[epoch: 21575/100000, batch:    12/   30, ite: 172595] train loss: 0.315743, tar: 0.015832 \n",
      "It 172595\n",
      "l0: 0.010416, l1: 0.010414, l2: 0.011534, l3: 0.012483, l4: 0.016114, l5: 0.054164, l6: 0.139785\n",
      "\n",
      "[epoch: 21575/100000, batch:    16/   30, ite: 172596] train loss: 0.315641, tar: 0.015823 \n",
      "It 172596\n",
      "l0: 0.012665, l1: 0.012666, l2: 0.013513, l3: 0.014409, l4: 0.018215, l5: 0.090249, l6: 0.257562\n",
      "\n",
      "[epoch: 21575/100000, batch:    20/   30, ite: 172597] train loss: 0.315814, tar: 0.015818 \n",
      "It 172597\n",
      "l0: 0.006466, l1: 0.006465, l2: 0.007104, l3: 0.007560, l4: 0.008850, l5: 0.025679, l6: 0.099617\n",
      "\n",
      "[epoch: 21575/100000, batch:    24/   30, ite: 172598] train loss: 0.315556, tar: 0.015802 \n",
      "It 172598\n",
      "l0: 0.028279, l1: 0.028343, l2: 0.029290, l3: 0.031246, l4: 0.035405, l5: 0.055002, l6: 0.112822\n",
      "\n",
      "[epoch: 21575/100000, batch:    28/   30, ite: 172599] train loss: 0.315565, tar: 0.015823 \n",
      "It 172599\n",
      "l0: 0.061326, l1: 0.061172, l2: 0.063390, l3: 0.068067, l4: 0.078906, l5: 0.123242, l6: 0.285981\n",
      "\n",
      "[epoch: 21575/100000, batch:    32/   30, ite: 172600] train loss: 0.316275, tar: 0.015899 \n",
      "It 172600\n",
      "l0: 0.013760, l1: 0.013745, l2: 0.014636, l3: 0.015412, l4: 0.019808, l5: 0.091126, l6: 0.251401\n",
      "\n",
      "[epoch: 21576/100000, batch:     4/   30, ite: 172601] train loss: 0.316448, tar: 0.015895 \n",
      "It 172601\n",
      "l0: 0.010331, l1: 0.010335, l2: 0.011392, l3: 0.012288, l4: 0.015748, l5: 0.036207, l6: 0.160689\n",
      "\n",
      "[epoch: 21576/100000, batch:     8/   30, ite: 172602] train loss: 0.316349, tar: 0.015886 \n",
      "It 172602\n",
      "l0: 0.009338, l1: 0.009342, l2: 0.010291, l3: 0.011351, l4: 0.015333, l5: 0.051957, l6: 0.125615\n",
      "\n",
      "[epoch: 21576/100000, batch:    12/   30, ite: 172603] train loss: 0.316211, tar: 0.015875 \n",
      "It 172603\n",
      "l0: 0.035546, l1: 0.035635, l2: 0.036837, l3: 0.039799, l4: 0.046233, l5: 0.084561, l6: 0.236271\n",
      "\n",
      "[epoch: 21576/100000, batch:    16/   30, ite: 172604] train loss: 0.316540, tar: 0.015908 \n",
      "It 172604\n",
      "l0: 0.006064, l1: 0.006064, l2: 0.006752, l3: 0.007216, l4: 0.008703, l5: 0.017454, l6: 0.068039\n",
      "\n",
      "[epoch: 21576/100000, batch:    20/   30, ite: 172605] train loss: 0.316216, tar: 0.015892 \n",
      "It 172605\n",
      "l0: 0.005515, l1: 0.005515, l2: 0.006114, l3: 0.006572, l4: 0.008667, l5: 0.013999, l6: 0.057972\n",
      "\n",
      "[epoch: 21576/100000, batch:    24/   30, ite: 172606] train loss: 0.315866, tar: 0.015874 \n",
      "It 172606\n",
      "l0: 0.013385, l1: 0.013383, l2: 0.014410, l3: 0.015178, l4: 0.019206, l5: 0.051448, l6: 0.269504\n",
      "\n",
      "[epoch: 21576/100000, batch:    28/   30, ite: 172607] train loss: 0.315999, tar: 0.015870 \n",
      "It 172607\n",
      "l0: 0.047289, l1: 0.047166, l2: 0.048229, l3: 0.051701, l4: 0.054995, l5: 0.091335, l6: 0.202499\n",
      "\n",
      "[epoch: 21576/100000, batch:    32/   30, ite: 172608] train loss: 0.316373, tar: 0.015922 \n",
      "It 172608\n",
      "l0: 0.011341, l1: 0.011335, l2: 0.012322, l3: 0.013150, l4: 0.016114, l5: 0.062959, l6: 0.175629\n",
      "\n",
      "[epoch: 21577/100000, batch:     4/   30, ite: 172609] train loss: 0.316350, tar: 0.015914 \n",
      "It 172609\n",
      "l0: 0.010876, l1: 0.010872, l2: 0.011782, l3: 0.012505, l4: 0.015079, l5: 0.048306, l6: 0.183528\n",
      "\n",
      "[epoch: 21577/100000, batch:     8/   30, ite: 172610] train loss: 0.316312, tar: 0.015906 \n",
      "It 172610\n",
      "l0: 0.032395, l1: 0.032472, l2: 0.033567, l3: 0.036202, l4: 0.041194, l5: 0.069121, l6: 0.158578\n",
      "\n",
      "[epoch: 21577/100000, batch:    12/   30, ite: 172611] train loss: 0.316455, tar: 0.015933 \n",
      "It 172611\n",
      "l0: 0.009256, l1: 0.009255, l2: 0.010190, l3: 0.010697, l4: 0.012408, l5: 0.030350, l6: 0.135813\n",
      "\n",
      "[epoch: 21577/100000, batch:    16/   30, ite: 172612] train loss: 0.316294, tar: 0.015922 \n",
      "It 172612\n",
      "l0: 0.010757, l1: 0.010755, l2: 0.011672, l3: 0.012276, l4: 0.015367, l5: 0.059249, l6: 0.190233\n",
      "\n",
      "[epoch: 21577/100000, batch:    20/   30, ite: 172613] train loss: 0.316284, tar: 0.015914 \n",
      "It 172613\n",
      "l0: 0.008599, l1: 0.008598, l2: 0.009466, l3: 0.010080, l4: 0.013159, l5: 0.026754, l6: 0.146780\n",
      "\n",
      "[epoch: 21577/100000, batch:    24/   30, ite: 172614] train loss: 0.316133, tar: 0.015902 \n",
      "It 172614\n",
      "l0: 0.029533, l1: 0.029444, l2: 0.030351, l3: 0.031034, l4: 0.031548, l5: 0.057717, l6: 0.151073\n",
      "\n",
      "[epoch: 21577/100000, batch:    28/   30, ite: 172615] train loss: 0.316205, tar: 0.015924 \n",
      "It 172615\n",
      "l0: 0.016876, l1: 0.016874, l2: 0.018150, l3: 0.019331, l4: 0.030159, l5: 0.112857, l6: 0.320834\n",
      "\n",
      "[epoch: 21577/100000, batch:    32/   30, ite: 172616] train loss: 0.316561, tar: 0.015926 \n",
      "It 172616\n",
      "l0: 0.011683, l1: 0.011682, l2: 0.012690, l3: 0.013403, l4: 0.018378, l5: 0.063525, l6: 0.191158\n",
      "\n",
      "[epoch: 21578/100000, batch:     4/   30, ite: 172617] train loss: 0.316570, tar: 0.015919 \n",
      "It 172617\n",
      "l0: 0.004716, l1: 0.004715, l2: 0.005254, l3: 0.005605, l4: 0.006925, l5: 0.014320, l6: 0.056142\n",
      "\n",
      "[epoch: 21578/100000, batch:     8/   30, ite: 172618] train loss: 0.316216, tar: 0.015901 \n",
      "It 172618\n",
      "l0: 0.011662, l1: 0.011666, l2: 0.012608, l3: 0.013346, l4: 0.016902, l5: 0.077920, l6: 0.218090\n",
      "\n",
      "[epoch: 21578/100000, batch:    12/   30, ite: 172619] train loss: 0.316290, tar: 0.015894 \n",
      "It 172619\n",
      "l0: 0.008935, l1: 0.008935, l2: 0.009766, l3: 0.010561, l4: 0.012619, l5: 0.032521, l6: 0.170667\n",
      "\n",
      "[epoch: 21578/100000, batch:    16/   30, ite: 172620] train loss: 0.316190, tar: 0.015883 \n",
      "It 172620\n",
      "l0: 0.006108, l1: 0.006107, l2: 0.006928, l3: 0.007398, l4: 0.008946, l5: 0.015734, l6: 0.071203\n",
      "\n",
      "[epoch: 21578/100000, batch:    20/   30, ite: 172621] train loss: 0.315878, tar: 0.015867 \n",
      "It 172621\n",
      "l0: 0.010703, l1: 0.010703, l2: 0.011585, l3: 0.012289, l4: 0.016183, l5: 0.029847, l6: 0.182411\n",
      "\n",
      "[epoch: 21578/100000, batch:    24/   30, ite: 172622] train loss: 0.315810, tar: 0.015859 \n",
      "It 172622\n",
      "l0: 0.034763, l1: 0.034697, l2: 0.035692, l3: 0.036863, l4: 0.039662, l5: 0.095263, l6: 0.257336\n",
      "\n",
      "[epoch: 21578/100000, batch:    28/   30, ite: 172623] train loss: 0.316161, tar: 0.015889 \n",
      "It 172623\n",
      "l0: 0.067607, l1: 0.067681, l2: 0.070253, l3: 0.076607, l4: 0.086903, l5: 0.164562, l6: 0.379656\n",
      "\n",
      "[epoch: 21578/100000, batch:    32/   30, ite: 172624] train loss: 0.317118, tar: 0.015972 \n",
      "It 172624\n",
      "l0: 0.056281, l1: 0.056278, l2: 0.058270, l3: 0.062211, l4: 0.070318, l5: 0.121858, l6: 0.270794\n",
      "\n",
      "[epoch: 21579/100000, batch:     4/   30, ite: 172625] train loss: 0.317724, tar: 0.016036 \n",
      "It 172625\n",
      "l0: 0.012459, l1: 0.012460, l2: 0.013711, l3: 0.014741, l4: 0.018729, l5: 0.074539, l6: 0.198377\n",
      "\n",
      "[epoch: 21579/100000, batch:     8/   30, ite: 172626] train loss: 0.317768, tar: 0.016031 \n",
      "It 172626\n",
      "l0: 0.008527, l1: 0.008525, l2: 0.009264, l3: 0.009748, l4: 0.011944, l5: 0.025157, l6: 0.127996\n",
      "\n",
      "[epoch: 21579/100000, batch:    12/   30, ite: 172627] train loss: 0.317582, tar: 0.016019 \n",
      "It 172627\n",
      "l0: 0.011102, l1: 0.011096, l2: 0.012019, l3: 0.012729, l4: 0.018049, l5: 0.065899, l6: 0.196064\n",
      "\n",
      "[epoch: 21579/100000, batch:    16/   30, ite: 172628] train loss: 0.317597, tar: 0.016011 \n",
      "It 172628\n",
      "l0: 0.011819, l1: 0.011812, l2: 0.012700, l3: 0.013524, l4: 0.017050, l5: 0.050502, l6: 0.224857\n",
      "\n",
      "[epoch: 21579/100000, batch:    20/   30, ite: 172629] train loss: 0.317636, tar: 0.016004 \n",
      "It 172629\n",
      "l0: 0.004914, l1: 0.004905, l2: 0.005468, l3: 0.005775, l4: 0.007347, l5: 0.013780, l6: 0.064776\n",
      "\n",
      "[epoch: 21579/100000, batch:    24/   30, ite: 172630] train loss: 0.317301, tar: 0.015986 \n",
      "It 172630\n",
      "l0: 0.008803, l1: 0.008801, l2: 0.009615, l3: 0.010145, l4: 0.012335, l5: 0.052084, l6: 0.115093\n",
      "\n",
      "[epoch: 21579/100000, batch:    28/   30, ite: 172631] train loss: 0.317142, tar: 0.015975 \n",
      "It 172631\n",
      "l0: 0.010672, l1: 0.010669, l2: 0.011537, l3: 0.012064, l4: 0.014584, l5: 0.043798, l6: 0.170667\n",
      "\n",
      "[epoch: 21579/100000, batch:    32/   30, ite: 172632] train loss: 0.317074, tar: 0.015967 \n",
      "It 172632\n",
      "l0: 0.007574, l1: 0.007573, l2: 0.008348, l3: 0.008931, l4: 0.011504, l5: 0.031305, l6: 0.114785\n",
      "\n",
      "[epoch: 21580/100000, batch:     4/   30, ite: 172633] train loss: 0.316873, tar: 0.015953 \n",
      "It 172633\n",
      "l0: 0.007901, l1: 0.007904, l2: 0.008616, l3: 0.009028, l4: 0.010755, l5: 0.021888, l6: 0.134789\n",
      "\n",
      "[epoch: 21580/100000, batch:     8/   30, ite: 172634] train loss: 0.316690, tar: 0.015941 \n",
      "It 172634\n",
      "l0: 0.035684, l1: 0.035760, l2: 0.036953, l3: 0.039663, l4: 0.046115, l5: 0.090690, l6: 0.198894\n",
      "\n",
      "[epoch: 21580/100000, batch:    12/   30, ite: 172635] train loss: 0.316953, tar: 0.015972 \n",
      "It 172635\n",
      "l0: 0.031022, l1: 0.030944, l2: 0.031634, l3: 0.033481, l4: 0.037492, l5: 0.063594, l6: 0.196955\n",
      "\n",
      "[epoch: 21580/100000, batch:    16/   30, ite: 172636] train loss: 0.317123, tar: 0.015995 \n",
      "It 172636\n",
      "l0: 0.010853, l1: 0.010850, l2: 0.011738, l3: 0.012762, l4: 0.016533, l5: 0.058409, l6: 0.197954\n",
      "\n",
      "[epoch: 21580/100000, batch:    20/   30, ite: 172637] train loss: 0.317127, tar: 0.015987 \n",
      "It 172637\n",
      "l0: 0.012353, l1: 0.012353, l2: 0.013517, l3: 0.014405, l4: 0.019958, l5: 0.069270, l6: 0.199159\n",
      "\n",
      "[epoch: 21580/100000, batch:    24/   30, ite: 172638] train loss: 0.317164, tar: 0.015982 \n",
      "It 172638\n",
      "l0: 0.009596, l1: 0.009594, l2: 0.010305, l3: 0.011110, l4: 0.014591, l5: 0.038368, l6: 0.166484\n",
      "\n",
      "[epoch: 21580/100000, batch:    28/   30, ite: 172639] train loss: 0.317075, tar: 0.015972 \n",
      "It 172639\n",
      "l0: 0.012818, l1: 0.012815, l2: 0.013845, l3: 0.014869, l4: 0.018194, l5: 0.054385, l6: 0.240747\n",
      "\n",
      "[epoch: 21580/100000, batch:    32/   30, ite: 172640] train loss: 0.317154, tar: 0.015967 \n",
      "It 172640\n",
      "l0: 0.007603, l1: 0.007601, l2: 0.008313, l3: 0.008895, l4: 0.010473, l5: 0.020589, l6: 0.152001\n",
      "\n",
      "[epoch: 21581/100000, batch:     4/   30, ite: 172641] train loss: 0.316995, tar: 0.015954 \n",
      "It 172641\n",
      "l0: 0.011678, l1: 0.011675, l2: 0.012542, l3: 0.013224, l4: 0.015364, l5: 0.040363, l6: 0.226286\n",
      "\n",
      "[epoch: 21581/100000, batch:     8/   30, ite: 172642] train loss: 0.317017, tar: 0.015947 \n",
      "It 172642\n",
      "l0: 0.014550, l1: 0.014546, l2: 0.015596, l3: 0.016765, l4: 0.022959, l5: 0.102543, l6: 0.253125\n",
      "\n",
      "[epoch: 21581/100000, batch:    12/   30, ite: 172643] train loss: 0.317208, tar: 0.015945 \n",
      "It 172643\n",
      "l0: 0.030406, l1: 0.030324, l2: 0.031534, l3: 0.033034, l4: 0.035977, l5: 0.058063, l6: 0.148821\n",
      "\n",
      "[epoch: 21581/100000, batch:    16/   30, ite: 172644] train loss: 0.317288, tar: 0.015967 \n",
      "It 172644\n",
      "l0: 0.009876, l1: 0.009875, l2: 0.010720, l3: 0.011478, l4: 0.014068, l5: 0.072425, l6: 0.158356\n",
      "\n",
      "[epoch: 21581/100000, batch:    20/   30, ite: 172645] train loss: 0.317240, tar: 0.015958 \n",
      "It 172645\n",
      "l0: 0.036515, l1: 0.036598, l2: 0.038457, l3: 0.042550, l4: 0.051328, l5: 0.088955, l6: 0.260340\n",
      "\n",
      "[epoch: 21581/100000, batch:    24/   30, ite: 172646] train loss: 0.317608, tar: 0.015990 \n",
      "It 172646\n",
      "l0: 0.005774, l1: 0.005774, l2: 0.006528, l3: 0.006885, l4: 0.008257, l5: 0.014484, l6: 0.055841\n",
      "\n",
      "[epoch: 21581/100000, batch:    28/   30, ite: 172647] train loss: 0.317277, tar: 0.015974 \n",
      "It 172647\n",
      "l0: 0.011413, l1: 0.011420, l2: 0.012262, l3: 0.013151, l4: 0.015389, l5: 0.052246, l6: 0.183593\n",
      "\n",
      "[epoch: 21581/100000, batch:    32/   30, ite: 172648] train loss: 0.317250, tar: 0.015967 \n",
      "It 172648\n",
      "l0: 0.030709, l1: 0.030651, l2: 0.031936, l3: 0.034044, l4: 0.038239, l5: 0.067917, l6: 0.212944\n",
      "\n",
      "[epoch: 21582/100000, batch:     4/   30, ite: 172649] train loss: 0.317449, tar: 0.015990 \n",
      "It 172649\n",
      "l0: 0.016762, l1: 0.016766, l2: 0.017943, l3: 0.018961, l4: 0.023605, l5: 0.100893, l6: 0.333566\n",
      "\n",
      "[epoch: 21582/100000, batch:     8/   30, ite: 172650] train loss: 0.317773, tar: 0.015991 \n",
      "It 172650\n",
      "l0: 0.005148, l1: 0.005149, l2: 0.005720, l3: 0.006064, l4: 0.007244, l5: 0.015979, l6: 0.060907\n",
      "\n",
      "[epoch: 21582/100000, batch:    12/   30, ite: 172651] train loss: 0.317448, tar: 0.015974 \n",
      "It 172651\n",
      "l0: 0.012155, l1: 0.012159, l2: 0.013307, l3: 0.014634, l4: 0.020964, l5: 0.045187, l6: 0.188806\n",
      "\n",
      "[epoch: 21582/100000, batch:    16/   30, ite: 172652] train loss: 0.317433, tar: 0.015968 \n",
      "It 172652\n",
      "l0: 0.029695, l1: 0.029771, l2: 0.030655, l3: 0.033012, l4: 0.038346, l5: 0.096511, l6: 0.158715\n",
      "\n",
      "[epoch: 21582/100000, batch:    20/   30, ite: 172653] train loss: 0.317585, tar: 0.015989 \n",
      "It 172653\n",
      "l0: 0.009174, l1: 0.009174, l2: 0.009912, l3: 0.010456, l4: 0.013093, l5: 0.040035, l6: 0.164950\n",
      "\n",
      "[epoch: 21582/100000, batch:    24/   30, ite: 172654] train loss: 0.317492, tar: 0.015979 \n",
      "It 172654\n",
      "l0: 0.008227, l1: 0.008224, l2: 0.008977, l3: 0.009726, l4: 0.012435, l5: 0.034770, l6: 0.113243\n",
      "\n",
      "[epoch: 21582/100000, batch:    28/   30, ite: 172655] train loss: 0.317306, tar: 0.015967 \n",
      "It 172655\n",
      "l0: 0.006868, l1: 0.006862, l2: 0.007500, l3: 0.007989, l4: 0.010024, l5: 0.015447, l6: 0.104044\n",
      "\n",
      "[epoch: 21582/100000, batch:    32/   30, ite: 172656] train loss: 0.317064, tar: 0.015953 \n",
      "It 172656\n",
      "l0: 0.010411, l1: 0.010407, l2: 0.011300, l3: 0.011997, l4: 0.015943, l5: 0.053516, l6: 0.175900\n",
      "\n",
      "[epoch: 21583/100000, batch:     4/   30, ite: 172657] train loss: 0.317022, tar: 0.015945 \n",
      "It 172657\n",
      "l0: 0.029423, l1: 0.029462, l2: 0.030831, l3: 0.032903, l4: 0.038648, l5: 0.063048, l6: 0.169788\n",
      "\n",
      "[epoch: 21583/100000, batch:     8/   30, ite: 172658] train loss: 0.317139, tar: 0.015965 \n",
      "It 172658\n",
      "l0: 0.003597, l1: 0.003594, l2: 0.004056, l3: 0.004359, l4: 0.005917, l5: 0.012084, l6: 0.041526\n",
      "\n",
      "[epoch: 21583/100000, batch:    12/   30, ite: 172659] train loss: 0.316772, tar: 0.015946 \n",
      "It 172659\n",
      "l0: 0.016103, l1: 0.016102, l2: 0.017140, l3: 0.018087, l4: 0.023343, l5: 0.096369, l6: 0.307236\n",
      "\n",
      "[epoch: 21583/100000, batch:    16/   30, ite: 172660] train loss: 0.317041, tar: 0.015947 \n",
      "It 172660\n",
      "l0: 0.008304, l1: 0.008302, l2: 0.009049, l3: 0.009482, l4: 0.010991, l5: 0.030289, l6: 0.149351\n",
      "\n",
      "[epoch: 21583/100000, batch:    20/   30, ite: 172661] train loss: 0.316903, tar: 0.015935 \n",
      "It 172661\n",
      "l0: 0.008332, l1: 0.008337, l2: 0.009282, l3: 0.009973, l4: 0.012913, l5: 0.020093, l6: 0.081230\n",
      "\n",
      "[epoch: 21583/100000, batch:    24/   30, ite: 172662] train loss: 0.316651, tar: 0.015924 \n",
      "It 172662\n",
      "l0: 0.012706, l1: 0.012703, l2: 0.013721, l3: 0.014761, l4: 0.021953, l5: 0.068146, l6: 0.237938\n",
      "\n",
      "[epoch: 21583/100000, batch:    28/   30, ite: 172663] train loss: 0.316749, tar: 0.015919 \n",
      "It 172663\n",
      "l0: 0.054114, l1: 0.053970, l2: 0.055694, l3: 0.059980, l4: 0.065366, l5: 0.095347, l6: 0.211705\n",
      "\n",
      "[epoch: 21583/100000, batch:    32/   30, ite: 172664] train loss: 0.317170, tar: 0.015976 \n",
      "It 172664\n",
      "l0: 0.009115, l1: 0.009114, l2: 0.009819, l3: 0.010369, l4: 0.013069, l5: 0.034727, l6: 0.158050\n",
      "\n",
      "[epoch: 21584/100000, batch:     4/   30, ite: 172665] train loss: 0.317061, tar: 0.015966 \n",
      "It 172665\n",
      "l0: 0.008348, l1: 0.008353, l2: 0.009194, l3: 0.009851, l4: 0.012361, l5: 0.022049, l6: 0.127722\n",
      "\n",
      "[epoch: 21584/100000, batch:     8/   30, ite: 172666] train loss: 0.316882, tar: 0.015955 \n",
      "It 172666\n",
      "l0: 0.012284, l1: 0.012288, l2: 0.013307, l3: 0.014225, l4: 0.017315, l5: 0.051697, l6: 0.232046\n",
      "\n",
      "[epoch: 21584/100000, batch:    12/   30, ite: 172667] train loss: 0.316936, tar: 0.015949 \n",
      "It 172667\n",
      "l0: 0.039061, l1: 0.038998, l2: 0.040480, l3: 0.041751, l4: 0.046612, l5: 0.146682, l6: 0.326874\n",
      "\n",
      "[epoch: 21584/100000, batch:    16/   30, ite: 172668] train loss: 0.317480, tar: 0.015984 \n",
      "It 172668\n",
      "l0: 0.007431, l1: 0.007428, l2: 0.008042, l3: 0.008684, l4: 0.012701, l5: 0.042387, l6: 0.117782\n",
      "\n",
      "[epoch: 21584/100000, batch:    20/   30, ite: 172669] train loss: 0.317311, tar: 0.015971 \n",
      "It 172669\n",
      "l0: 0.031563, l1: 0.031635, l2: 0.033053, l3: 0.035475, l4: 0.040721, l5: 0.068960, l6: 0.159179\n",
      "\n",
      "[epoch: 21584/100000, batch:    24/   30, ite: 172670] train loss: 0.317436, tar: 0.015994 \n",
      "It 172670\n",
      "l0: 0.007252, l1: 0.007255, l2: 0.007899, l3: 0.008328, l4: 0.010495, l5: 0.029266, l6: 0.093979\n",
      "\n",
      "[epoch: 21584/100000, batch:    28/   30, ite: 172671] train loss: 0.317208, tar: 0.015981 \n",
      "It 172671\n",
      "l0: 0.003609, l1: 0.003609, l2: 0.004084, l3: 0.004345, l4: 0.005645, l5: 0.009238, l6: 0.037153\n",
      "\n",
      "[epoch: 21584/100000, batch:    32/   30, ite: 172672] train loss: 0.316836, tar: 0.015963 \n",
      "It 172672\n",
      "l0: 0.008038, l1: 0.008035, l2: 0.008786, l3: 0.009344, l4: 0.012669, l5: 0.039288, l6: 0.090986\n",
      "\n",
      "[epoch: 21585/100000, batch:     4/   30, ite: 172673] train loss: 0.316629, tar: 0.015951 \n",
      "It 172673\n",
      "l0: 0.010414, l1: 0.010409, l2: 0.011458, l3: 0.012325, l4: 0.015348, l5: 0.032179, l6: 0.162826\n",
      "\n",
      "[epoch: 21585/100000, batch:     8/   30, ite: 172674] train loss: 0.316537, tar: 0.015943 \n",
      "It 172674\n",
      "l0: 0.009491, l1: 0.009486, l2: 0.010312, l3: 0.010897, l4: 0.013861, l5: 0.036947, l6: 0.167156\n",
      "\n",
      "[epoch: 21585/100000, batch:    12/   30, ite: 172675] train loss: 0.316451, tar: 0.015933 \n",
      "It 172675\n",
      "l0: 0.005895, l1: 0.005891, l2: 0.006605, l3: 0.007145, l4: 0.008729, l5: 0.016309, l6: 0.070696\n",
      "\n",
      "[epoch: 21585/100000, batch:    16/   30, ite: 172676] train loss: 0.316162, tar: 0.015918 \n",
      "It 172676\n",
      "l0: 0.035621, l1: 0.035567, l2: 0.036526, l3: 0.037417, l4: 0.040696, l5: 0.096824, l6: 0.274983\n",
      "\n",
      "[epoch: 21585/100000, batch:    20/   30, ite: 172677] train loss: 0.316519, tar: 0.015947 \n",
      "It 172677\n",
      "l0: 0.012058, l1: 0.012057, l2: 0.013076, l3: 0.014089, l4: 0.018841, l5: 0.063389, l6: 0.212243\n",
      "\n",
      "[epoch: 21585/100000, batch:    24/   30, ite: 172678] train loss: 0.316562, tar: 0.015942 \n",
      "It 172678\n",
      "l0: 0.034669, l1: 0.034722, l2: 0.036123, l3: 0.039614, l4: 0.044453, l5: 0.131881, l6: 0.245337\n",
      "\n",
      "[epoch: 21585/100000, batch:    28/   30, ite: 172679] train loss: 0.316930, tar: 0.015969 \n",
      "It 172679\n",
      "l0: 0.009462, l1: 0.009465, l2: 0.010409, l3: 0.010887, l4: 0.012847, l5: 0.035659, l6: 0.170360\n",
      "\n",
      "[epoch: 21585/100000, batch:    32/   30, ite: 172680] train loss: 0.316845, tar: 0.015960 \n",
      "It 172680\n",
      "l0: 0.009596, l1: 0.009592, l2: 0.010328, l3: 0.010788, l4: 0.012329, l5: 0.055447, l6: 0.179742\n",
      "\n",
      "[epoch: 21586/100000, batch:     4/   30, ite: 172681] train loss: 0.316803, tar: 0.015950 \n",
      "It 172681\n",
      "l0: 0.010682, l1: 0.010679, l2: 0.011655, l3: 0.012285, l4: 0.014532, l5: 0.034380, l6: 0.194863\n",
      "\n",
      "[epoch: 21586/100000, batch:     8/   30, ite: 172682] train loss: 0.316762, tar: 0.015943 \n",
      "It 172682\n",
      "l0: 0.007205, l1: 0.007208, l2: 0.008012, l3: 0.008407, l4: 0.010100, l5: 0.021991, l6: 0.079635\n",
      "\n",
      "[epoch: 21586/100000, batch:    12/   30, ite: 172683] train loss: 0.316507, tar: 0.015930 \n",
      "It 172683\n",
      "l0: 0.029574, l1: 0.029542, l2: 0.030722, l3: 0.032000, l4: 0.035401, l5: 0.059925, l6: 0.152915\n",
      "\n",
      "[epoch: 21586/100000, batch:    16/   30, ite: 172684] train loss: 0.316585, tar: 0.015950 \n",
      "It 172684\n",
      "l0: 0.034685, l1: 0.034746, l2: 0.036349, l3: 0.039863, l4: 0.048090, l5: 0.101333, l6: 0.224556\n",
      "\n",
      "[epoch: 21586/100000, batch:    20/   30, ite: 172685] train loss: 0.316882, tar: 0.015977 \n",
      "It 172685\n",
      "l0: 0.008826, l1: 0.008828, l2: 0.009525, l3: 0.009977, l4: 0.012526, l5: 0.039905, l6: 0.174828\n",
      "\n",
      "[epoch: 21586/100000, batch:    24/   30, ite: 172686] train loss: 0.316805, tar: 0.015967 \n",
      "It 172686\n",
      "l0: 0.009091, l1: 0.009096, l2: 0.009968, l3: 0.010829, l4: 0.013635, l5: 0.028609, l6: 0.144495\n",
      "\n",
      "[epoch: 21586/100000, batch:    28/   30, ite: 172687] train loss: 0.316673, tar: 0.015957 \n",
      "It 172687\n",
      "l0: 0.015853, l1: 0.015841, l2: 0.016898, l3: 0.017673, l4: 0.024977, l5: 0.165046, l6: 0.310194\n",
      "\n",
      "[epoch: 21586/100000, batch:    32/   30, ite: 172688] train loss: 0.317036, tar: 0.015957 \n",
      "It 172688\n",
      "l0: 0.031511, l1: 0.031473, l2: 0.032521, l3: 0.033922, l4: 0.036278, l5: 0.080935, l6: 0.266822\n",
      "\n",
      "[epoch: 21587/100000, batch:     4/   30, ite: 172689] train loss: 0.317321, tar: 0.015979 \n",
      "It 172689\n",
      "l0: 0.011769, l1: 0.011726, l2: 0.012615, l3: 0.013079, l4: 0.016607, l5: 0.063022, l6: 0.209861\n",
      "\n",
      "[epoch: 21587/100000, batch:     8/   30, ite: 172690] train loss: 0.317352, tar: 0.015973 \n",
      "It 172690\n",
      "l0: 0.012712, l1: 0.012709, l2: 0.013548, l3: 0.014333, l4: 0.018851, l5: 0.078905, l6: 0.212589\n",
      "\n",
      "[epoch: 21587/100000, batch:    12/   30, ite: 172691] train loss: 0.317419, tar: 0.015968 \n",
      "It 172691\n",
      "l0: 0.009095, l1: 0.009096, l2: 0.009969, l3: 0.010528, l4: 0.013218, l5: 0.037602, l6: 0.145222\n",
      "\n",
      "[epoch: 21587/100000, batch:    16/   30, ite: 172692] train loss: 0.317299, tar: 0.015958 \n",
      "It 172692\n",
      "l0: 0.008462, l1: 0.008460, l2: 0.009289, l3: 0.009818, l4: 0.012045, l5: 0.046561, l6: 0.120861\n",
      "\n",
      "[epoch: 21587/100000, batch:    20/   30, ite: 172693] train loss: 0.317152, tar: 0.015948 \n",
      "It 172693\n",
      "l0: 0.005583, l1: 0.005585, l2: 0.006184, l3: 0.006508, l4: 0.007748, l5: 0.022115, l6: 0.087794\n",
      "\n",
      "[epoch: 21587/100000, batch:    24/   30, ite: 172694] train loss: 0.316899, tar: 0.015933 \n",
      "It 172694\n",
      "l0: 0.007525, l1: 0.007527, l2: 0.008444, l3: 0.009090, l4: 0.011595, l5: 0.021536, l6: 0.106780\n",
      "\n",
      "[epoch: 21587/100000, batch:    28/   30, ite: 172695] train loss: 0.316692, tar: 0.015921 \n",
      "It 172695\n",
      "l0: 0.052094, l1: 0.052225, l2: 0.054368, l3: 0.059483, l4: 0.067891, l5: 0.101983, l6: 0.208740\n",
      "\n",
      "[epoch: 21587/100000, batch:    32/   30, ite: 172696] train loss: 0.317094, tar: 0.015972 \n",
      "It 172696\n",
      "l0: 0.013223, l1: 0.013227, l2: 0.014432, l3: 0.015415, l4: 0.020722, l5: 0.070451, l6: 0.229531\n",
      "\n",
      "[epoch: 21588/100000, batch:     4/   30, ite: 172697] train loss: 0.317180, tar: 0.015969 \n",
      "It 172697\n",
      "l0: 0.034726, l1: 0.034629, l2: 0.035682, l3: 0.037516, l4: 0.045616, l5: 0.092738, l6: 0.232246\n",
      "\n",
      "[epoch: 21588/100000, batch:     8/   30, ite: 172698] train loss: 0.317461, tar: 0.015995 \n",
      "It 172698\n",
      "l0: 0.009182, l1: 0.009151, l2: 0.009971, l3: 0.010748, l4: 0.013681, l5: 0.038011, l6: 0.150658\n",
      "\n",
      "[epoch: 21588/100000, batch:    12/   30, ite: 172699] train loss: 0.317352, tar: 0.015986 \n",
      "It 172699\n",
      "l0: 0.005783, l1: 0.005783, l2: 0.006427, l3: 0.006902, l4: 0.008672, l5: 0.025522, l6: 0.061300\n",
      "\n",
      "[epoch: 21588/100000, batch:    16/   30, ite: 172700] train loss: 0.317071, tar: 0.015971 \n",
      "It 172700\n",
      "l0: 0.034929, l1: 0.034993, l2: 0.036629, l3: 0.038929, l4: 0.044600, l5: 0.090845, l6: 0.256500\n",
      "\n",
      "[epoch: 21588/100000, batch:    20/   30, ite: 172701] train loss: 0.317385, tar: 0.015998 \n",
      "It 172701\n",
      "l0: 0.008960, l1: 0.008961, l2: 0.009673, l3: 0.010333, l4: 0.012038, l5: 0.025542, l6: 0.168587\n",
      "\n",
      "[epoch: 21588/100000, batch:    24/   30, ite: 172702] train loss: 0.317280, tar: 0.015988 \n",
      "It 172702\n",
      "l0: 0.007063, l1: 0.007062, l2: 0.007800, l3: 0.008480, l4: 0.010154, l5: 0.027448, l6: 0.109335\n",
      "\n",
      "[epoch: 21588/100000, batch:    28/   30, ite: 172703] train loss: 0.317081, tar: 0.015975 \n",
      "It 172703\n",
      "l0: 0.012303, l1: 0.012300, l2: 0.013518, l3: 0.014602, l4: 0.018081, l5: 0.103451, l6: 0.207914\n",
      "\n",
      "[epoch: 21588/100000, batch:    32/   30, ite: 172704] train loss: 0.317174, tar: 0.015970 \n",
      "It 172704\n",
      "l0: 0.056881, l1: 0.056875, l2: 0.058635, l3: 0.061358, l4: 0.066225, l5: 0.123954, l6: 0.285587\n",
      "\n",
      "[epoch: 21589/100000, batch:     4/   30, ite: 172705] train loss: 0.317730, tar: 0.016028 \n",
      "It 172705\n",
      "l0: 0.009780, l1: 0.009774, l2: 0.010759, l3: 0.011761, l4: 0.014329, l5: 0.033687, l6: 0.151770\n",
      "\n",
      "[epoch: 21589/100000, batch:     8/   30, ite: 172706] train loss: 0.317623, tar: 0.016019 \n",
      "It 172706\n",
      "l0: 0.003655, l1: 0.003656, l2: 0.004129, l3: 0.004366, l4: 0.004955, l5: 0.008242, l6: 0.035730\n",
      "\n",
      "[epoch: 21589/100000, batch:    12/   30, ite: 172707] train loss: 0.317265, tar: 0.016002 \n",
      "It 172707\n",
      "l0: 0.010450, l1: 0.010450, l2: 0.011333, l3: 0.011782, l4: 0.013595, l5: 0.045554, l6: 0.184678\n",
      "\n",
      "[epoch: 21589/100000, batch:    16/   30, ite: 172708] train loss: 0.317224, tar: 0.015994 \n",
      "It 172708\n",
      "l0: 0.012026, l1: 0.012026, l2: 0.013206, l3: 0.014366, l4: 0.019025, l5: 0.050940, l6: 0.207329\n",
      "\n",
      "[epoch: 21589/100000, batch:    20/   30, ite: 172709] train loss: 0.317240, tar: 0.015988 \n",
      "It 172709\n",
      "l0: 0.008964, l1: 0.008962, l2: 0.009636, l3: 0.010216, l4: 0.014638, l5: 0.074377, l6: 0.160107\n",
      "\n",
      "[epoch: 21589/100000, batch:    24/   30, ite: 172710] train loss: 0.317197, tar: 0.015979 \n",
      "It 172710\n",
      "l0: 0.009738, l1: 0.009734, l2: 0.010511, l3: 0.011218, l4: 0.014558, l5: 0.033191, l6: 0.184105\n",
      "\n",
      "[epoch: 21589/100000, batch:    28/   30, ite: 172711] train loss: 0.317135, tar: 0.015970 \n",
      "It 172711\n",
      "l0: 0.008753, l1: 0.008751, l2: 0.009405, l3: 0.009841, l4: 0.011766, l5: 0.058856, l6: 0.135410\n",
      "\n",
      "[epoch: 21589/100000, batch:    32/   30, ite: 172712] train loss: 0.317031, tar: 0.015960 \n",
      "It 172712\n",
      "l0: 0.025356, l1: 0.025295, l2: 0.026219, l3: 0.027247, l4: 0.028029, l5: 0.047538, l6: 0.130893\n",
      "\n",
      "[epoch: 21590/100000, batch:     4/   30, ite: 172713] train loss: 0.317022, tar: 0.015973 \n",
      "It 172713\n",
      "l0: 0.010851, l1: 0.010852, l2: 0.011798, l3: 0.012609, l4: 0.015774, l5: 0.049538, l6: 0.200444\n",
      "\n",
      "[epoch: 21590/100000, batch:     8/   30, ite: 172714] train loss: 0.317015, tar: 0.015966 \n",
      "It 172714\n",
      "l0: 0.007993, l1: 0.007993, l2: 0.008701, l3: 0.009364, l4: 0.012389, l5: 0.047382, l6: 0.119774\n",
      "\n",
      "[epoch: 21590/100000, batch:    12/   30, ite: 172715] train loss: 0.316870, tar: 0.015955 \n",
      "It 172715\n",
      "l0: 0.010643, l1: 0.010649, l2: 0.011510, l3: 0.012405, l4: 0.016101, l5: 0.046419, l6: 0.199830\n",
      "\n",
      "[epoch: 21590/100000, batch:    16/   30, ite: 172716] train loss: 0.316857, tar: 0.015947 \n",
      "It 172716\n",
      "l0: 0.008716, l1: 0.008717, l2: 0.009491, l3: 0.009933, l4: 0.011267, l5: 0.031850, l6: 0.149212\n",
      "\n",
      "[epoch: 21590/100000, batch:    20/   30, ite: 172717] train loss: 0.316735, tar: 0.015937 \n",
      "It 172717\n",
      "l0: 0.032483, l1: 0.032537, l2: 0.033988, l3: 0.037008, l4: 0.042978, l5: 0.072860, l6: 0.183849\n",
      "\n",
      "[epoch: 21590/100000, batch:    24/   30, ite: 172718] train loss: 0.316900, tar: 0.015960 \n",
      "It 172718\n",
      "l0: 0.012313, l1: 0.012314, l2: 0.013356, l3: 0.014358, l4: 0.018788, l5: 0.063824, l6: 0.209075\n",
      "\n",
      "[epoch: 21590/100000, batch:    28/   30, ite: 172719] train loss: 0.316938, tar: 0.015955 \n",
      "It 172719\n",
      "l0: 0.012531, l1: 0.012531, l2: 0.013462, l3: 0.014692, l4: 0.021184, l5: 0.128638, l6: 0.194027\n",
      "\n",
      "[epoch: 21590/100000, batch:    32/   30, ite: 172720] train loss: 0.317049, tar: 0.015950 \n",
      "It 172720\n",
      "l0: 0.007019, l1: 0.007032, l2: 0.007700, l3: 0.008128, l4: 0.010680, l5: 0.049730, l6: 0.111627\n",
      "\n",
      "[epoch: 21591/100000, batch:     4/   30, ite: 172721] train loss: 0.316890, tar: 0.015938 \n",
      "It 172721\n",
      "l0: 0.012126, l1: 0.012125, l2: 0.013038, l3: 0.013911, l4: 0.019682, l5: 0.057332, l6: 0.201625\n",
      "\n",
      "[epoch: 21591/100000, batch:     8/   30, ite: 172722] train loss: 0.316908, tar: 0.015933 \n",
      "It 172722\n",
      "l0: 0.009290, l1: 0.009291, l2: 0.010277, l3: 0.011070, l4: 0.014018, l5: 0.032191, l6: 0.138820\n",
      "\n",
      "[epoch: 21591/100000, batch:    12/   30, ite: 172723] train loss: 0.316780, tar: 0.015923 \n",
      "It 172723\n",
      "l0: 0.034346, l1: 0.034447, l2: 0.035818, l3: 0.037842, l4: 0.041964, l5: 0.076452, l6: 0.248825\n",
      "\n",
      "[epoch: 21591/100000, batch:    16/   30, ite: 172724] train loss: 0.317047, tar: 0.015949 \n",
      "It 172724\n",
      "l0: 0.005307, l1: 0.005307, l2: 0.005821, l3: 0.006135, l4: 0.007161, l5: 0.015630, l6: 0.079451\n",
      "\n",
      "[epoch: 21591/100000, batch:    20/   30, ite: 172725] train loss: 0.316782, tar: 0.015934 \n",
      "It 172725\n",
      "l0: 0.035107, l1: 0.035046, l2: 0.036677, l3: 0.039370, l4: 0.044801, l5: 0.094303, l6: 0.292609\n",
      "\n",
      "[epoch: 21591/100000, batch:    24/   30, ite: 172726] train loss: 0.317141, tar: 0.015961 \n",
      "It 172726\n",
      "l0: 0.007819, l1: 0.007818, l2: 0.008526, l3: 0.008957, l4: 0.010800, l5: 0.029924, l6: 0.125654\n",
      "\n",
      "[epoch: 21591/100000, batch:    28/   30, ite: 172727] train loss: 0.316980, tar: 0.015949 \n",
      "It 172727\n",
      "l0: 0.010911, l1: 0.010912, l2: 0.011960, l3: 0.012598, l4: 0.015545, l5: 0.053569, l6: 0.161635\n",
      "\n",
      "[epoch: 21591/100000, batch:    32/   30, ite: 172728] train loss: 0.316925, tar: 0.015942 \n",
      "It 172728\n",
      "l0: 0.010662, l1: 0.010665, l2: 0.011400, l3: 0.012242, l4: 0.017710, l5: 0.067425, l6: 0.181353\n",
      "\n",
      "[epoch: 21592/100000, batch:     4/   30, ite: 172729] train loss: 0.316917, tar: 0.015935 \n",
      "It 172729\n",
      "l0: 0.010004, l1: 0.010008, l2: 0.010934, l3: 0.011750, l4: 0.015741, l5: 0.059955, l6: 0.174202\n",
      "\n",
      "[epoch: 21592/100000, batch:     8/   30, ite: 172730] train loss: 0.316884, tar: 0.015927 \n",
      "It 172730\n",
      "l0: 0.007480, l1: 0.007486, l2: 0.008269, l3: 0.008758, l4: 0.010525, l5: 0.017030, l6: 0.080456\n",
      "\n",
      "[epoch: 21592/100000, batch:    12/   30, ite: 172731] train loss: 0.316642, tar: 0.015915 \n",
      "It 172731\n",
      "l0: 0.034058, l1: 0.033942, l2: 0.035318, l3: 0.037984, l4: 0.043884, l5: 0.076736, l6: 0.181739\n",
      "\n",
      "[epoch: 21592/100000, batch:    16/   30, ite: 172732] train loss: 0.316816, tar: 0.015940 \n",
      "It 172732\n",
      "l0: 0.030638, l1: 0.030688, l2: 0.031770, l3: 0.033965, l4: 0.040102, l5: 0.077838, l6: 0.216859\n",
      "\n",
      "[epoch: 21592/100000, batch:    20/   30, ite: 172733] train loss: 0.317014, tar: 0.015960 \n",
      "It 172733\n",
      "l0: 0.009154, l1: 0.009152, l2: 0.010111, l3: 0.010677, l4: 0.012909, l5: 0.033945, l6: 0.138049\n",
      "\n",
      "[epoch: 21592/100000, batch:    24/   30, ite: 172734] train loss: 0.316887, tar: 0.015951 \n",
      "It 172734\n",
      "l0: 0.010262, l1: 0.010262, l2: 0.011009, l3: 0.011893, l4: 0.014552, l5: 0.047625, l6: 0.181798\n",
      "\n",
      "[epoch: 21592/100000, batch:    28/   30, ite: 172735] train loss: 0.316847, tar: 0.015943 \n",
      "It 172735\n",
      "l0: 0.015275, l1: 0.015277, l2: 0.016813, l3: 0.018118, l4: 0.027373, l5: 0.044093, l6: 0.225086\n",
      "\n",
      "[epoch: 21592/100000, batch:    32/   30, ite: 172736] train loss: 0.316908, tar: 0.015942 \n",
      "It 172736\n",
      "l0: 0.010415, l1: 0.010413, l2: 0.011325, l3: 0.011924, l4: 0.015543, l5: 0.038654, l6: 0.183524\n",
      "\n",
      "[epoch: 21593/100000, batch:     4/   30, ite: 172737] train loss: 0.316860, tar: 0.015935 \n",
      "It 172737\n",
      "l0: 0.057732, l1: 0.057668, l2: 0.059474, l3: 0.063332, l4: 0.071102, l5: 0.135549, l6: 0.269374\n",
      "\n",
      "[epoch: 21593/100000, batch:     8/   30, ite: 172738] train loss: 0.317399, tar: 0.015992 \n",
      "It 172738\n",
      "l0: 0.014085, l1: 0.014083, l2: 0.015315, l3: 0.016509, l4: 0.022039, l5: 0.078845, l6: 0.255995\n",
      "\n",
      "[epoch: 21593/100000, batch:    12/   30, ite: 172739] train loss: 0.317533, tar: 0.015989 \n",
      "It 172739\n",
      "l0: 0.010520, l1: 0.010515, l2: 0.011413, l3: 0.012241, l4: 0.015885, l5: 0.049471, l6: 0.161186\n",
      "\n",
      "[epoch: 21593/100000, batch:    16/   30, ite: 172740] train loss: 0.317471, tar: 0.015982 \n",
      "It 172740\n",
      "l0: 0.006800, l1: 0.006801, l2: 0.007568, l3: 0.007960, l4: 0.009723, l5: 0.023812, l6: 0.082743\n",
      "\n",
      "[epoch: 21593/100000, batch:    20/   30, ite: 172741] train loss: 0.317239, tar: 0.015969 \n",
      "It 172741\n",
      "l0: 0.008412, l1: 0.008416, l2: 0.009104, l3: 0.009749, l4: 0.013577, l5: 0.040456, l6: 0.115164\n",
      "\n",
      "[epoch: 21593/100000, batch:    24/   30, ite: 172742] train loss: 0.317087, tar: 0.015959 \n",
      "It 172742\n",
      "l0: 0.009845, l1: 0.009846, l2: 0.010669, l3: 0.011227, l4: 0.012928, l5: 0.028672, l6: 0.185542\n",
      "\n",
      "[epoch: 21593/100000, batch:    28/   30, ite: 172743] train loss: 0.317022, tar: 0.015951 \n",
      "It 172743\n",
      "l0: 0.003814, l1: 0.003814, l2: 0.004247, l3: 0.004509, l4: 0.005425, l5: 0.012353, l6: 0.064089\n",
      "\n",
      "[epoch: 21593/100000, batch:    32/   30, ite: 172744] train loss: 0.316728, tar: 0.015934 \n",
      "It 172744\n",
      "l0: 0.008791, l1: 0.008793, l2: 0.009373, l3: 0.009979, l4: 0.012977, l5: 0.036889, l6: 0.166873\n",
      "\n",
      "[epoch: 21594/100000, batch:     4/   30, ite: 172745] train loss: 0.316644, tar: 0.015925 \n",
      "It 172745\n",
      "l0: 0.011517, l1: 0.011529, l2: 0.012311, l3: 0.013034, l4: 0.015109, l5: 0.049750, l6: 0.205375\n",
      "\n",
      "[epoch: 21594/100000, batch:     8/   30, ite: 172746] train loss: 0.316646, tar: 0.015919 \n",
      "It 172746\n",
      "l0: 0.032701, l1: 0.032632, l2: 0.034163, l3: 0.035814, l4: 0.040126, l5: 0.080108, l6: 0.254519\n",
      "\n",
      "[epoch: 21594/100000, batch:    12/   30, ite: 172747] train loss: 0.316905, tar: 0.015941 \n",
      "It 172747\n",
      "l0: 0.012585, l1: 0.012587, l2: 0.013745, l3: 0.014785, l4: 0.020746, l5: 0.108533, l6: 0.195807\n",
      "\n",
      "[epoch: 21594/100000, batch:    16/   30, ite: 172748] train loss: 0.316988, tar: 0.015937 \n",
      "It 172748\n",
      "l0: 0.009172, l1: 0.009173, l2: 0.009972, l3: 0.010802, l4: 0.014105, l5: 0.032872, l6: 0.174571\n",
      "\n",
      "[epoch: 21594/100000, batch:    20/   30, ite: 172749] train loss: 0.316913, tar: 0.015928 \n",
      "It 172749\n",
      "l0: 0.007298, l1: 0.007296, l2: 0.008187, l3: 0.008618, l4: 0.010056, l5: 0.019689, l6: 0.069645\n",
      "\n",
      "[epoch: 21594/100000, batch:    24/   30, ite: 172750] train loss: 0.316664, tar: 0.015916 \n",
      "It 172750\n",
      "l0: 0.008941, l1: 0.008941, l2: 0.009812, l3: 0.010494, l4: 0.013472, l5: 0.043351, l6: 0.152463\n",
      "\n",
      "[epoch: 21594/100000, batch:    28/   30, ite: 172751] train loss: 0.316572, tar: 0.015907 \n",
      "It 172751\n",
      "l0: 0.046775, l1: 0.046913, l2: 0.048218, l3: 0.050748, l4: 0.057288, l5: 0.077668, l6: 0.179363\n",
      "\n",
      "[epoch: 21594/100000, batch:    32/   30, ite: 172752] train loss: 0.316826, tar: 0.015948 \n",
      "It 172752\n",
      "l0: 0.008702, l1: 0.008698, l2: 0.009609, l3: 0.010326, l4: 0.013961, l5: 0.047602, l6: 0.123135\n",
      "\n",
      "[epoch: 21595/100000, batch:     4/   30, ite: 172753] train loss: 0.316700, tar: 0.015939 \n",
      "It 172753\n",
      "l0: 0.008344, l1: 0.008341, l2: 0.009207, l3: 0.009782, l4: 0.011513, l5: 0.021168, l6: 0.119685\n",
      "\n",
      "[epoch: 21595/100000, batch:     8/   30, ite: 172754] train loss: 0.316529, tar: 0.015928 \n",
      "It 172754\n",
      "l0: 0.038088, l1: 0.037996, l2: 0.039605, l3: 0.042282, l4: 0.047004, l5: 0.128013, l6: 0.305557\n",
      "\n",
      "[epoch: 21595/100000, batch:    12/   30, ite: 172755] train loss: 0.316956, tar: 0.015958 \n",
      "It 172755\n",
      "l0: 0.012378, l1: 0.012374, l2: 0.013368, l3: 0.014142, l4: 0.016633, l5: 0.043876, l6: 0.240517\n",
      "\n",
      "[epoch: 21595/100000, batch:    16/   30, ite: 172756] train loss: 0.317004, tar: 0.015953 \n",
      "It 172756\n",
      "l0: 0.035050, l1: 0.035107, l2: 0.036512, l3: 0.039730, l4: 0.047056, l5: 0.102761, l6: 0.281344\n",
      "\n",
      "[epoch: 21595/100000, batch:    20/   30, ite: 172757] train loss: 0.317348, tar: 0.015978 \n",
      "It 172757\n",
      "l0: 0.005817, l1: 0.005816, l2: 0.006549, l3: 0.007012, l4: 0.009199, l5: 0.018246, l6: 0.096174\n",
      "\n",
      "[epoch: 21595/100000, batch:    24/   30, ite: 172758] train loss: 0.317125, tar: 0.015965 \n",
      "It 172758\n",
      "l0: 0.008679, l1: 0.008677, l2: 0.009445, l3: 0.010233, l4: 0.013818, l5: 0.045247, l6: 0.127179\n",
      "\n",
      "[epoch: 21595/100000, batch:    28/   30, ite: 172759] train loss: 0.317002, tar: 0.015955 \n",
      "It 172759\n",
      "l0: 0.006298, l1: 0.006300, l2: 0.007181, l3: 0.007861, l4: 0.009565, l5: 0.016541, l6: 0.057219\n",
      "\n",
      "[epoch: 21595/100000, batch:    32/   30, ite: 172760] train loss: 0.316731, tar: 0.015943 \n",
      "It 172760\n",
      "l0: 0.005840, l1: 0.005842, l2: 0.006528, l3: 0.006964, l4: 0.008370, l5: 0.015971, l6: 0.056487\n",
      "\n",
      "[epoch: 21596/100000, batch:     4/   30, ite: 172761] train loss: 0.316454, tar: 0.015929 \n",
      "It 172761\n",
      "l0: 0.010888, l1: 0.010885, l2: 0.011904, l3: 0.012688, l4: 0.016673, l5: 0.037634, l6: 0.201769\n",
      "\n",
      "[epoch: 21596/100000, batch:     8/   30, ite: 172762] train loss: 0.316435, tar: 0.015923 \n",
      "It 172762\n",
      "l0: 0.009684, l1: 0.009685, l2: 0.010484, l3: 0.011185, l4: 0.014061, l5: 0.062761, l6: 0.157918\n",
      "\n",
      "[epoch: 21596/100000, batch:    12/   30, ite: 172763] train loss: 0.316382, tar: 0.015915 \n",
      "It 172763\n",
      "l0: 0.058304, l1: 0.058232, l2: 0.060029, l3: 0.063513, l4: 0.071550, l5: 0.128733, l6: 0.283811\n",
      "\n",
      "[epoch: 21596/100000, batch:    16/   30, ite: 172764] train loss: 0.316916, tar: 0.015970 \n",
      "It 172764\n",
      "l0: 0.007469, l1: 0.007468, l2: 0.008077, l3: 0.008568, l4: 0.011939, l5: 0.036388, l6: 0.133637\n",
      "\n",
      "[epoch: 21596/100000, batch:    20/   30, ite: 172765] train loss: 0.316781, tar: 0.015959 \n",
      "It 172765\n",
      "l0: 0.013627, l1: 0.013625, l2: 0.014678, l3: 0.015516, l4: 0.018760, l5: 0.058941, l6: 0.244102\n",
      "\n",
      "[epoch: 21596/100000, batch:    24/   30, ite: 172766] train loss: 0.316862, tar: 0.015956 \n",
      "It 172766\n",
      "l0: 0.011602, l1: 0.011599, l2: 0.012726, l3: 0.013733, l4: 0.018002, l5: 0.079199, l6: 0.179187\n",
      "\n",
      "[epoch: 21596/100000, batch:    28/   30, ite: 172767] train loss: 0.316874, tar: 0.015950 \n",
      "It 172767\n",
      "l0: 0.006647, l1: 0.006646, l2: 0.007417, l3: 0.007923, l4: 0.010176, l5: 0.019932, l6: 0.104096\n",
      "\n",
      "[epoch: 21596/100000, batch:    32/   30, ite: 172768] train loss: 0.316674, tar: 0.015938 \n",
      "It 172768\n",
      "l0: 0.008376, l1: 0.008374, l2: 0.009207, l3: 0.010103, l4: 0.013740, l5: 0.055578, l6: 0.122647\n",
      "\n",
      "[epoch: 21597/100000, batch:     4/   30, ite: 172769] train loss: 0.316558, tar: 0.015928 \n",
      "It 172769\n",
      "l0: 0.028981, l1: 0.029042, l2: 0.029837, l3: 0.031819, l4: 0.036841, l5: 0.067228, l6: 0.165157\n",
      "\n",
      "[epoch: 21597/100000, batch:     8/   30, ite: 172770] train loss: 0.316652, tar: 0.015945 \n",
      "It 172770\n",
      "l0: 0.011009, l1: 0.011010, l2: 0.011829, l3: 0.012337, l4: 0.015001, l5: 0.080225, l6: 0.194458\n",
      "\n",
      "[epoch: 21597/100000, batch:    12/   30, ite: 172771] train loss: 0.316677, tar: 0.015939 \n",
      "It 172771\n",
      "l0: 0.008177, l1: 0.008178, l2: 0.008976, l3: 0.009499, l4: 0.012471, l5: 0.042563, l6: 0.131003\n",
      "\n",
      "[epoch: 21597/100000, batch:    16/   30, ite: 172772] train loss: 0.316553, tar: 0.015929 \n",
      "It 172772\n",
      "l0: 0.009133, l1: 0.009134, l2: 0.010238, l3: 0.010812, l4: 0.013349, l5: 0.034015, l6: 0.129773\n",
      "\n",
      "[epoch: 21597/100000, batch:    20/   30, ite: 172773] train loss: 0.316424, tar: 0.015920 \n",
      "It 172773\n",
      "l0: 0.007791, l1: 0.007793, l2: 0.008493, l3: 0.008966, l4: 0.012094, l5: 0.023293, l6: 0.115384\n",
      "\n",
      "[epoch: 21597/100000, batch:    24/   30, ite: 172774] train loss: 0.316252, tar: 0.015909 \n",
      "It 172774\n",
      "l0: 0.031725, l1: 0.031664, l2: 0.033348, l3: 0.035424, l4: 0.039498, l5: 0.080520, l6: 0.236052\n",
      "\n",
      "[epoch: 21597/100000, batch:    28/   30, ite: 172775] train loss: 0.316474, tar: 0.015930 \n",
      "It 172775\n",
      "l0: 0.016460, l1: 0.016467, l2: 0.017572, l3: 0.018478, l4: 0.021556, l5: 0.046527, l6: 0.341371\n",
      "\n",
      "[epoch: 21597/100000, batch:    32/   30, ite: 172776] train loss: 0.316683, tar: 0.015931 \n",
      "It 172776\n",
      "l0: 0.027735, l1: 0.027663, l2: 0.029207, l3: 0.031347, l4: 0.036854, l5: 0.059795, l6: 0.132118\n",
      "\n",
      "[epoch: 21598/100000, batch:     4/   30, ite: 172777] train loss: 0.316719, tar: 0.015946 \n",
      "It 172777\n",
      "l0: 0.012054, l1: 0.012055, l2: 0.012960, l3: 0.013573, l4: 0.017275, l5: 0.102428, l6: 0.212407\n",
      "\n",
      "[epoch: 21598/100000, batch:     8/   30, ite: 172778] train loss: 0.316804, tar: 0.015941 \n",
      "It 172778\n",
      "l0: 0.008080, l1: 0.008082, l2: 0.008918, l3: 0.009360, l4: 0.010670, l5: 0.025636, l6: 0.115755\n",
      "\n",
      "[epoch: 21598/100000, batch:    12/   30, ite: 172779] train loss: 0.316637, tar: 0.015931 \n",
      "It 172779\n",
      "l0: 0.007751, l1: 0.007750, l2: 0.008397, l3: 0.008940, l4: 0.011437, l5: 0.030784, l6: 0.124685\n",
      "\n",
      "[epoch: 21598/100000, batch:    16/   30, ite: 172780] train loss: 0.316487, tar: 0.015920 \n",
      "It 172780\n",
      "l0: 0.010153, l1: 0.010152, l2: 0.011041, l3: 0.011571, l4: 0.013127, l5: 0.026760, l6: 0.159983\n",
      "\n",
      "[epoch: 21598/100000, batch:    20/   30, ite: 172781] train loss: 0.316392, tar: 0.015913 \n",
      "It 172781\n",
      "l0: 0.035933, l1: 0.036006, l2: 0.037055, l3: 0.039555, l4: 0.046545, l5: 0.097569, l6: 0.280187\n",
      "\n",
      "[epoch: 21598/100000, batch:    24/   30, ite: 172782] train loss: 0.316720, tar: 0.015938 \n",
      "It 172782\n",
      "l0: 0.009474, l1: 0.009473, l2: 0.010378, l3: 0.011000, l4: 0.014175, l5: 0.042326, l6: 0.153512\n",
      "\n",
      "[epoch: 21598/100000, batch:    28/   30, ite: 172783] train loss: 0.316636, tar: 0.015930 \n",
      "It 172783\n",
      "l0: 0.009599, l1: 0.009604, l2: 0.010382, l3: 0.011420, l4: 0.014411, l5: 0.036157, l6: 0.193478\n",
      "\n",
      "[epoch: 21598/100000, batch:    32/   30, ite: 172784] train loss: 0.316595, tar: 0.015922 \n",
      "It 172784\n",
      "l0: 0.009479, l1: 0.009478, l2: 0.010323, l3: 0.010869, l4: 0.012811, l5: 0.035749, l6: 0.172779\n",
      "\n",
      "[epoch: 21599/100000, batch:     4/   30, ite: 172785] train loss: 0.316525, tar: 0.015914 \n",
      "It 172785\n",
      "l0: 0.027516, l1: 0.027416, l2: 0.028613, l3: 0.030496, l4: 0.031847, l5: 0.052989, l6: 0.125185\n",
      "\n",
      "[epoch: 21599/100000, batch:     8/   30, ite: 172786] train loss: 0.316535, tar: 0.015929 \n",
      "It 172786\n",
      "l0: 0.011489, l1: 0.011483, l2: 0.012450, l3: 0.013272, l4: 0.017254, l5: 0.068709, l6: 0.189636\n",
      "\n",
      "[epoch: 21599/100000, batch:    12/   30, ite: 172787] train loss: 0.316545, tar: 0.015923 \n",
      "It 172787\n",
      "l0: 0.005482, l1: 0.005480, l2: 0.006039, l3: 0.006330, l4: 0.008198, l5: 0.017694, l6: 0.089116\n",
      "\n",
      "[epoch: 21599/100000, batch:    16/   30, ite: 172788] train loss: 0.316318, tar: 0.015910 \n",
      "It 172788\n",
      "l0: 0.033292, l1: 0.033358, l2: 0.034681, l3: 0.037185, l4: 0.043445, l5: 0.076859, l6: 0.259010\n",
      "\n",
      "[epoch: 21599/100000, batch:    20/   30, ite: 172789] train loss: 0.316574, tar: 0.015932 \n",
      "It 172789\n",
      "l0: 0.010519, l1: 0.010520, l2: 0.011347, l3: 0.012011, l4: 0.015619, l5: 0.051430, l6: 0.181748\n",
      "\n",
      "[epoch: 21599/100000, batch:    24/   30, ite: 172790] train loss: 0.316544, tar: 0.015925 \n",
      "It 172790\n",
      "l0: 0.013916, l1: 0.013918, l2: 0.014920, l3: 0.016023, l4: 0.023330, l5: 0.073668, l6: 0.261887\n",
      "\n",
      "[epoch: 21599/100000, batch:    28/   30, ite: 172791] train loss: 0.316672, tar: 0.015922 \n",
      "It 172791\n",
      "l0: 0.009081, l1: 0.009084, l2: 0.010264, l3: 0.011172, l4: 0.013771, l5: 0.022628, l6: 0.091397\n",
      "\n",
      "[epoch: 21599/100000, batch:    32/   30, ite: 172792] train loss: 0.316484, tar: 0.015914 \n",
      "It 172792\n",
      "l0: 0.030096, l1: 0.030019, l2: 0.031351, l3: 0.033142, l4: 0.036423, l5: 0.053945, l6: 0.136933\n",
      "\n",
      "[epoch: 21600/100000, batch:     4/   30, ite: 172793] train loss: 0.316528, tar: 0.015932 \n",
      "It 172793\n",
      "l0: 0.015472, l1: 0.015471, l2: 0.016677, l3: 0.017904, l4: 0.022641, l5: 0.067616, l6: 0.283381\n",
      "\n",
      "[epoch: 21600/100000, batch:     8/   30, ite: 172794] train loss: 0.316683, tar: 0.015931 \n",
      "It 172794\n",
      "l0: 0.009484, l1: 0.009485, l2: 0.010246, l3: 0.010912, l4: 0.014985, l5: 0.065485, l6: 0.155990\n",
      "\n",
      "[epoch: 21600/100000, batch:    12/   30, ite: 172795] train loss: 0.316632, tar: 0.015923 \n",
      "It 172795\n",
      "l0: 0.005979, l1: 0.005980, l2: 0.006574, l3: 0.006858, l4: 0.008080, l5: 0.017594, l6: 0.077075\n",
      "\n",
      "[epoch: 21600/100000, batch:    16/   30, ite: 172796] train loss: 0.316395, tar: 0.015910 \n",
      "It 172796\n",
      "l0: 0.029970, l1: 0.030027, l2: 0.031216, l3: 0.033467, l4: 0.038741, l5: 0.065626, l6: 0.170523\n",
      "\n",
      "[epoch: 21600/100000, batch:    20/   30, ite: 172797] train loss: 0.316500, tar: 0.015928 \n",
      "It 172797\n",
      "l0: 0.013186, l1: 0.013188, l2: 0.014126, l3: 0.014832, l4: 0.018561, l5: 0.105623, l6: 0.243808\n",
      "\n",
      "[epoch: 21600/100000, batch:    24/   30, ite: 172798] train loss: 0.316634, tar: 0.015925 \n",
      "It 172798\n",
      "l0: 0.012191, l1: 0.012190, l2: 0.013299, l3: 0.014036, l4: 0.016238, l5: 0.039213, l6: 0.230240\n",
      "\n",
      "[epoch: 21600/100000, batch:    28/   30, ite: 172799] train loss: 0.316660, tar: 0.015920 \n",
      "It 172799\n",
      "l0: 0.003628, l1: 0.003629, l2: 0.004104, l3: 0.004380, l4: 0.005594, l5: 0.012514, l6: 0.053242\n",
      "\n",
      "[epoch: 21600/100000, batch:    32/   30, ite: 172800] train loss: 0.316373, tar: 0.015905 \n",
      "It 172800\n",
      "l0: 0.007581, l1: 0.007580, l2: 0.008298, l3: 0.008842, l4: 0.010735, l5: 0.038608, l6: 0.129180\n",
      "\n",
      "[epoch: 21601/100000, batch:     4/   30, ite: 172801] train loss: 0.316241, tar: 0.015894 \n",
      "It 172801\n",
      "l0: 0.006086, l1: 0.006083, l2: 0.006665, l3: 0.007170, l4: 0.011400, l5: 0.038940, l6: 0.092787\n",
      "\n",
      "[epoch: 21601/100000, batch:     8/   30, ite: 172802] train loss: 0.316058, tar: 0.015882 \n",
      "It 172802\n",
      "l0: 0.036782, l1: 0.036692, l2: 0.038480, l3: 0.040678, l4: 0.047341, l5: 0.102251, l6: 0.251561\n",
      "\n",
      "[epoch: 21601/100000, batch:    12/   30, ite: 172803] train loss: 0.316354, tar: 0.015908 \n",
      "It 172803\n",
      "l0: 0.007847, l1: 0.007844, l2: 0.008546, l3: 0.009063, l4: 0.011227, l5: 0.034955, l6: 0.108525\n",
      "\n",
      "[epoch: 21601/100000, batch:    16/   30, ite: 172804] train loss: 0.316194, tar: 0.015898 \n",
      "It 172804\n",
      "l0: 0.008987, l1: 0.008986, l2: 0.009844, l3: 0.010593, l4: 0.012408, l5: 0.057231, l6: 0.113788\n",
      "\n",
      "[epoch: 21601/100000, batch:    20/   30, ite: 172805] train loss: 0.316077, tar: 0.015889 \n",
      "It 172805\n",
      "l0: 0.036314, l1: 0.036380, l2: 0.037715, l3: 0.040513, l4: 0.046869, l5: 0.091024, l6: 0.334017\n",
      "\n",
      "[epoch: 21601/100000, batch:    24/   30, ite: 172806] train loss: 0.316457, tar: 0.015915 \n",
      "It 172806\n",
      "l0: 0.009749, l1: 0.009752, l2: 0.010705, l3: 0.011256, l4: 0.013797, l5: 0.027617, l6: 0.150861\n",
      "\n",
      "[epoch: 21601/100000, batch:    28/   30, ite: 172807] train loss: 0.316355, tar: 0.015907 \n",
      "It 172807\n",
      "l0: 0.010775, l1: 0.010777, l2: 0.011820, l3: 0.012433, l4: 0.015825, l5: 0.048259, l6: 0.187716\n",
      "\n",
      "[epoch: 21601/100000, batch:    32/   30, ite: 172808] train loss: 0.316332, tar: 0.015901 \n",
      "It 172808\n",
      "l0: 0.011537, l1: 0.011539, l2: 0.012550, l3: 0.013519, l4: 0.017603, l5: 0.051840, l6: 0.188096\n",
      "\n",
      "[epoch: 21602/100000, batch:     4/   30, ite: 172809] train loss: 0.316320, tar: 0.015895 \n",
      "It 172809\n",
      "l0: 0.007722, l1: 0.007722, l2: 0.008438, l3: 0.008873, l4: 0.010331, l5: 0.037446, l6: 0.113359\n",
      "\n",
      "[epoch: 21602/100000, batch:     8/   30, ite: 172810] train loss: 0.316169, tar: 0.015885 \n",
      "It 172810\n",
      "l0: 0.012400, l1: 0.012399, l2: 0.013534, l3: 0.014715, l4: 0.020291, l5: 0.066753, l6: 0.203471\n",
      "\n",
      "[epoch: 21602/100000, batch:    12/   30, ite: 172811] train loss: 0.316202, tar: 0.015881 \n",
      "It 172811\n",
      "l0: 0.015262, l1: 0.015262, l2: 0.016286, l3: 0.017216, l4: 0.021030, l5: 0.058269, l6: 0.318549\n",
      "\n",
      "[epoch: 21602/100000, batch:    16/   30, ite: 172812] train loss: 0.316382, tar: 0.015880 \n",
      "It 172812\n",
      "l0: 0.031347, l1: 0.031416, l2: 0.032548, l3: 0.034147, l4: 0.041468, l5: 0.073155, l6: 0.181978\n",
      "\n",
      "[epoch: 21602/100000, batch:    20/   30, ite: 172813] train loss: 0.316517, tar: 0.015899 \n",
      "It 172813\n",
      "l0: 0.005905, l1: 0.005905, l2: 0.006517, l3: 0.006903, l4: 0.009129, l5: 0.014931, l6: 0.079158\n",
      "\n",
      "[epoch: 21602/100000, batch:    24/   30, ite: 172814] train loss: 0.316286, tar: 0.015887 \n",
      "It 172814\n",
      "l0: 0.030400, l1: 0.030325, l2: 0.031764, l3: 0.034008, l4: 0.038880, l5: 0.106217, l6: 0.194299\n",
      "\n",
      "[epoch: 21602/100000, batch:    28/   30, ite: 172815] train loss: 0.316469, tar: 0.015905 \n",
      "It 172815\n",
      "l0: 0.003689, l1: 0.003691, l2: 0.004155, l3: 0.004518, l4: 0.005831, l5: 0.013343, l6: 0.056420\n",
      "\n",
      "[epoch: 21602/100000, batch:    32/   30, ite: 172816] train loss: 0.316194, tar: 0.015890 \n",
      "It 172816\n",
      "l0: 0.033116, l1: 0.033043, l2: 0.034423, l3: 0.036583, l4: 0.039632, l5: 0.070869, l6: 0.161958\n",
      "\n",
      "[epoch: 21603/100000, batch:     4/   30, ite: 172817] train loss: 0.316308, tar: 0.015911 \n",
      "It 172817\n",
      "l0: 0.007258, l1: 0.007259, l2: 0.007944, l3: 0.008425, l4: 0.011521, l5: 0.061500, l6: 0.134129\n",
      "\n",
      "[epoch: 21603/100000, batch:     8/   30, ite: 172818] train loss: 0.316212, tar: 0.015900 \n",
      "It 172818\n",
      "l0: 0.038896, l1: 0.038976, l2: 0.040505, l3: 0.043133, l4: 0.048850, l5: 0.106209, l6: 0.293425\n",
      "\n",
      "[epoch: 21603/100000, batch:    12/   30, ite: 172819] train loss: 0.316571, tar: 0.015928 \n",
      "It 172819\n",
      "l0: 0.016304, l1: 0.016303, l2: 0.017461, l3: 0.018505, l4: 0.024251, l5: 0.074343, l6: 0.319497\n",
      "\n",
      "[epoch: 21603/100000, batch:    16/   30, ite: 172820] train loss: 0.316778, tar: 0.015929 \n",
      "It 172820\n",
      "l0: 0.007464, l1: 0.007467, l2: 0.008079, l3: 0.008611, l4: 0.010730, l5: 0.036586, l6: 0.114929\n",
      "\n",
      "[epoch: 21603/100000, batch:    20/   30, ite: 172821] train loss: 0.316629, tar: 0.015918 \n",
      "It 172821\n",
      "l0: 0.003124, l1: 0.003124, l2: 0.003554, l3: 0.003695, l4: 0.004224, l5: 0.006696, l6: 0.027343\n",
      "\n",
      "[epoch: 21603/100000, batch:    24/   30, ite: 172822] train loss: 0.316307, tar: 0.015903 \n",
      "It 172822\n",
      "l0: 0.009520, l1: 0.009522, l2: 0.010601, l3: 0.011295, l4: 0.013572, l5: 0.026719, l6: 0.125671\n",
      "\n",
      "[epoch: 21603/100000, batch:    28/   30, ite: 172823] train loss: 0.316174, tar: 0.015895 \n",
      "It 172823\n",
      "l0: 0.011402, l1: 0.011402, l2: 0.012328, l3: 0.013156, l4: 0.015337, l5: 0.026840, l6: 0.225438\n",
      "\n",
      "[epoch: 21603/100000, batch:    32/   30, ite: 172824] train loss: 0.316173, tar: 0.015890 \n",
      "It 172824\n",
      "l0: 0.008498, l1: 0.008498, l2: 0.009389, l3: 0.010168, l4: 0.013183, l5: 0.041602, l6: 0.114074\n",
      "\n",
      "[epoch: 21604/100000, batch:     4/   30, ite: 172825] train loss: 0.316039, tar: 0.015881 \n",
      "It 172825\n",
      "l0: 0.009934, l1: 0.009933, l2: 0.010727, l3: 0.011922, l4: 0.016526, l5: 0.057641, l6: 0.178343\n",
      "\n",
      "[epoch: 21604/100000, batch:     8/   30, ite: 172826] train loss: 0.316014, tar: 0.015874 \n",
      "It 172826\n",
      "l0: 0.008136, l1: 0.008137, l2: 0.008942, l3: 0.009548, l4: 0.013033, l5: 0.040195, l6: 0.133623\n",
      "\n",
      "[epoch: 21604/100000, batch:    12/   30, ite: 172827] train loss: 0.315899, tar: 0.015864 \n",
      "It 172827\n",
      "l0: 0.010957, l1: 0.010954, l2: 0.011996, l3: 0.012897, l4: 0.016481, l5: 0.041636, l6: 0.151026\n",
      "\n",
      "[epoch: 21604/100000, batch:    16/   30, ite: 172828] train loss: 0.315827, tar: 0.015858 \n",
      "It 172828\n",
      "l0: 0.007813, l1: 0.007811, l2: 0.008567, l3: 0.009328, l4: 0.011433, l5: 0.022872, l6: 0.136640\n",
      "\n",
      "[epoch: 21604/100000, batch:    20/   30, ite: 172829] train loss: 0.315693, tar: 0.015849 \n",
      "It 172829\n",
      "l0: 0.030465, l1: 0.030525, l2: 0.031605, l3: 0.034935, l4: 0.041077, l5: 0.080035, l6: 0.190205\n",
      "\n",
      "[epoch: 21604/100000, batch:    24/   30, ite: 172830] train loss: 0.315841, tar: 0.015866 \n",
      "It 172830\n",
      "l0: 0.033478, l1: 0.033395, l2: 0.035164, l3: 0.037232, l4: 0.040732, l5: 0.079034, l6: 0.282254\n",
      "\n",
      "[epoch: 21604/100000, batch:    28/   30, ite: 172831] train loss: 0.316112, tar: 0.015887 \n",
      "It 172831\n",
      "l0: 0.008095, l1: 0.008072, l2: 0.008597, l3: 0.009105, l4: 0.012118, l5: 0.082017, l6: 0.151252\n",
      "\n",
      "[epoch: 21604/100000, batch:    32/   30, ite: 172832] train loss: 0.316068, tar: 0.015878 \n",
      "It 172832\n",
      "l0: 0.007019, l1: 0.007021, l2: 0.007847, l3: 0.008458, l4: 0.010539, l5: 0.020178, l6: 0.083976\n",
      "\n",
      "[epoch: 21605/100000, batch:     4/   30, ite: 172833] train loss: 0.315863, tar: 0.015867 \n",
      "It 172833\n",
      "l0: 0.008181, l1: 0.008182, l2: 0.009113, l3: 0.009774, l4: 0.012117, l5: 0.034306, l6: 0.127991\n",
      "\n",
      "[epoch: 21605/100000, batch:     8/   30, ite: 172834] train loss: 0.315735, tar: 0.015858 \n",
      "It 172834\n",
      "l0: 0.012634, l1: 0.012633, l2: 0.013438, l3: 0.014337, l4: 0.018724, l5: 0.092355, l6: 0.225192\n",
      "\n",
      "[epoch: 21605/100000, batch:    12/   30, ite: 172835] train loss: 0.315824, tar: 0.015854 \n",
      "It 172835\n",
      "l0: 0.032090, l1: 0.032024, l2: 0.033607, l3: 0.035863, l4: 0.041071, l5: 0.064778, l6: 0.229633\n",
      "\n",
      "[epoch: 21605/100000, batch:    16/   30, ite: 172836] train loss: 0.316007, tar: 0.015874 \n",
      "It 172836\n",
      "l0: 0.008770, l1: 0.008768, l2: 0.009453, l3: 0.010119, l4: 0.012435, l5: 0.035518, l6: 0.150263\n",
      "\n",
      "[epoch: 21605/100000, batch:    20/   30, ite: 172837] train loss: 0.315910, tar: 0.015865 \n",
      "It 172837\n",
      "l0: 0.032448, l1: 0.032514, l2: 0.033897, l3: 0.036645, l4: 0.043616, l5: 0.102678, l6: 0.223056\n",
      "\n",
      "[epoch: 21605/100000, batch:    24/   30, ite: 172838] train loss: 0.316136, tar: 0.015885 \n",
      "It 172838\n",
      "l0: 0.005563, l1: 0.005563, l2: 0.006155, l3: 0.006512, l4: 0.008043, l5: 0.023713, l6: 0.064848\n",
      "\n",
      "[epoch: 21605/100000, batch:    28/   30, ite: 172839] train loss: 0.315903, tar: 0.015873 \n",
      "It 172839\n",
      "l0: 0.016922, l1: 0.016917, l2: 0.018172, l3: 0.019398, l4: 0.027184, l5: 0.081004, l6: 0.343042\n",
      "\n",
      "[epoch: 21605/100000, batch:    32/   30, ite: 172840] train loss: 0.316149, tar: 0.015874 \n",
      "It 172840\n",
      "l0: 0.009270, l1: 0.009268, l2: 0.010018, l3: 0.010583, l4: 0.012876, l5: 0.052303, l6: 0.147329\n",
      "\n",
      "[epoch: 21606/100000, batch:     4/   30, ite: 172841] train loss: 0.316072, tar: 0.015866 \n",
      "It 172841\n",
      "l0: 0.034493, l1: 0.034431, l2: 0.036059, l3: 0.038612, l4: 0.046011, l5: 0.094138, l6: 0.249838\n",
      "\n",
      "[epoch: 21606/100000, batch:     8/   30, ite: 172842] train loss: 0.316330, tar: 0.015888 \n",
      "It 172842\n",
      "l0: 0.007008, l1: 0.007006, l2: 0.007842, l3: 0.008407, l4: 0.010259, l5: 0.019387, l6: 0.091223\n",
      "\n",
      "[epoch: 21606/100000, batch:    12/   30, ite: 172843] train loss: 0.316134, tar: 0.015878 \n",
      "It 172843\n",
      "l0: 0.014987, l1: 0.014981, l2: 0.016024, l3: 0.016947, l4: 0.021614, l5: 0.067860, l6: 0.304248\n",
      "\n",
      "[epoch: 21606/100000, batch:    16/   30, ite: 172844] train loss: 0.316301, tar: 0.015877 \n",
      "It 172844\n",
      "l0: 0.007854, l1: 0.007852, l2: 0.008782, l3: 0.009242, l4: 0.010652, l5: 0.024560, l6: 0.102161\n",
      "\n",
      "[epoch: 21606/100000, batch:    20/   30, ite: 172845] train loss: 0.316129, tar: 0.015867 \n",
      "It 172845\n",
      "l0: 0.003099, l1: 0.003100, l2: 0.003500, l3: 0.003687, l4: 0.004197, l5: 0.006849, l6: 0.025781\n",
      "\n",
      "[epoch: 21606/100000, batch:    24/   30, ite: 172846] train loss: 0.315815, tar: 0.015852 \n",
      "It 172846\n",
      "l0: 0.013708, l1: 0.013709, l2: 0.014774, l3: 0.015786, l4: 0.021779, l5: 0.108230, l6: 0.263186\n",
      "\n",
      "[epoch: 21606/100000, batch:    28/   30, ite: 172847] train loss: 0.315975, tar: 0.015850 \n",
      "It 172847\n",
      "l0: 0.061403, l1: 0.061600, l2: 0.063777, l3: 0.069004, l4: 0.079394, l5: 0.135059, l6: 0.286337\n",
      "\n",
      "[epoch: 21606/100000, batch:    32/   30, ite: 172848] train loss: 0.316494, tar: 0.015903 \n",
      "It 172848\n",
      "l0: 0.036443, l1: 0.036344, l2: 0.037755, l3: 0.039597, l4: 0.046052, l5: 0.098238, l6: 0.265382\n",
      "\n",
      "[epoch: 21607/100000, batch:     4/   30, ite: 172849] train loss: 0.316781, tar: 0.015927 \n",
      "It 172849\n",
      "l0: 0.009195, l1: 0.009193, l2: 0.009991, l3: 0.010581, l4: 0.012519, l5: 0.025300, l6: 0.143422\n",
      "\n",
      "[epoch: 21607/100000, batch:     8/   30, ite: 172850] train loss: 0.316667, tar: 0.015920 \n",
      "It 172850\n",
      "l0: 0.011371, l1: 0.011370, l2: 0.012355, l3: 0.013228, l4: 0.017566, l5: 0.059580, l6: 0.192053\n",
      "\n",
      "[epoch: 21607/100000, batch:    12/   30, ite: 172851] train loss: 0.316668, tar: 0.015914 \n",
      "It 172851\n",
      "l0: 0.035645, l1: 0.035718, l2: 0.036791, l3: 0.039407, l4: 0.046256, l5: 0.110830, l6: 0.272033\n",
      "\n",
      "[epoch: 21607/100000, batch:    16/   30, ite: 172852] train loss: 0.316973, tar: 0.015937 \n",
      "It 172852\n",
      "l0: 0.009350, l1: 0.009350, l2: 0.010272, l3: 0.011152, l4: 0.015193, l5: 0.040455, l6: 0.138824\n",
      "\n",
      "[epoch: 21607/100000, batch:    20/   30, ite: 172853] train loss: 0.316877, tar: 0.015930 \n",
      "It 172853\n",
      "l0: 0.009311, l1: 0.009311, l2: 0.010175, l3: 0.011031, l4: 0.014070, l5: 0.041029, l6: 0.141625\n",
      "\n",
      "[epoch: 21607/100000, batch:    24/   30, ite: 172854] train loss: 0.316783, tar: 0.015922 \n",
      "It 172854\n",
      "l0: 0.006312, l1: 0.006313, l2: 0.006895, l3: 0.007183, l4: 0.008552, l5: 0.016110, l6: 0.102530\n",
      "\n",
      "[epoch: 21607/100000, batch:    28/   30, ite: 172855] train loss: 0.316592, tar: 0.015911 \n",
      "It 172855\n",
      "l0: 0.004014, l1: 0.004024, l2: 0.004447, l3: 0.004607, l4: 0.005300, l5: 0.009000, l6: 0.038538\n",
      "\n",
      "[epoch: 21607/100000, batch:    32/   30, ite: 172856] train loss: 0.316304, tar: 0.015897 \n",
      "It 172856\n",
      "l0: 0.031966, l1: 0.032015, l2: 0.032866, l3: 0.034706, l4: 0.039023, l5: 0.082715, l6: 0.235745\n",
      "\n",
      "[epoch: 21608/100000, batch:     4/   30, ite: 172857] train loss: 0.316506, tar: 0.015915 \n",
      "It 172857\n",
      "l0: 0.008251, l1: 0.008253, l2: 0.009020, l3: 0.009641, l4: 0.012852, l5: 0.023324, l6: 0.134047\n",
      "\n",
      "[epoch: 21608/100000, batch:     8/   30, ite: 172858] train loss: 0.316376, tar: 0.015907 \n",
      "It 172858\n",
      "l0: 0.009244, l1: 0.009245, l2: 0.010180, l3: 0.010911, l4: 0.013475, l5: 0.048026, l6: 0.150959\n",
      "\n",
      "[epoch: 21608/100000, batch:    12/   30, ite: 172859] train loss: 0.316301, tar: 0.015899 \n",
      "It 172859\n",
      "l0: 0.011872, l1: 0.011871, l2: 0.012995, l3: 0.014005, l4: 0.019028, l5: 0.059303, l6: 0.187278\n",
      "\n",
      "[epoch: 21608/100000, batch:    16/   30, ite: 172860] train loss: 0.316301, tar: 0.015894 \n",
      "It 172860\n",
      "l0: 0.034577, l1: 0.034506, l2: 0.036085, l3: 0.039216, l4: 0.043018, l5: 0.085835, l6: 0.225725\n",
      "\n",
      "[epoch: 21608/100000, batch:    20/   30, ite: 172861] train loss: 0.316513, tar: 0.015916 \n",
      "It 172861\n",
      "l0: 0.010371, l1: 0.010369, l2: 0.011343, l3: 0.012339, l4: 0.016210, l5: 0.069777, l6: 0.180860\n",
      "\n",
      "[epoch: 21608/100000, batch:    24/   30, ite: 172862] train loss: 0.316507, tar: 0.015909 \n",
      "It 172862\n",
      "l0: 0.008404, l1: 0.008401, l2: 0.009125, l3: 0.009504, l4: 0.011244, l5: 0.026169, l6: 0.150059\n",
      "\n",
      "[epoch: 21608/100000, batch:    28/   30, ite: 172863] train loss: 0.316399, tar: 0.015901 \n",
      "It 172863\n",
      "l0: 0.003818, l1: 0.003814, l2: 0.004316, l3: 0.004450, l4: 0.006030, l5: 0.010409, l6: 0.041866\n",
      "\n",
      "[epoch: 21608/100000, batch:    32/   30, ite: 172864] train loss: 0.316119, tar: 0.015887 \n",
      "It 172864\n",
      "l0: 0.012840, l1: 0.012838, l2: 0.014037, l3: 0.015024, l4: 0.019596, l5: 0.085332, l6: 0.200910\n",
      "\n",
      "[epoch: 21609/100000, batch:     4/   30, ite: 172865] train loss: 0.316170, tar: 0.015883 \n",
      "It 172865\n",
      "l0: 0.009230, l1: 0.009228, l2: 0.010038, l3: 0.010764, l4: 0.013041, l5: 0.044752, l6: 0.171325\n",
      "\n",
      "[epoch: 21609/100000, batch:     8/   30, ite: 172866] train loss: 0.316115, tar: 0.015875 \n",
      "It 172866\n",
      "l0: 0.008543, l1: 0.008544, l2: 0.009174, l3: 0.009603, l4: 0.011115, l5: 0.035530, l6: 0.149440\n",
      "\n",
      "[epoch: 21609/100000, batch:    12/   30, ite: 172867] train loss: 0.316018, tar: 0.015867 \n",
      "It 172867\n",
      "l0: 0.008877, l1: 0.008875, l2: 0.009625, l3: 0.010170, l4: 0.012541, l5: 0.027493, l6: 0.157728\n",
      "\n",
      "[epoch: 21609/100000, batch:    16/   30, ite: 172868] train loss: 0.315925, tar: 0.015859 \n",
      "It 172868\n",
      "l0: 0.034218, l1: 0.034099, l2: 0.035669, l3: 0.037838, l4: 0.038448, l5: 0.076094, l6: 0.185754\n",
      "\n",
      "[epoch: 21609/100000, batch:    20/   30, ite: 172869] train loss: 0.316070, tar: 0.015880 \n",
      "It 172869\n",
      "l0: 0.032648, l1: 0.032746, l2: 0.033674, l3: 0.036533, l4: 0.044659, l5: 0.099930, l6: 0.256251\n",
      "\n",
      "[epoch: 21609/100000, batch:    24/   30, ite: 172870] train loss: 0.316324, tar: 0.015899 \n",
      "It 172870\n",
      "l0: 0.010540, l1: 0.010540, l2: 0.011602, l3: 0.012443, l4: 0.016532, l5: 0.055813, l6: 0.137793\n",
      "\n",
      "[epoch: 21609/100000, batch:    28/   30, ite: 172871] train loss: 0.316254, tar: 0.015893 \n",
      "It 172871\n",
      "l0: 0.005163, l1: 0.005161, l2: 0.005833, l3: 0.006240, l4: 0.007706, l5: 0.016922, l6: 0.071578\n",
      "\n",
      "[epoch: 21609/100000, batch:    32/   30, ite: 172872] train loss: 0.316027, tar: 0.015881 \n",
      "It 172872\n",
      "l0: 0.033158, l1: 0.033036, l2: 0.034678, l3: 0.036497, l4: 0.038381, l5: 0.071735, l6: 0.186105\n",
      "\n",
      "[epoch: 21610/100000, batch:     4/   30, ite: 172873] train loss: 0.316162, tar: 0.015901 \n",
      "It 172873\n",
      "l0: 0.012405, l1: 0.012400, l2: 0.013201, l3: 0.013880, l4: 0.016919, l5: 0.064843, l6: 0.228199\n",
      "\n",
      "[epoch: 21610/100000, batch:     8/   30, ite: 172874] train loss: 0.316214, tar: 0.015897 \n",
      "It 172874\n",
      "l0: 0.010767, l1: 0.010760, l2: 0.011624, l3: 0.012482, l4: 0.015472, l5: 0.077236, l6: 0.199768\n",
      "\n",
      "[epoch: 21610/100000, batch:    12/   30, ite: 172875] train loss: 0.316239, tar: 0.015891 \n",
      "It 172875\n",
      "l0: 0.006431, l1: 0.006429, l2: 0.006981, l3: 0.007433, l4: 0.010145, l5: 0.045836, l6: 0.088866\n",
      "\n",
      "[epoch: 21610/100000, batch:    16/   30, ite: 172876] train loss: 0.316074, tar: 0.015880 \n",
      "It 172876\n",
      "l0: 0.009844, l1: 0.009842, l2: 0.010864, l3: 0.011618, l4: 0.014581, l5: 0.031223, l6: 0.138697\n",
      "\n",
      "[epoch: 21610/100000, batch:    20/   30, ite: 172877] train loss: 0.315972, tar: 0.015873 \n",
      "It 172877\n",
      "l0: 0.008104, l1: 0.008102, l2: 0.008962, l3: 0.009627, l4: 0.012522, l5: 0.033988, l6: 0.135351\n",
      "\n",
      "[epoch: 21610/100000, batch:    24/   30, ite: 172878] train loss: 0.315859, tar: 0.015864 \n",
      "It 172878\n",
      "l0: 0.034048, l1: 0.034141, l2: 0.035422, l3: 0.037818, l4: 0.044425, l5: 0.078935, l6: 0.240390\n",
      "\n",
      "[epoch: 21610/100000, batch:    28/   30, ite: 172879] train loss: 0.316075, tar: 0.015885 \n",
      "It 172879\n",
      "l0: 0.010545, l1: 0.010541, l2: 0.011400, l3: 0.012457, l4: 0.016250, l5: 0.033174, l6: 0.194531\n",
      "\n",
      "[epoch: 21610/100000, batch:    32/   30, ite: 172880] train loss: 0.316044, tar: 0.015879 \n",
      "It 172880\n",
      "l0: 0.006890, l1: 0.006889, l2: 0.007784, l3: 0.008417, l4: 0.010279, l5: 0.018748, l6: 0.081747\n",
      "\n",
      "[epoch: 21611/100000, batch:     4/   30, ite: 172881] train loss: 0.315845, tar: 0.015869 \n",
      "It 172881\n",
      "l0: 0.029530, l1: 0.029417, l2: 0.030665, l3: 0.032462, l4: 0.034906, l5: 0.056588, l6: 0.127681\n",
      "\n",
      "[epoch: 21611/100000, batch:     8/   30, ite: 172882] train loss: 0.315874, tar: 0.015884 \n",
      "It 172882\n",
      "l0: 0.012948, l1: 0.012945, l2: 0.013796, l3: 0.014787, l4: 0.018166, l5: 0.067884, l6: 0.248774\n",
      "\n",
      "[epoch: 21611/100000, batch:    12/   30, ite: 172883] train loss: 0.315957, tar: 0.015881 \n",
      "It 172883\n",
      "l0: 0.012936, l1: 0.012935, l2: 0.013845, l3: 0.014672, l4: 0.019903, l5: 0.068490, l6: 0.248225\n",
      "\n",
      "[epoch: 21611/100000, batch:    16/   30, ite: 172884] train loss: 0.316042, tar: 0.015878 \n",
      "It 172884\n",
      "l0: 0.035036, l1: 0.035114, l2: 0.036473, l3: 0.039051, l4: 0.047342, l5: 0.107421, l6: 0.287738\n",
      "\n",
      "[epoch: 21611/100000, batch:    20/   30, ite: 172885] train loss: 0.316349, tar: 0.015899 \n",
      "It 172885\n",
      "l0: 0.010676, l1: 0.010676, l2: 0.011518, l3: 0.012120, l4: 0.016094, l5: 0.034238, l6: 0.190120\n",
      "\n",
      "[epoch: 21611/100000, batch:    24/   30, ite: 172886] train loss: 0.316314, tar: 0.015893 \n",
      "It 172886\n",
      "l0: 0.007214, l1: 0.007215, l2: 0.007985, l3: 0.008437, l4: 0.010392, l5: 0.029070, l6: 0.103065\n",
      "\n",
      "[epoch: 21611/100000, batch:    28/   30, ite: 172887] train loss: 0.316153, tar: 0.015884 \n",
      "It 172887\n",
      "l0: 0.003816, l1: 0.003817, l2: 0.004307, l3: 0.004603, l4: 0.005929, l5: 0.009379, l6: 0.046060\n",
      "\n",
      "[epoch: 21611/100000, batch:    32/   30, ite: 172888] train loss: 0.315885, tar: 0.015870 \n",
      "It 172888\n",
      "l0: 0.028758, l1: 0.028689, l2: 0.029918, l3: 0.031615, l4: 0.033720, l5: 0.074878, l6: 0.178395\n",
      "\n",
      "[epoch: 21612/100000, batch:     4/   30, ite: 172889] train loss: 0.315986, tar: 0.015884 \n",
      "It 172889\n",
      "l0: 0.008003, l1: 0.008000, l2: 0.008798, l3: 0.009583, l4: 0.012651, l5: 0.038454, l6: 0.128618\n",
      "\n",
      "[epoch: 21612/100000, batch:     8/   30, ite: 172890] train loss: 0.315872, tar: 0.015876 \n",
      "It 172890\n",
      "l0: 0.011875, l1: 0.011872, l2: 0.012843, l3: 0.013538, l4: 0.016217, l5: 0.045319, l6: 0.220646\n",
      "\n",
      "[epoch: 21612/100000, batch:    12/   30, ite: 172891] train loss: 0.315890, tar: 0.015871 \n",
      "It 172891\n",
      "l0: 0.036355, l1: 0.036459, l2: 0.037689, l3: 0.041054, l4: 0.044634, l5: 0.075698, l6: 0.232715\n",
      "\n",
      "[epoch: 21612/100000, batch:    16/   30, ite: 172892] train loss: 0.316102, tar: 0.015894 \n",
      "It 172892\n",
      "l0: 0.008619, l1: 0.008620, l2: 0.009716, l3: 0.010490, l4: 0.012906, l5: 0.025993, l6: 0.084012\n",
      "\n",
      "[epoch: 21612/100000, batch:    20/   30, ite: 172893] train loss: 0.315927, tar: 0.015886 \n",
      "It 172893\n",
      "l0: 0.010483, l1: 0.010480, l2: 0.011299, l3: 0.011920, l4: 0.013787, l5: 0.062274, l6: 0.168562\n",
      "\n",
      "[epoch: 21612/100000, batch:    24/   30, ite: 172894] train loss: 0.315897, tar: 0.015880 \n",
      "It 172894\n",
      "l0: 0.011428, l1: 0.011427, l2: 0.012239, l3: 0.013177, l4: 0.015877, l5: 0.074455, l6: 0.223239\n",
      "\n",
      "[epoch: 21612/100000, batch:    28/   30, ite: 172895] train loss: 0.315948, tar: 0.015875 \n",
      "It 172895\n",
      "l0: 0.005183, l1: 0.005184, l2: 0.005729, l3: 0.006039, l4: 0.007289, l5: 0.028944, l6: 0.086402\n",
      "\n",
      "[epoch: 21612/100000, batch:    32/   30, ite: 172896] train loss: 0.315757, tar: 0.015863 \n",
      "It 172896\n",
      "l0: 0.008050, l1: 0.008049, l2: 0.008919, l3: 0.009497, l4: 0.011588, l5: 0.019167, l6: 0.110805\n",
      "\n",
      "[epoch: 21613/100000, batch:     4/   30, ite: 172897] train loss: 0.315602, tar: 0.015854 \n",
      "It 172897\n",
      "l0: 0.008522, l1: 0.008523, l2: 0.009053, l3: 0.009448, l4: 0.011328, l5: 0.039371, l6: 0.157059\n",
      "\n",
      "[epoch: 21613/100000, batch:     8/   30, ite: 172898] train loss: 0.315521, tar: 0.015846 \n",
      "It 172898\n",
      "l0: 0.036737, l1: 0.036658, l2: 0.038245, l3: 0.040804, l4: 0.046278, l5: 0.110630, l6: 0.340579\n",
      "\n",
      "[epoch: 21613/100000, batch:    12/   30, ite: 172899] train loss: 0.315893, tar: 0.015869 \n",
      "It 172899\n",
      "l0: 0.009997, l1: 0.009997, l2: 0.010736, l3: 0.011310, l4: 0.013490, l5: 0.041774, l6: 0.201347\n",
      "\n",
      "[epoch: 21613/100000, batch:    16/   30, ite: 172900] train loss: 0.315874, tar: 0.015863 \n",
      "It 172900\n",
      "l0: 0.029456, l1: 0.029544, l2: 0.030479, l3: 0.032626, l4: 0.037225, l5: 0.056205, l6: 0.133970\n",
      "\n",
      "[epoch: 21613/100000, batch:    20/   30, ite: 172901] train loss: 0.315911, tar: 0.015878 \n",
      "It 172901\n",
      "l0: 0.007835, l1: 0.007836, l2: 0.008696, l3: 0.009182, l4: 0.011231, l5: 0.028499, l6: 0.117319\n",
      "\n",
      "[epoch: 21613/100000, batch:    24/   30, ite: 172902] train loss: 0.315772, tar: 0.015869 \n",
      "It 172902\n",
      "l0: 0.011487, l1: 0.011486, l2: 0.012413, l3: 0.013185, l4: 0.016816, l5: 0.075978, l6: 0.180504\n",
      "\n",
      "[epoch: 21613/100000, batch:    28/   30, ite: 172903] train loss: 0.315779, tar: 0.015864 \n",
      "It 172903\n",
      "l0: 0.010166, l1: 0.010167, l2: 0.011004, l3: 0.011700, l4: 0.015040, l5: 0.056054, l6: 0.163335\n",
      "\n",
      "[epoch: 21613/100000, batch:    32/   30, ite: 172904] train loss: 0.315737, tar: 0.015858 \n",
      "It 172904\n",
      "l0: 0.028582, l1: 0.028655, l2: 0.029889, l3: 0.032256, l4: 0.037242, l5: 0.062805, l6: 0.167749\n",
      "\n",
      "[epoch: 21614/100000, batch:     4/   30, ite: 172905] train loss: 0.315816, tar: 0.015872 \n",
      "It 172905\n",
      "l0: 0.008775, l1: 0.008773, l2: 0.009484, l3: 0.010106, l4: 0.013543, l5: 0.042399, l6: 0.147267\n",
      "\n",
      "[epoch: 21614/100000, batch:     8/   30, ite: 172906] train loss: 0.315732, tar: 0.015864 \n",
      "It 172906\n",
      "l0: 0.036357, l1: 0.036258, l2: 0.037835, l3: 0.039892, l4: 0.042007, l5: 0.092193, l6: 0.285734\n",
      "\n",
      "[epoch: 21614/100000, batch:    12/   30, ite: 172907] train loss: 0.316013, tar: 0.015887 \n",
      "It 172907\n",
      "l0: 0.010302, l1: 0.010298, l2: 0.011211, l3: 0.011922, l4: 0.014485, l5: 0.042131, l6: 0.175294\n",
      "\n",
      "[epoch: 21614/100000, batch:    16/   30, ite: 172908] train loss: 0.315968, tar: 0.015880 \n",
      "It 172908\n",
      "l0: 0.010693, l1: 0.010693, l2: 0.011658, l3: 0.012383, l4: 0.014439, l5: 0.046395, l6: 0.170272\n",
      "\n",
      "[epoch: 21614/100000, batch:    20/   30, ite: 172909] train loss: 0.315925, tar: 0.015875 \n",
      "It 172909\n",
      "l0: 0.011032, l1: 0.011032, l2: 0.012017, l3: 0.012797, l4: 0.015805, l5: 0.068585, l6: 0.182042\n",
      "\n",
      "[epoch: 21614/100000, batch:    24/   30, ite: 172910] train loss: 0.315922, tar: 0.015869 \n",
      "It 172910\n",
      "l0: 0.003571, l1: 0.003570, l2: 0.004026, l3: 0.004330, l4: 0.005169, l5: 0.008349, l6: 0.033008\n",
      "\n",
      "[epoch: 21614/100000, batch:    28/   30, ite: 172911] train loss: 0.315644, tar: 0.015856 \n",
      "It 172911\n",
      "l0: 0.014594, l1: 0.014594, l2: 0.015741, l3: 0.017004, l4: 0.024001, l5: 0.107044, l6: 0.252260\n",
      "\n",
      "[epoch: 21614/100000, batch:    32/   30, ite: 172912] train loss: 0.315786, tar: 0.015855 \n",
      "It 172912\n",
      "l0: 0.033073, l1: 0.032978, l2: 0.034367, l3: 0.036357, l4: 0.040665, l5: 0.081002, l6: 0.242888\n",
      "\n",
      "[epoch: 21615/100000, batch:     4/   30, ite: 172913] train loss: 0.315989, tar: 0.015873 \n",
      "It 172913\n",
      "l0: 0.008009, l1: 0.008011, l2: 0.008841, l3: 0.009425, l4: 0.011438, l5: 0.029172, l6: 0.110987\n",
      "\n",
      "[epoch: 21615/100000, batch:     8/   30, ite: 172914] train loss: 0.315846, tar: 0.015865 \n",
      "It 172914\n",
      "l0: 0.010796, l1: 0.010797, l2: 0.011787, l3: 0.012477, l4: 0.015462, l5: 0.044085, l6: 0.188590\n",
      "\n",
      "[epoch: 21615/100000, batch:    12/   30, ite: 172915] train loss: 0.315823, tar: 0.015859 \n",
      "It 172915\n",
      "l0: 0.007434, l1: 0.007435, l2: 0.008321, l3: 0.008836, l4: 0.010763, l5: 0.021197, l6: 0.094919\n",
      "\n",
      "[epoch: 21615/100000, batch:    16/   30, ite: 172916] train loss: 0.315651, tar: 0.015850 \n",
      "It 172916\n",
      "l0: 0.010854, l1: 0.010855, l2: 0.011801, l3: 0.012661, l4: 0.016168, l5: 0.062073, l6: 0.197444\n",
      "\n",
      "[epoch: 21615/100000, batch:    20/   30, ite: 172917] train loss: 0.315658, tar: 0.015845 \n",
      "It 172917\n",
      "l0: 0.026851, l1: 0.026923, l2: 0.027774, l3: 0.030782, l4: 0.036760, l5: 0.056026, l6: 0.125208\n",
      "\n",
      "[epoch: 21615/100000, batch:    24/   30, ite: 172918] train loss: 0.315674, tar: 0.015857 \n",
      "It 172918\n",
      "l0: 0.011028, l1: 0.011028, l2: 0.011875, l3: 0.012648, l4: 0.016600, l5: 0.062240, l6: 0.190809\n",
      "\n",
      "[epoch: 21615/100000, batch:    28/   30, ite: 172919] train loss: 0.315675, tar: 0.015851 \n",
      "It 172919\n",
      "l0: 0.012942, l1: 0.012942, l2: 0.013711, l3: 0.014435, l4: 0.019733, l5: 0.122127, l6: 0.248280\n",
      "\n",
      "[epoch: 21615/100000, batch:    32/   30, ite: 172920] train loss: 0.315814, tar: 0.015848 \n",
      "It 172920\n",
      "l0: 0.033580, l1: 0.033659, l2: 0.034499, l3: 0.036278, l4: 0.040685, l5: 0.081857, l6: 0.203622\n",
      "\n",
      "[epoch: 21616/100000, batch:     4/   30, ite: 172921] train loss: 0.315975, tar: 0.015867 \n",
      "It 172921\n",
      "l0: 0.009168, l1: 0.009165, l2: 0.009965, l3: 0.010584, l4: 0.012643, l5: 0.023546, l6: 0.153944\n",
      "\n",
      "[epoch: 21616/100000, batch:     8/   30, ite: 172922] train loss: 0.315881, tar: 0.015860 \n",
      "It 172922\n",
      "l0: 0.011049, l1: 0.011047, l2: 0.012143, l3: 0.012967, l4: 0.015702, l5: 0.048769, l6: 0.180455\n",
      "\n",
      "[epoch: 21616/100000, batch:    12/   30, ite: 172923] train loss: 0.315855, tar: 0.015855 \n",
      "It 172923\n",
      "l0: 0.005874, l1: 0.005871, l2: 0.006569, l3: 0.007309, l4: 0.008987, l5: 0.019446, l6: 0.064878\n",
      "\n",
      "[epoch: 21616/100000, batch:    16/   30, ite: 172924] train loss: 0.315642, tar: 0.015844 \n",
      "It 172924\n",
      "l0: 0.034057, l1: 0.033959, l2: 0.035538, l3: 0.037653, l4: 0.042261, l5: 0.111326, l6: 0.228816\n",
      "\n",
      "[epoch: 21616/100000, batch:    20/   30, ite: 172925] train loss: 0.315867, tar: 0.015864 \n",
      "It 172925\n",
      "l0: 0.011719, l1: 0.011718, l2: 0.012649, l3: 0.013491, l4: 0.018511, l5: 0.071110, l6: 0.195958\n",
      "\n",
      "[epoch: 21616/100000, batch:    24/   30, ite: 172926] train loss: 0.315888, tar: 0.015859 \n",
      "It 172926\n",
      "l0: 0.006531, l1: 0.006530, l2: 0.007249, l3: 0.007728, l4: 0.009931, l5: 0.021827, l6: 0.092326\n",
      "\n",
      "[epoch: 21616/100000, batch:    28/   30, ite: 172927] train loss: 0.315711, tar: 0.015849 \n",
      "It 172927\n",
      "l0: 0.016291, l1: 0.016295, l2: 0.017412, l3: 0.018199, l4: 0.021577, l5: 0.049187, l6: 0.353927\n",
      "\n",
      "[epoch: 21616/100000, batch:    32/   30, ite: 172928] train loss: 0.315902, tar: 0.015850 \n",
      "It 172928\n",
      "l0: 0.007872, l1: 0.007874, l2: 0.008431, l3: 0.008769, l4: 0.010343, l5: 0.032514, l6: 0.155099\n",
      "\n",
      "[epoch: 21617/100000, batch:     4/   30, ite: 172929] train loss: 0.315811, tar: 0.015841 \n",
      "It 172929\n",
      "l0: 0.032406, l1: 0.032330, l2: 0.034148, l3: 0.036266, l4: 0.040945, l5: 0.070702, l6: 0.204596\n",
      "\n",
      "[epoch: 21617/100000, batch:     8/   30, ite: 172930] train loss: 0.315956, tar: 0.015859 \n",
      "It 172930\n",
      "l0: 0.007888, l1: 0.007891, l2: 0.008543, l3: 0.009008, l4: 0.010697, l5: 0.041354, l6: 0.118039\n",
      "\n",
      "[epoch: 21617/100000, batch:    12/   30, ite: 172931] train loss: 0.315836, tar: 0.015850 \n",
      "It 172931\n",
      "l0: 0.010849, l1: 0.010854, l2: 0.011980, l3: 0.012919, l4: 0.016465, l5: 0.067373, l6: 0.155004\n",
      "\n",
      "[epoch: 21617/100000, batch:    16/   30, ite: 172932] train loss: 0.315803, tar: 0.015845 \n",
      "It 172932\n",
      "l0: 0.011501, l1: 0.011503, l2: 0.012513, l3: 0.013722, l4: 0.018378, l5: 0.061438, l6: 0.192392\n",
      "\n",
      "[epoch: 21617/100000, batch:    20/   30, ite: 172933] train loss: 0.315809, tar: 0.015840 \n",
      "It 172933\n",
      "l0: 0.008159, l1: 0.008158, l2: 0.008928, l3: 0.009559, l4: 0.012672, l5: 0.044500, l6: 0.145054\n",
      "\n",
      "[epoch: 21617/100000, batch:    24/   30, ite: 172934] train loss: 0.315725, tar: 0.015832 \n",
      "It 172934\n",
      "l0: 0.008842, l1: 0.008840, l2: 0.009596, l3: 0.010145, l4: 0.013162, l5: 0.036242, l6: 0.129552\n",
      "\n",
      "[epoch: 21617/100000, batch:    28/   30, ite: 172935] train loss: 0.315618, tar: 0.015825 \n",
      "It 172935\n",
      "l0: 0.054039, l1: 0.054163, l2: 0.055701, l3: 0.059567, l4: 0.070239, l5: 0.101918, l6: 0.339520\n",
      "\n",
      "[epoch: 21617/100000, batch:    32/   30, ite: 172936] train loss: 0.316067, tar: 0.015866 \n",
      "It 172936\n",
      "l0: 0.005656, l1: 0.005655, l2: 0.006237, l3: 0.006531, l4: 0.007605, l5: 0.014105, l6: 0.084587\n",
      "\n",
      "[epoch: 21618/100000, batch:     4/   30, ite: 172937] train loss: 0.315868, tar: 0.015855 \n",
      "It 172937\n",
      "l0: 0.012989, l1: 0.012993, l2: 0.013925, l3: 0.014680, l4: 0.020102, l5: 0.084649, l6: 0.242114\n",
      "\n",
      "[epoch: 21618/100000, batch:     8/   30, ite: 172938] train loss: 0.315960, tar: 0.015852 \n",
      "It 172938\n",
      "l0: 0.029564, l1: 0.029642, l2: 0.030534, l3: 0.032742, l4: 0.037671, l5: 0.074102, l6: 0.172870\n",
      "\n",
      "[epoch: 21618/100000, batch:    12/   30, ite: 172939] train loss: 0.316057, tar: 0.015866 \n",
      "It 172939\n",
      "l0: 0.006000, l1: 0.005996, l2: 0.006545, l3: 0.006875, l4: 0.008066, l5: 0.017528, l6: 0.073680\n",
      "\n",
      "[epoch: 21618/100000, batch:    16/   30, ite: 172940] train loss: 0.315853, tar: 0.015856 \n",
      "It 172940\n",
      "l0: 0.038188, l1: 0.038085, l2: 0.039801, l3: 0.042775, l4: 0.049436, l5: 0.087975, l6: 0.285827\n",
      "\n",
      "[epoch: 21618/100000, batch:    20/   30, ite: 172941] train loss: 0.316136, tar: 0.015879 \n",
      "It 172941\n",
      "l0: 0.010797, l1: 0.010794, l2: 0.011849, l3: 0.012811, l4: 0.015567, l5: 0.030972, l6: 0.191082\n",
      "\n",
      "[epoch: 21618/100000, batch:    24/   30, ite: 172942] train loss: 0.316102, tar: 0.015874 \n",
      "It 172942\n",
      "l0: 0.010795, l1: 0.010796, l2: 0.011768, l3: 0.012547, l4: 0.016455, l5: 0.063477, l6: 0.181743\n",
      "\n",
      "[epoch: 21618/100000, batch:    28/   30, ite: 172943] train loss: 0.316093, tar: 0.015869 \n",
      "It 172943\n",
      "l0: 0.012559, l1: 0.012564, l2: 0.013660, l3: 0.014402, l4: 0.017581, l5: 0.039685, l6: 0.216807\n",
      "\n",
      "[epoch: 21618/100000, batch:    32/   30, ite: 172944] train loss: 0.316105, tar: 0.015865 \n",
      "It 172944\n",
      "l0: 0.007418, l1: 0.007420, l2: 0.007904, l3: 0.008353, l4: 0.009949, l5: 0.059705, l6: 0.131897\n",
      "\n",
      "[epoch: 21619/100000, batch:     4/   30, ite: 172945] train loss: 0.316016, tar: 0.015856 \n",
      "It 172945\n",
      "l0: 0.008703, l1: 0.008705, l2: 0.009742, l3: 0.010403, l4: 0.013406, l5: 0.025977, l6: 0.103094\n",
      "\n",
      "[epoch: 21619/100000, batch:     8/   30, ite: 172946] train loss: 0.315873, tar: 0.015849 \n",
      "It 172946\n",
      "l0: 0.011947, l1: 0.011951, l2: 0.012994, l3: 0.014002, l4: 0.019109, l5: 0.071807, l6: 0.221476\n",
      "\n",
      "[epoch: 21619/100000, batch:    12/   30, ite: 172947] train loss: 0.315923, tar: 0.015845 \n",
      "It 172947\n",
      "l0: 0.029644, l1: 0.029734, l2: 0.030542, l3: 0.032754, l4: 0.038484, l5: 0.058629, l6: 0.132080\n",
      "\n",
      "[epoch: 21619/100000, batch:    16/   30, ite: 172948] train loss: 0.315961, tar: 0.015859 \n",
      "It 172948\n",
      "l0: 0.007580, l1: 0.007582, l2: 0.008231, l3: 0.008630, l4: 0.010685, l5: 0.034015, l6: 0.108345\n",
      "\n",
      "[epoch: 21619/100000, batch:    20/   30, ite: 172949] train loss: 0.315823, tar: 0.015850 \n",
      "It 172949\n",
      "l0: 0.011312, l1: 0.011311, l2: 0.012234, l3: 0.012988, l4: 0.016655, l5: 0.033211, l6: 0.215126\n",
      "\n",
      "[epoch: 21619/100000, batch:    24/   30, ite: 172950] train loss: 0.315820, tar: 0.015846 \n",
      "It 172950\n",
      "l0: 0.033123, l1: 0.033038, l2: 0.034673, l3: 0.037226, l4: 0.042037, l5: 0.087264, l6: 0.284560\n",
      "\n",
      "[epoch: 21619/100000, batch:    28/   30, ite: 172951] train loss: 0.316068, tar: 0.015864 \n",
      "It 172951\n",
      "l0: 0.013580, l1: 0.013575, l2: 0.014724, l3: 0.015518, l4: 0.021661, l5: 0.064295, l6: 0.200722\n",
      "\n",
      "[epoch: 21619/100000, batch:    32/   30, ite: 172952] train loss: 0.316097, tar: 0.015861 \n",
      "It 172952\n",
      "l0: 0.013969, l1: 0.013961, l2: 0.014981, l3: 0.015644, l4: 0.018071, l5: 0.058968, l6: 0.282297\n",
      "\n",
      "[epoch: 21620/100000, batch:     4/   30, ite: 172953] train loss: 0.316204, tar: 0.015859 \n",
      "It 172953\n",
      "l0: 0.008527, l1: 0.008521, l2: 0.009271, l3: 0.009847, l4: 0.014284, l5: 0.055008, l6: 0.117238\n",
      "\n",
      "[epoch: 21620/100000, batch:     8/   30, ite: 172954] train loss: 0.316106, tar: 0.015852 \n",
      "It 172954\n",
      "l0: 0.006262, l1: 0.006258, l2: 0.006962, l3: 0.007338, l4: 0.008919, l5: 0.020036, l6: 0.089004\n",
      "\n",
      "[epoch: 21620/100000, batch:    12/   30, ite: 172955] train loss: 0.315927, tar: 0.015842 \n",
      "It 172955\n",
      "l0: 0.033914, l1: 0.034015, l2: 0.034870, l3: 0.037295, l4: 0.043525, l5: 0.074928, l6: 0.221277\n",
      "\n",
      "[epoch: 21620/100000, batch:    16/   30, ite: 172956] train loss: 0.316098, tar: 0.015861 \n",
      "It 172956\n",
      "l0: 0.034512, l1: 0.034378, l2: 0.036253, l3: 0.038938, l4: 0.042958, l5: 0.068937, l6: 0.213895\n",
      "\n",
      "[epoch: 21620/100000, batch:    20/   30, ite: 172957] train loss: 0.316259, tar: 0.015880 \n",
      "It 172957\n",
      "l0: 0.006843, l1: 0.006845, l2: 0.007512, l3: 0.008126, l4: 0.011118, l5: 0.033868, l6: 0.124272\n",
      "\n",
      "[epoch: 21620/100000, batch:    24/   30, ite: 172958] train loss: 0.316136, tar: 0.015871 \n",
      "It 172958\n",
      "l0: 0.008591, l1: 0.008595, l2: 0.009440, l3: 0.010124, l4: 0.012433, l5: 0.032041, l6: 0.136428\n",
      "\n",
      "[epoch: 21620/100000, batch:    28/   30, ite: 172959] train loss: 0.316033, tar: 0.015863 \n",
      "It 172959\n",
      "l0: 0.016413, l1: 0.016419, l2: 0.017440, l3: 0.019221, l4: 0.027999, l5: 0.116664, l6: 0.283188\n",
      "\n",
      "[epoch: 21620/100000, batch:    32/   30, ite: 172960] train loss: 0.316222, tar: 0.015864 \n",
      "It 172960\n",
      "l0: 0.035900, l1: 0.035803, l2: 0.037501, l3: 0.039564, l4: 0.044503, l5: 0.123272, l6: 0.285182\n",
      "\n",
      "[epoch: 21621/100000, batch:     4/   30, ite: 172961] train loss: 0.316519, tar: 0.015884 \n",
      "It 172961\n",
      "l0: 0.009303, l1: 0.009303, l2: 0.010099, l3: 0.010536, l4: 0.012759, l5: 0.025895, l6: 0.146605\n",
      "\n",
      "[epoch: 21621/100000, batch:     8/   30, ite: 172962] train loss: 0.316424, tar: 0.015878 \n",
      "It 172962\n",
      "l0: 0.009989, l1: 0.009989, l2: 0.010816, l3: 0.011450, l4: 0.013649, l5: 0.055117, l6: 0.179915\n",
      "\n",
      "[epoch: 21621/100000, batch:    12/   30, ite: 172963] train loss: 0.316397, tar: 0.015871 \n",
      "It 172963\n",
      "l0: 0.032561, l1: 0.032645, l2: 0.033708, l3: 0.035522, l4: 0.041027, l5: 0.083248, l6: 0.250242\n",
      "\n",
      "[epoch: 21621/100000, batch:    16/   30, ite: 172964] train loss: 0.316597, tar: 0.015889 \n",
      "It 172964\n",
      "l0: 0.006725, l1: 0.006721, l2: 0.007343, l3: 0.007859, l4: 0.010006, l5: 0.048261, l6: 0.120427\n",
      "\n",
      "[epoch: 21621/100000, batch:    20/   30, ite: 172965] train loss: 0.316484, tar: 0.015879 \n",
      "It 172965\n",
      "l0: 0.010113, l1: 0.010115, l2: 0.010988, l3: 0.011582, l4: 0.013931, l5: 0.033809, l6: 0.175159\n",
      "\n",
      "[epoch: 21621/100000, batch:    24/   30, ite: 172966] train loss: 0.316431, tar: 0.015873 \n",
      "It 172966\n",
      "l0: 0.007707, l1: 0.007704, l2: 0.008505, l3: 0.009399, l4: 0.010868, l5: 0.023882, l6: 0.124789\n",
      "\n",
      "[epoch: 21621/100000, batch:    28/   30, ite: 172967] train loss: 0.316303, tar: 0.015865 \n",
      "It 172967\n",
      "l0: 0.006461, l1: 0.006459, l2: 0.007391, l3: 0.008023, l4: 0.009776, l5: 0.015184, l6: 0.054182\n",
      "\n",
      "[epoch: 21621/100000, batch:    32/   30, ite: 172968] train loss: 0.316087, tar: 0.015855 \n",
      "It 172968\n",
      "l0: 0.007155, l1: 0.007152, l2: 0.007873, l3: 0.008259, l4: 0.009658, l5: 0.028932, l6: 0.119803\n",
      "\n",
      "[epoch: 21622/100000, batch:     4/   30, ite: 172969] train loss: 0.315956, tar: 0.015846 \n",
      "It 172969\n",
      "l0: 0.011255, l1: 0.011256, l2: 0.012195, l3: 0.012997, l4: 0.016011, l5: 0.045490, l6: 0.195739\n",
      "\n",
      "[epoch: 21622/100000, batch:     8/   30, ite: 172970] train loss: 0.315945, tar: 0.015841 \n",
      "It 172970\n",
      "l0: 0.007940, l1: 0.007936, l2: 0.008728, l3: 0.009313, l4: 0.011282, l5: 0.060025, l6: 0.121266\n",
      "\n",
      "[epoch: 21622/100000, batch:    12/   30, ite: 172971] train loss: 0.315853, tar: 0.015833 \n",
      "It 172971\n",
      "l0: 0.029784, l1: 0.029853, l2: 0.030877, l3: 0.032742, l4: 0.038798, l5: 0.095617, l6: 0.209511\n",
      "\n",
      "[epoch: 21622/100000, batch:    16/   30, ite: 172972] train loss: 0.316008, tar: 0.015848 \n",
      "It 172972\n",
      "l0: 0.012345, l1: 0.012346, l2: 0.013373, l3: 0.014114, l4: 0.017738, l5: 0.031913, l6: 0.209115\n",
      "\n",
      "[epoch: 21622/100000, batch:    20/   30, ite: 172973] train loss: 0.316003, tar: 0.015844 \n",
      "It 172973\n",
      "l0: 0.006256, l1: 0.006257, l2: 0.006812, l3: 0.007124, l4: 0.008486, l5: 0.021307, l6: 0.104366\n",
      "\n",
      "[epoch: 21622/100000, batch:    24/   30, ite: 172974] train loss: 0.315844, tar: 0.015834 \n",
      "It 172974\n",
      "l0: 0.031833, l1: 0.031741, l2: 0.033240, l3: 0.035008, l4: 0.038226, l5: 0.076907, l6: 0.178288\n",
      "\n",
      "[epoch: 21622/100000, batch:    28/   30, ite: 172975] train loss: 0.315956, tar: 0.015851 \n",
      "It 172975\n",
      "l0: 0.016579, l1: 0.016585, l2: 0.017831, l3: 0.018913, l4: 0.024652, l5: 0.104600, l6: 0.325964\n",
      "\n",
      "[epoch: 21622/100000, batch:    32/   30, ite: 172976] train loss: 0.316170, tar: 0.015851 \n",
      "It 172976\n",
      "l0: 0.035177, l1: 0.035100, l2: 0.036546, l3: 0.037828, l4: 0.040748, l5: 0.116886, l6: 0.269406\n",
      "\n",
      "[epoch: 21623/100000, batch:     4/   30, ite: 172977] train loss: 0.316432, tar: 0.015871 \n",
      "It 172977\n",
      "l0: 0.032449, l1: 0.032548, l2: 0.033385, l3: 0.035333, l4: 0.041249, l5: 0.066964, l6: 0.146660\n",
      "\n",
      "[epoch: 21623/100000, batch:     8/   30, ite: 172978] train loss: 0.316505, tar: 0.015888 \n",
      "It 172978\n",
      "l0: 0.007488, l1: 0.007490, l2: 0.008316, l3: 0.008917, l4: 0.011542, l5: 0.030177, l6: 0.094161\n",
      "\n",
      "[epoch: 21623/100000, batch:    12/   30, ite: 172979] train loss: 0.316354, tar: 0.015880 \n",
      "It 172979\n",
      "l0: 0.011412, l1: 0.011411, l2: 0.012255, l3: 0.012948, l4: 0.014675, l5: 0.034818, l6: 0.231402\n",
      "\n",
      "[epoch: 21623/100000, batch:    16/   30, ite: 172980] train loss: 0.316367, tar: 0.015875 \n",
      "It 172980\n",
      "l0: 0.012163, l1: 0.012160, l2: 0.013426, l3: 0.014354, l4: 0.017640, l5: 0.053636, l6: 0.209274\n",
      "\n",
      "[epoch: 21623/100000, batch:    20/   30, ite: 172981] train loss: 0.316383, tar: 0.015871 \n",
      "It 172981\n",
      "l0: 0.007599, l1: 0.007591, l2: 0.008170, l3: 0.008848, l4: 0.011192, l5: 0.041610, l6: 0.144095\n",
      "\n",
      "[epoch: 21623/100000, batch:    24/   30, ite: 172982] train loss: 0.316294, tar: 0.015863 \n",
      "It 172982\n",
      "l0: 0.007940, l1: 0.007929, l2: 0.008677, l3: 0.009361, l4: 0.011360, l5: 0.035617, l6: 0.121901\n",
      "\n",
      "[epoch: 21623/100000, batch:    28/   30, ite: 172983] train loss: 0.316179, tar: 0.015855 \n",
      "It 172983\n",
      "l0: 0.009295, l1: 0.009292, l2: 0.010131, l3: 0.010693, l4: 0.013497, l5: 0.025876, l6: 0.172168\n",
      "\n",
      "[epoch: 21623/100000, batch:    32/   30, ite: 172984] train loss: 0.316113, tar: 0.015848 \n",
      "It 172984\n",
      "l0: 0.029308, l1: 0.029413, l2: 0.030071, l3: 0.031919, l4: 0.035649, l5: 0.071997, l6: 0.190855\n",
      "\n",
      "[epoch: 21624/100000, batch:     4/   30, ite: 172985] train loss: 0.316217, tar: 0.015862 \n",
      "It 172985\n",
      "l0: 0.010896, l1: 0.010894, l2: 0.011748, l3: 0.012530, l4: 0.015920, l5: 0.065990, l6: 0.192513\n",
      "\n",
      "[epoch: 21624/100000, batch:     8/   30, ite: 172986] train loss: 0.316222, tar: 0.015857 \n",
      "It 172986\n",
      "l0: 0.017642, l1: 0.017643, l2: 0.018776, l3: 0.019992, l4: 0.028787, l5: 0.102401, l6: 0.357838\n",
      "\n",
      "[epoch: 21624/100000, batch:    12/   30, ite: 172987] train loss: 0.316472, tar: 0.015858 \n",
      "It 172987\n",
      "l0: 0.004235, l1: 0.004234, l2: 0.004797, l3: 0.005050, l4: 0.006219, l5: 0.009873, l6: 0.022759\n",
      "\n",
      "[epoch: 21624/100000, batch:    16/   30, ite: 172988] train loss: 0.316209, tar: 0.015847 \n",
      "It 172988\n",
      "l0: 0.030067, l1: 0.029964, l2: 0.031447, l3: 0.033785, l4: 0.036637, l5: 0.065685, l6: 0.160044\n",
      "\n",
      "[epoch: 21624/100000, batch:    20/   30, ite: 172989] train loss: 0.316281, tar: 0.015861 \n",
      "It 172989\n",
      "l0: 0.012594, l1: 0.012597, l2: 0.013665, l3: 0.014497, l4: 0.018338, l5: 0.065191, l6: 0.215218\n",
      "\n",
      "[epoch: 21624/100000, batch:    24/   30, ite: 172990] train loss: 0.316318, tar: 0.015858 \n",
      "It 172990\n",
      "l0: 0.007233, l1: 0.007235, l2: 0.007964, l3: 0.008407, l4: 0.009678, l5: 0.019800, l6: 0.133561\n",
      "\n",
      "[epoch: 21624/100000, batch:    28/   30, ite: 172991] train loss: 0.316194, tar: 0.015849 \n",
      "It 172991\n",
      "l0: 0.006472, l1: 0.006471, l2: 0.007228, l3: 0.007771, l4: 0.010462, l5: 0.024913, l6: 0.096623\n",
      "\n",
      "[epoch: 21624/100000, batch:    32/   30, ite: 172992] train loss: 0.316037, tar: 0.015840 \n",
      "It 172992\n",
      "l0: 0.011115, l1: 0.011118, l2: 0.011822, l3: 0.012523, l4: 0.016308, l5: 0.062751, l6: 0.188009\n",
      "\n",
      "[epoch: 21625/100000, batch:     4/   30, ite: 172993] train loss: 0.316034, tar: 0.015835 \n",
      "It 172993\n",
      "l0: 0.010887, l1: 0.010889, l2: 0.012025, l3: 0.012788, l4: 0.016490, l5: 0.049648, l6: 0.164676\n",
      "\n",
      "[epoch: 21625/100000, batch:     8/   30, ite: 172994] train loss: 0.315995, tar: 0.015830 \n",
      "It 172994\n",
      "l0: 0.030448, l1: 0.030531, l2: 0.031093, l3: 0.033427, l4: 0.038100, l5: 0.069979, l6: 0.210146\n",
      "\n",
      "[epoch: 21625/100000, batch:    12/   30, ite: 172995] train loss: 0.316124, tar: 0.015845 \n",
      "It 172995\n",
      "l0: 0.011148, l1: 0.011125, l2: 0.012069, l3: 0.012829, l4: 0.016451, l5: 0.073407, l6: 0.209088\n",
      "\n",
      "[epoch: 21625/100000, batch:    16/   30, ite: 172996] train loss: 0.316154, tar: 0.015840 \n",
      "It 172996\n",
      "l0: 0.007283, l1: 0.007279, l2: 0.008016, l3: 0.008502, l4: 0.010460, l5: 0.022707, l6: 0.095673\n",
      "\n",
      "[epoch: 21625/100000, batch:    20/   30, ite: 172997] train loss: 0.315997, tar: 0.015831 \n",
      "It 172997\n",
      "l0: 0.036073, l1: 0.035923, l2: 0.037886, l3: 0.040064, l4: 0.047166, l5: 0.080503, l6: 0.249696\n",
      "\n",
      "[epoch: 21625/100000, batch:    24/   30, ite: 172998] train loss: 0.316209, tar: 0.015852 \n",
      "It 172998\n",
      "l0: 0.005402, l1: 0.005398, l2: 0.006120, l3: 0.006652, l4: 0.008918, l5: 0.016630, l6: 0.079931\n",
      "\n",
      "[epoch: 21625/100000, batch:    28/   30, ite: 172999] train loss: 0.316021, tar: 0.015841 \n",
      "It 172999\n",
      "l0: 0.013556, l1: 0.013551, l2: 0.014370, l3: 0.015165, l4: 0.019926, l5: 0.085727, l6: 0.266442\n",
      "\n",
      "[epoch: 21625/100000, batch:    32/   30, ite: 173000] train loss: 0.316134, tar: 0.015839 \n",
      "It 173000\n",
      "palm_saved_models/u2netnew_bce_itr_173000_train_0.316134_tar_0.015839.pth -----------------------------------------\n",
      "l0: 0.009423, l1: 0.009422, l2: 0.010213, l3: 0.010879, l4: 0.014365, l5: 0.039892, l6: 0.179912\n",
      "\n",
      "[epoch: 21626/100000, batch:     4/   30, ite: 173001] train loss: 0.274107, tar: 0.009423 \n",
      "It 173001\n",
      "l0: 0.034154, l1: 0.034245, l2: 0.035252, l3: 0.037555, l4: 0.043408, l5: 0.105636, l6: 0.258122\n",
      "\n",
      "[epoch: 21626/100000, batch:     8/   30, ite: 173002] train loss: 0.411239, tar: 0.021789 \n",
      "It 173002\n",
      "l0: 0.007407, l1: 0.007406, l2: 0.008177, l3: 0.008790, l4: 0.011188, l5: 0.028064, l6: 0.125398\n",
      "\n",
      "[epoch: 21626/100000, batch:    12/   30, ite: 173003] train loss: 0.339637, tar: 0.016995 \n",
      "It 173003\n",
      "l0: 0.028759, l1: 0.028675, l2: 0.030013, l3: 0.031873, l4: 0.034837, l5: 0.053820, l6: 0.132552\n",
      "\n",
      "[epoch: 21626/100000, batch:    16/   30, ite: 173004] train loss: 0.339860, tar: 0.019936 \n",
      "It 173004\n",
      "l0: 0.013040, l1: 0.013045, l2: 0.014230, l3: 0.015399, l4: 0.019508, l5: 0.078157, l6: 0.205571\n",
      "\n",
      "[epoch: 21626/100000, batch:    20/   30, ite: 173005] train loss: 0.343678, tar: 0.018557 \n",
      "It 173005\n",
      "l0: 0.008662, l1: 0.008662, l2: 0.009367, l3: 0.010060, l4: 0.015341, l5: 0.058366, l6: 0.151805\n",
      "\n",
      "[epoch: 21626/100000, batch:    24/   30, ite: 173006] train loss: 0.330109, tar: 0.016908 \n",
      "It 173006\n",
      "l0: 0.007700, l1: 0.007701, l2: 0.008360, l3: 0.008807, l4: 0.011023, l5: 0.030471, l6: 0.121273\n",
      "\n",
      "[epoch: 21626/100000, batch:    28/   30, ite: 173007] train loss: 0.310855, tar: 0.015592 \n",
      "It 173007\n",
      "l0: 0.012235, l1: 0.012240, l2: 0.013167, l3: 0.013798, l4: 0.015607, l5: 0.040645, l6: 0.250767\n",
      "\n",
      "[epoch: 21626/100000, batch:    32/   30, ite: 173008] train loss: 0.316806, tar: 0.015173 \n",
      "It 173008\n",
      "l0: 0.033065, l1: 0.033130, l2: 0.034339, l3: 0.036588, l4: 0.043830, l5: 0.118932, l6: 0.302467\n",
      "\n",
      "[epoch: 21627/100000, batch:     4/   30, ite: 173009] train loss: 0.348533, tar: 0.017161 \n",
      "It 173009\n",
      "l0: 0.008286, l1: 0.008288, l2: 0.008950, l3: 0.009656, l4: 0.011562, l5: 0.033575, l6: 0.148272\n",
      "\n",
      "[epoch: 21627/100000, batch:     8/   30, ite: 173010] train loss: 0.336539, tar: 0.016273 \n",
      "It 173010\n",
      "l0: 0.010994, l1: 0.010991, l2: 0.011954, l3: 0.012630, l4: 0.017779, l5: 0.063048, l6: 0.183596\n",
      "\n",
      "[epoch: 21627/100000, batch:    12/   30, ite: 173011] train loss: 0.334216, tar: 0.015793 \n",
      "It 173011\n",
      "l0: 0.004825, l1: 0.004824, l2: 0.005418, l3: 0.005730, l4: 0.007117, l5: 0.013201, l6: 0.067435\n",
      "\n",
      "[epoch: 21627/100000, batch:    16/   30, ite: 173012] train loss: 0.315410, tar: 0.014879 \n",
      "It 173012\n",
      "l0: 0.012821, l1: 0.012818, l2: 0.013756, l3: 0.014511, l4: 0.018578, l5: 0.086306, l6: 0.239671\n",
      "\n",
      "[epoch: 21627/100000, batch:    20/   30, ite: 173013] train loss: 0.321799, tar: 0.014721 \n",
      "It 173013\n",
      "l0: 0.031635, l1: 0.031538, l2: 0.033123, l3: 0.036096, l4: 0.041306, l5: 0.068305, l6: 0.206828\n",
      "\n",
      "[epoch: 21627/100000, batch:    24/   30, ite: 173014] train loss: 0.330873, tar: 0.015929 \n",
      "It 173014\n",
      "l0: 0.008494, l1: 0.008494, l2: 0.009316, l3: 0.009893, l4: 0.012460, l5: 0.034185, l6: 0.113607\n",
      "\n",
      "[epoch: 21627/100000, batch:    28/   30, ite: 173015] train loss: 0.321911, tar: 0.015433 \n",
      "It 173015\n",
      "l0: 0.006974, l1: 0.006973, l2: 0.007919, l3: 0.008684, l4: 0.010978, l5: 0.017873, l6: 0.059219\n",
      "\n",
      "[epoch: 21627/100000, batch:    32/   30, ite: 173016] train loss: 0.309206, tar: 0.014905 \n",
      "It 173016\n",
      "l0: 0.029910, l1: 0.029823, l2: 0.031100, l3: 0.032766, l4: 0.035031, l5: 0.080765, l6: 0.191832\n",
      "\n",
      "[epoch: 21628/100000, batch:     4/   30, ite: 173017] train loss: 0.316383, tar: 0.015787 \n",
      "It 173017\n",
      "l0: 0.013931, l1: 0.013934, l2: 0.015007, l3: 0.015870, l4: 0.021170, l5: 0.071231, l6: 0.239494\n",
      "\n",
      "[epoch: 21628/100000, batch:     8/   30, ite: 173018] train loss: 0.320509, tar: 0.015684 \n",
      "It 173018\n",
      "l0: 0.028559, l1: 0.028653, l2: 0.029468, l3: 0.031839, l4: 0.036607, l5: 0.055640, l6: 0.169031\n",
      "\n",
      "[epoch: 21628/100000, batch:    12/   30, ite: 173019] train loss: 0.323629, tar: 0.016362 \n",
      "It 173019\n",
      "l0: 0.008332, l1: 0.008337, l2: 0.009234, l3: 0.010003, l4: 0.013079, l5: 0.043633, l6: 0.115780\n",
      "\n",
      "[epoch: 21628/100000, batch:    16/   30, ite: 173020] train loss: 0.317868, tar: 0.015960 \n",
      "It 173020\n",
      "l0: 0.008835, l1: 0.008837, l2: 0.009771, l3: 0.010412, l4: 0.012744, l5: 0.024155, l6: 0.144971\n",
      "\n",
      "[epoch: 21628/100000, batch:    20/   30, ite: 173021] train loss: 0.313194, tar: 0.015621 \n",
      "It 173021\n",
      "l0: 0.008207, l1: 0.008208, l2: 0.008885, l3: 0.009373, l4: 0.011506, l5: 0.044545, l6: 0.144074\n",
      "\n",
      "[epoch: 21628/100000, batch:    24/   30, ite: 173022] train loss: 0.309631, tar: 0.015284 \n",
      "It 173022\n",
      "l0: 0.011328, l1: 0.011329, l2: 0.012290, l3: 0.012993, l4: 0.016776, l5: 0.061230, l6: 0.188303\n",
      "\n",
      "[epoch: 21628/100000, batch:    28/   30, ite: 173023] train loss: 0.309831, tar: 0.015112 \n",
      "It 173023\n",
      "l0: 0.011680, l1: 0.011679, l2: 0.012584, l3: 0.013388, l4: 0.015968, l5: 0.048225, l6: 0.244644\n",
      "\n",
      "[epoch: 21628/100000, batch:    32/   30, ite: 173024] train loss: 0.311845, tar: 0.014969 \n",
      "It 173024\n",
      "l0: 0.011072, l1: 0.011066, l2: 0.012215, l3: 0.013067, l4: 0.017214, l5: 0.038144, l6: 0.174574\n",
      "\n",
      "[epoch: 21629/100000, batch:     4/   30, ite: 173025] train loss: 0.310466, tar: 0.014813 \n",
      "It 173025\n",
      "l0: 0.008456, l1: 0.008453, l2: 0.009287, l3: 0.009904, l4: 0.012686, l5: 0.034866, l6: 0.147272\n",
      "\n",
      "[epoch: 21629/100000, batch:     8/   30, ite: 173026] train loss: 0.307407, tar: 0.014569 \n",
      "It 173026\n",
      "l0: 0.005757, l1: 0.005753, l2: 0.006402, l3: 0.006711, l4: 0.007958, l5: 0.015475, l6: 0.060717\n",
      "\n",
      "[epoch: 21629/100000, batch:    12/   30, ite: 173027] train loss: 0.300050, tar: 0.014242 \n",
      "It 173027\n",
      "l0: 0.026992, l1: 0.026914, l2: 0.027898, l3: 0.029681, l4: 0.033577, l5: 0.058529, l6: 0.136523\n",
      "\n",
      "[epoch: 21629/100000, batch:    16/   30, ite: 173028] train loss: 0.301481, tar: 0.014698 \n",
      "It 173028\n",
      "l0: 0.011327, l1: 0.011326, l2: 0.012138, l3: 0.012704, l4: 0.016363, l5: 0.061317, l6: 0.193729\n",
      "\n",
      "[epoch: 21629/100000, batch:    20/   30, ite: 173029] train loss: 0.302081, tar: 0.014581 \n",
      "It 173029\n",
      "l0: 0.014825, l1: 0.014825, l2: 0.015930, l3: 0.016923, l4: 0.021692, l5: 0.077318, l6: 0.289994\n",
      "\n",
      "[epoch: 21629/100000, batch:    24/   30, ite: 173030] train loss: 0.307062, tar: 0.014590 \n",
      "It 173030\n",
      "l0: 0.009375, l1: 0.009375, l2: 0.010053, l3: 0.010890, l4: 0.016005, l5: 0.076055, l6: 0.148916\n",
      "\n",
      "[epoch: 21629/100000, batch:    28/   30, ite: 173031] train loss: 0.306211, tar: 0.014421 \n",
      "It 173031\n",
      "l0: 0.059013, l1: 0.059187, l2: 0.060938, l3: 0.066344, l4: 0.073350, l5: 0.110197, l6: 0.232669\n",
      "\n",
      "[epoch: 21629/100000, batch:    32/   30, ite: 173032] train loss: 0.317320, tar: 0.015815 \n",
      "It 173032\n",
      "l0: 0.028141, l1: 0.028071, l2: 0.029141, l3: 0.030843, l4: 0.034494, l5: 0.054372, l6: 0.164074\n",
      "\n",
      "[epoch: 21630/100000, batch:     4/   30, ite: 173033] train loss: 0.318890, tar: 0.016188 \n",
      "It 173033\n",
      "l0: 0.007306, l1: 0.007304, l2: 0.007896, l3: 0.008404, l4: 0.011328, l5: 0.040957, l6: 0.121157\n",
      "\n",
      "[epoch: 21630/100000, batch:     8/   30, ite: 173034] train loss: 0.315521, tar: 0.015927 \n",
      "It 173034\n",
      "l0: 0.014072, l1: 0.014071, l2: 0.015198, l3: 0.016093, l4: 0.022587, l5: 0.113730, l6: 0.255026\n",
      "\n",
      "[epoch: 21630/100000, batch:    12/   30, ite: 173035] train loss: 0.319386, tar: 0.015874 \n",
      "It 173035\n",
      "l0: 0.009417, l1: 0.009418, l2: 0.010256, l3: 0.010813, l4: 0.012764, l5: 0.039376, l6: 0.114142\n",
      "\n",
      "[epoch: 21630/100000, batch:    16/   30, ite: 173036] train loss: 0.316241, tar: 0.015695 \n",
      "It 173036\n",
      "l0: 0.009259, l1: 0.009262, l2: 0.010231, l3: 0.010947, l4: 0.013805, l5: 0.031018, l6: 0.140507\n",
      "\n",
      "[epoch: 21630/100000, batch:    20/   30, ite: 173037] train loss: 0.313776, tar: 0.015521 \n",
      "It 173037\n",
      "l0: 0.011940, l1: 0.011943, l2: 0.012800, l3: 0.013358, l4: 0.015327, l5: 0.041583, l6: 0.245946\n",
      "\n",
      "[epoch: 21630/100000, batch:    24/   30, ite: 173038] train loss: 0.314806, tar: 0.015427 \n",
      "It 173038\n",
      "l0: 0.030889, l1: 0.030996, l2: 0.031882, l3: 0.034632, l4: 0.038287, l5: 0.059729, l6: 0.147831\n",
      "\n",
      "[epoch: 21630/100000, batch:    28/   30, ite: 173039] train loss: 0.316330, tar: 0.015823 \n",
      "It 173039\n",
      "l0: 0.012710, l1: 0.012712, l2: 0.014063, l3: 0.015133, l4: 0.018748, l5: 0.038174, l6: 0.209617\n",
      "\n",
      "[epoch: 21630/100000, batch:    32/   30, ite: 173040] train loss: 0.316450, tar: 0.015745 \n",
      "It 173040\n",
      "l0: 0.012664, l1: 0.012663, l2: 0.013658, l3: 0.014686, l4: 0.020292, l5: 0.076248, l6: 0.196378\n",
      "\n",
      "[epoch: 21631/100000, batch:     4/   30, ite: 173041] train loss: 0.317185, tar: 0.015670 \n",
      "It 173041\n",
      "l0: 0.008192, l1: 0.008190, l2: 0.008783, l3: 0.009278, l4: 0.011026, l5: 0.084693, l6: 0.137255\n",
      "\n",
      "[epoch: 21631/100000, batch:     8/   30, ite: 173042] train loss: 0.316000, tar: 0.015492 \n",
      "It 173042\n",
      "l0: 0.010898, l1: 0.010896, l2: 0.011761, l3: 0.012452, l4: 0.015314, l5: 0.041728, l6: 0.208453\n",
      "\n",
      "[epoch: 21631/100000, batch:    12/   30, ite: 173043] train loss: 0.315896, tar: 0.015385 \n",
      "It 173043\n",
      "l0: 0.010568, l1: 0.010566, l2: 0.011521, l3: 0.012237, l4: 0.014811, l5: 0.036494, l6: 0.168446\n",
      "\n",
      "[epoch: 21631/100000, batch:    16/   30, ite: 173044] train loss: 0.314731, tar: 0.015276 \n",
      "It 173044\n",
      "l0: 0.034457, l1: 0.034341, l2: 0.036013, l3: 0.038770, l4: 0.042657, l5: 0.069443, l6: 0.158411\n",
      "\n",
      "[epoch: 21631/100000, batch:    20/   30, ite: 173045] train loss: 0.316939, tar: 0.015702 \n",
      "It 173045\n",
      "l0: 0.006961, l1: 0.006961, l2: 0.007728, l3: 0.008229, l4: 0.009686, l5: 0.027363, l6: 0.097434\n",
      "\n",
      "[epoch: 21631/100000, batch:    24/   30, ite: 173046] train loss: 0.313622, tar: 0.015512 \n",
      "It 173046\n",
      "l0: 0.031478, l1: 0.031576, l2: 0.032312, l3: 0.034356, l4: 0.039211, l5: 0.067807, l6: 0.203601\n",
      "\n",
      "[epoch: 21631/100000, batch:    28/   30, ite: 173047] train loss: 0.316318, tar: 0.015852 \n",
      "It 173047\n",
      "l0: 0.008657, l1: 0.008659, l2: 0.009431, l3: 0.010060, l4: 0.012334, l5: 0.042328, l6: 0.134889\n",
      "\n",
      "[epoch: 21631/100000, batch:    32/   30, ite: 173048] train loss: 0.314444, tar: 0.015702 \n",
      "It 173048\n",
      "l0: 0.004497, l1: 0.004497, l2: 0.005030, l3: 0.005311, l4: 0.006226, l5: 0.013271, l6: 0.046595\n",
      "\n",
      "[epoch: 21632/100000, batch:     4/   30, ite: 173049] train loss: 0.309770, tar: 0.015473 \n",
      "It 173049\n",
      "l0: 0.005975, l1: 0.005978, l2: 0.006655, l3: 0.007051, l4: 0.009390, l5: 0.029301, l6: 0.101255\n",
      "\n",
      "[epoch: 21632/100000, batch:     8/   30, ite: 173050] train loss: 0.306887, tar: 0.015283 \n",
      "It 173050\n",
      "l0: 0.007849, l1: 0.007849, l2: 0.008459, l3: 0.008833, l4: 0.011948, l5: 0.037561, l6: 0.138428\n",
      "\n",
      "[epoch: 21632/100000, batch:    12/   30, ite: 173051] train loss: 0.305202, tar: 0.015137 \n",
      "It 173051\n",
      "l0: 0.016950, l1: 0.016955, l2: 0.018242, l3: 0.019287, l4: 0.023487, l5: 0.063073, l6: 0.345891\n",
      "\n",
      "[epoch: 21632/100000, batch:    16/   30, ite: 173052] train loss: 0.309022, tar: 0.015172 \n",
      "It 173052\n",
      "l0: 0.010016, l1: 0.010016, l2: 0.011112, l3: 0.011792, l4: 0.015109, l5: 0.051065, l6: 0.127508\n",
      "\n",
      "[epoch: 21632/100000, batch:    20/   30, ite: 173053] train loss: 0.307656, tar: 0.015075 \n",
      "It 173053\n",
      "l0: 0.030301, l1: 0.030393, l2: 0.031185, l3: 0.032996, l4: 0.038798, l5: 0.098883, l6: 0.208548\n",
      "\n",
      "[epoch: 21632/100000, batch:    24/   30, ite: 173054] train loss: 0.310683, tar: 0.015357 \n",
      "It 173054\n",
      "l0: 0.035480, l1: 0.035332, l2: 0.037268, l3: 0.040818, l4: 0.048530, l5: 0.104002, l6: 0.220093\n",
      "\n",
      "[epoch: 21632/100000, batch:    28/   30, ite: 173055] train loss: 0.314517, tar: 0.015723 \n",
      "It 173055\n",
      "l0: 0.012850, l1: 0.012849, l2: 0.013692, l3: 0.014391, l4: 0.017734, l5: 0.057205, l6: 0.245445\n",
      "\n",
      "[epoch: 21632/100000, batch:    32/   30, ite: 173056] train loss: 0.315582, tar: 0.015671 \n",
      "It 173056\n",
      "l0: 0.010058, l1: 0.010058, l2: 0.010986, l3: 0.011538, l4: 0.014233, l5: 0.055144, l6: 0.167941\n",
      "\n",
      "[epoch: 21633/100000, batch:     4/   30, ite: 173057] train loss: 0.314957, tar: 0.015573 \n",
      "It 173057\n",
      "l0: 0.033443, l1: 0.033322, l2: 0.035113, l3: 0.038221, l4: 0.042979, l5: 0.065353, l6: 0.177411\n",
      "\n",
      "[epoch: 21633/100000, batch:     8/   30, ite: 173058] train loss: 0.316869, tar: 0.015881 \n",
      "It 173058\n",
      "l0: 0.008595, l1: 0.008597, l2: 0.009346, l3: 0.010073, l4: 0.014001, l5: 0.049968, l6: 0.138737\n",
      "\n",
      "[epoch: 21633/100000, batch:    12/   30, ite: 173059] train loss: 0.315554, tar: 0.015758 \n",
      "It 173059\n",
      "l0: 0.013781, l1: 0.013785, l2: 0.014738, l3: 0.015540, l4: 0.019050, l5: 0.058148, l6: 0.263262\n",
      "\n",
      "[epoch: 21633/100000, batch:    16/   30, ite: 173060] train loss: 0.316933, tar: 0.015725 \n",
      "It 173060\n",
      "l0: 0.030752, l1: 0.030857, l2: 0.031524, l3: 0.032943, l4: 0.038301, l5: 0.071605, l6: 0.179205\n",
      "\n",
      "[epoch: 21633/100000, batch:    20/   30, ite: 173061] train loss: 0.318544, tar: 0.015971 \n",
      "It 173061\n",
      "l0: 0.007720, l1: 0.007725, l2: 0.008741, l3: 0.009456, l4: 0.011305, l5: 0.019491, l6: 0.076070\n",
      "\n",
      "[epoch: 21633/100000, batch:    24/   30, ite: 173062] train loss: 0.315672, tar: 0.015838 \n",
      "It 173062\n",
      "l0: 0.009206, l1: 0.009207, l2: 0.010052, l3: 0.010576, l4: 0.012555, l5: 0.044443, l6: 0.166444\n",
      "\n",
      "[epoch: 21633/100000, batch:    28/   30, ite: 173063] train loss: 0.314828, tar: 0.015733 \n",
      "It 173063\n",
      "l0: 0.014657, l1: 0.014654, l2: 0.015572, l3: 0.016414, l4: 0.019834, l5: 0.154873, l6: 0.261799\n",
      "\n",
      "[epoch: 21633/100000, batch:    32/   30, ite: 173064] train loss: 0.317687, tar: 0.015716 \n",
      "It 173064\n",
      "l0: 0.011344, l1: 0.011344, l2: 0.012332, l3: 0.013132, l4: 0.016326, l5: 0.052632, l6: 0.208447\n",
      "\n",
      "[epoch: 21634/100000, batch:     4/   30, ite: 173065] train loss: 0.317808, tar: 0.015649 \n",
      "It 173065\n",
      "l0: 0.026525, l1: 0.026602, l2: 0.027296, l3: 0.029059, l4: 0.034298, l5: 0.058754, l6: 0.122852\n",
      "\n",
      "[epoch: 21634/100000, batch:     8/   30, ite: 173066] train loss: 0.317923, tar: 0.015813 \n",
      "It 173066\n",
      "l0: 0.010972, l1: 0.010970, l2: 0.011727, l3: 0.012345, l4: 0.015172, l5: 0.052753, l6: 0.204409\n",
      "\n",
      "[epoch: 21634/100000, batch:    12/   30, ite: 173067] train loss: 0.317929, tar: 0.015741 \n",
      "It 173067\n",
      "l0: 0.009129, l1: 0.009125, l2: 0.009816, l3: 0.010308, l4: 0.012072, l5: 0.062767, l6: 0.139281\n",
      "\n",
      "[epoch: 21634/100000, batch:    16/   30, ite: 173068] train loss: 0.316967, tar: 0.015644 \n",
      "It 173068\n",
      "l0: 0.012062, l1: 0.012055, l2: 0.013296, l3: 0.014375, l4: 0.019744, l5: 0.065130, l6: 0.166816\n",
      "\n",
      "[epoch: 21634/100000, batch:    20/   30, ite: 173069] train loss: 0.316772, tar: 0.015592 \n",
      "It 173069\n",
      "l0: 0.031258, l1: 0.031139, l2: 0.032641, l3: 0.035459, l4: 0.039872, l5: 0.072746, l6: 0.177486\n",
      "\n",
      "[epoch: 21634/100000, batch:    24/   30, ite: 173070] train loss: 0.318255, tar: 0.015816 \n",
      "It 173070\n",
      "l0: 0.011731, l1: 0.011732, l2: 0.012878, l3: 0.013650, l4: 0.016375, l5: 0.036164, l6: 0.204362\n",
      "\n",
      "[epoch: 21634/100000, batch:    28/   30, ite: 173071] train loss: 0.318095, tar: 0.015758 \n",
      "It 173071\n",
      "l0: 0.009645, l1: 0.009647, l2: 0.010464, l3: 0.011176, l4: 0.014072, l5: 0.099928, l6: 0.146236\n",
      "\n",
      "[epoch: 21634/100000, batch:    32/   30, ite: 173072] train loss: 0.317860, tar: 0.015673 \n",
      "It 173072\n",
      "l0: 0.035470, l1: 0.035577, l2: 0.036488, l3: 0.038360, l4: 0.045972, l5: 0.099073, l6: 0.239783\n",
      "\n",
      "[epoch: 21635/100000, batch:     4/   30, ite: 173073] train loss: 0.320776, tar: 0.015945 \n",
      "It 173073\n",
      "l0: 0.008639, l1: 0.008641, l2: 0.009320, l3: 0.009991, l4: 0.012087, l5: 0.031357, l6: 0.157886\n",
      "\n",
      "[epoch: 21635/100000, batch:     8/   30, ite: 173074] train loss: 0.319656, tar: 0.015846 \n",
      "It 173074\n",
      "l0: 0.008555, l1: 0.008557, l2: 0.009438, l3: 0.009830, l4: 0.011601, l5: 0.028268, l6: 0.143813\n",
      "\n",
      "[epoch: 21635/100000, batch:    12/   30, ite: 173075] train loss: 0.318328, tar: 0.015749 \n",
      "It 173075\n",
      "l0: 0.035098, l1: 0.035001, l2: 0.036870, l3: 0.039724, l4: 0.047571, l5: 0.113455, l6: 0.296011\n",
      "\n",
      "[epoch: 21635/100000, batch:    16/   30, ite: 173076] train loss: 0.322083, tar: 0.016003 \n",
      "It 173076\n",
      "l0: 0.011289, l1: 0.011288, l2: 0.012172, l3: 0.012955, l4: 0.016936, l5: 0.051201, l6: 0.189251\n",
      "\n",
      "[epoch: 21635/100000, batch:    20/   30, ite: 173077] train loss: 0.321863, tar: 0.015942 \n",
      "It 173077\n",
      "l0: 0.006272, l1: 0.006271, l2: 0.007060, l3: 0.007523, l4: 0.009536, l5: 0.019128, l6: 0.067601\n",
      "\n",
      "[epoch: 21635/100000, batch:    24/   30, ite: 173078] train loss: 0.319318, tar: 0.015818 \n",
      "It 173078\n",
      "l0: 0.006625, l1: 0.006625, l2: 0.007292, l3: 0.007694, l4: 0.009402, l5: 0.051034, l6: 0.084405\n",
      "\n",
      "[epoch: 21635/100000, batch:    28/   30, ite: 173079] train loss: 0.317467, tar: 0.015702 \n",
      "It 173079\n",
      "l0: 0.012691, l1: 0.012691, l2: 0.013796, l3: 0.014842, l4: 0.018078, l5: 0.052560, l6: 0.189610\n",
      "\n",
      "[epoch: 21635/100000, batch:    32/   30, ite: 173080] train loss: 0.317427, tar: 0.015664 \n",
      "It 173080\n",
      "l0: 0.013978, l1: 0.013971, l2: 0.015204, l3: 0.016144, l4: 0.020104, l5: 0.052846, l6: 0.261138\n",
      "\n",
      "[epoch: 21636/100000, batch:     4/   30, ite: 173081] train loss: 0.318365, tar: 0.015643 \n",
      "It 173081\n",
      "l0: 0.008525, l1: 0.008521, l2: 0.009174, l3: 0.009577, l4: 0.012546, l5: 0.079394, l6: 0.127650\n",
      "\n",
      "[epoch: 21636/100000, batch:     8/   30, ite: 173082] train loss: 0.317597, tar: 0.015556 \n",
      "It 173082\n",
      "l0: 0.009952, l1: 0.009949, l2: 0.010717, l3: 0.011351, l4: 0.013530, l5: 0.042512, l6: 0.158311\n",
      "\n",
      "[epoch: 21636/100000, batch:    12/   30, ite: 173083] train loss: 0.316859, tar: 0.015489 \n",
      "It 173083\n",
      "l0: 0.031512, l1: 0.031592, l2: 0.032619, l3: 0.034664, l4: 0.041504, l5: 0.089032, l6: 0.209700\n",
      "\n",
      "[epoch: 21636/100000, batch:    16/   30, ite: 173084] train loss: 0.318689, tar: 0.015680 \n",
      "It 173084\n",
      "l0: 0.010216, l1: 0.010217, l2: 0.011242, l3: 0.011836, l4: 0.014105, l5: 0.052410, l6: 0.154467\n",
      "\n",
      "[epoch: 21636/100000, batch:    20/   30, ite: 173085] train loss: 0.318052, tar: 0.015615 \n",
      "It 173085\n",
      "l0: 0.003843, l1: 0.003841, l2: 0.004344, l3: 0.004725, l4: 0.005944, l5: 0.012114, l6: 0.053163\n",
      "\n",
      "[epoch: 21636/100000, batch:    24/   30, ite: 173086] train loss: 0.315376, tar: 0.015478 \n",
      "It 173086\n",
      "l0: 0.030689, l1: 0.030590, l2: 0.032052, l3: 0.034598, l4: 0.038583, l5: 0.065247, l6: 0.207825\n",
      "\n",
      "[epoch: 21636/100000, batch:    28/   30, ite: 173087] train loss: 0.316804, tar: 0.015653 \n",
      "It 173087\n",
      "l0: 0.011977, l1: 0.011979, l2: 0.012912, l3: 0.013658, l4: 0.019025, l5: 0.065103, l6: 0.233449\n",
      "\n",
      "[epoch: 21636/100000, batch:    32/   30, ite: 173088] train loss: 0.317387, tar: 0.015611 \n",
      "It 173088\n",
      "l0: 0.007726, l1: 0.007728, l2: 0.008560, l3: 0.009021, l4: 0.010253, l5: 0.032625, l6: 0.090800\n",
      "\n",
      "[epoch: 21637/100000, batch:     4/   30, ite: 173089] train loss: 0.315694, tar: 0.015523 \n",
      "It 173089\n",
      "l0: 0.005413, l1: 0.005414, l2: 0.006001, l3: 0.006332, l4: 0.008291, l5: 0.016991, l6: 0.057399\n",
      "\n",
      "[epoch: 21637/100000, batch:     8/   30, ite: 173090] train loss: 0.313362, tar: 0.015411 \n",
      "It 173090\n",
      "l0: 0.011650, l1: 0.011652, l2: 0.012622, l3: 0.013292, l4: 0.016005, l5: 0.047452, l6: 0.201870\n",
      "\n",
      "[epoch: 21637/100000, batch:    12/   30, ite: 173091] train loss: 0.313375, tar: 0.015369 \n",
      "It 173091\n",
      "l0: 0.005012, l1: 0.005011, l2: 0.005633, l3: 0.006105, l4: 0.007873, l5: 0.016457, l6: 0.069622\n",
      "\n",
      "[epoch: 21637/100000, batch:    16/   30, ite: 173092] train loss: 0.311227, tar: 0.015257 \n",
      "It 173092\n",
      "l0: 0.010478, l1: 0.010478, l2: 0.011293, l3: 0.012085, l4: 0.015811, l5: 0.093971, l6: 0.184576\n",
      "\n",
      "[epoch: 21637/100000, batch:    20/   30, ite: 173093] train loss: 0.311522, tar: 0.015205 \n",
      "It 173093\n",
      "l0: 0.039472, l1: 0.039371, l2: 0.041336, l3: 0.044186, l4: 0.047363, l5: 0.098408, l6: 0.343056\n",
      "\n",
      "[epoch: 21637/100000, batch:    24/   30, ite: 173094] train loss: 0.315157, tar: 0.015463 \n",
      "It 173094\n",
      "l0: 0.032968, l1: 0.033062, l2: 0.034154, l3: 0.036266, l4: 0.044543, l5: 0.098466, l6: 0.214146\n",
      "\n",
      "[epoch: 21637/100000, batch:    28/   30, ite: 173095] train loss: 0.317035, tar: 0.015648 \n",
      "It 173095\n",
      "l0: 0.012644, l1: 0.012646, l2: 0.013601, l3: 0.014586, l4: 0.018705, l5: 0.063648, l6: 0.239178\n",
      "\n",
      "[epoch: 21637/100000, batch:    32/   30, ite: 173096] train loss: 0.317639, tar: 0.015616 \n",
      "It 173096\n",
      "l0: 0.007004, l1: 0.007004, l2: 0.007798, l3: 0.008301, l4: 0.010026, l5: 0.020312, l6: 0.098012\n",
      "\n",
      "[epoch: 21638/100000, batch:     4/   30, ite: 173097] train loss: 0.315998, tar: 0.015528 \n",
      "It 173097\n",
      "l0: 0.034339, l1: 0.034225, l2: 0.035841, l3: 0.037984, l4: 0.044948, l5: 0.077091, l6: 0.245614\n",
      "\n",
      "[epoch: 21638/100000, batch:     8/   30, ite: 173098] train loss: 0.317978, tar: 0.015720 \n",
      "It 173098\n",
      "l0: 0.011065, l1: 0.011064, l2: 0.011933, l3: 0.012623, l4: 0.016053, l5: 0.048211, l6: 0.220166\n",
      "\n",
      "[epoch: 21638/100000, batch:    12/   30, ite: 173099] train loss: 0.318111, tar: 0.015673 \n",
      "It 173099\n",
      "l0: 0.007890, l1: 0.007889, l2: 0.008716, l3: 0.009571, l4: 0.012076, l5: 0.028829, l6: 0.132704\n",
      "\n",
      "[epoch: 21638/100000, batch:    16/   30, ite: 173100] train loss: 0.317006, tar: 0.015595 \n",
      "It 173100\n",
      "l0: 0.005577, l1: 0.005576, l2: 0.006193, l3: 0.006528, l4: 0.007852, l5: 0.015360, l6: 0.099331\n",
      "\n",
      "[epoch: 21638/100000, batch:    20/   30, ite: 173101] train loss: 0.315317, tar: 0.015496 \n",
      "It 173101\n",
      "l0: 0.012697, l1: 0.012699, l2: 0.013703, l3: 0.014523, l4: 0.018063, l5: 0.074254, l6: 0.201416\n",
      "\n",
      "[epoch: 21638/100000, batch:    24/   30, ite: 173102] train loss: 0.315631, tar: 0.015468 \n",
      "It 173102\n",
      "l0: 0.012197, l1: 0.012198, l2: 0.013089, l3: 0.013889, l4: 0.018380, l5: 0.089319, l6: 0.221057\n",
      "\n",
      "[epoch: 21638/100000, batch:    28/   30, ite: 173103] train loss: 0.316258, tar: 0.015436 \n",
      "It 173103\n",
      "l0: 0.060253, l1: 0.060454, l2: 0.061822, l3: 0.065542, l4: 0.072930, l5: 0.108818, l6: 0.258649\n",
      "\n",
      "[epoch: 21638/100000, batch:    32/   30, ite: 173104] train loss: 0.319837, tar: 0.015867 \n",
      "It 173104\n",
      "l0: 0.011689, l1: 0.011687, l2: 0.012613, l3: 0.013507, l4: 0.016144, l5: 0.061216, l6: 0.212406\n",
      "\n",
      "[epoch: 21639/100000, batch:     4/   30, ite: 173105] train loss: 0.320022, tar: 0.015827 \n",
      "It 173105\n",
      "l0: 0.008708, l1: 0.008708, l2: 0.009495, l3: 0.010086, l4: 0.013280, l5: 0.040507, l6: 0.115901\n",
      "\n",
      "[epoch: 21639/100000, batch:     8/   30, ite: 173106] train loss: 0.318952, tar: 0.015760 \n",
      "It 173106\n",
      "l0: 0.007010, l1: 0.007005, l2: 0.007644, l3: 0.008141, l4: 0.010304, l5: 0.030115, l6: 0.118329\n",
      "\n",
      "[epoch: 21639/100000, batch:    12/   30, ite: 173107] train loss: 0.317734, tar: 0.015679 \n",
      "It 173107\n",
      "l0: 0.032052, l1: 0.032127, l2: 0.033130, l3: 0.035436, l4: 0.043415, l5: 0.104241, l6: 0.253652\n",
      "\n",
      "[epoch: 21639/100000, batch:    16/   30, ite: 173108] train loss: 0.319737, tar: 0.015830 \n",
      "It 173108\n",
      "l0: 0.006775, l1: 0.006774, l2: 0.007422, l3: 0.008018, l4: 0.010896, l5: 0.049215, l6: 0.082001\n",
      "\n",
      "[epoch: 21639/100000, batch:    20/   30, ite: 173109] train loss: 0.318373, tar: 0.015747 \n",
      "It 173109\n",
      "l0: 0.007098, l1: 0.007098, l2: 0.007835, l3: 0.008308, l4: 0.010160, l5: 0.015500, l6: 0.099455\n",
      "\n",
      "[epoch: 21639/100000, batch:    24/   30, ite: 173110] train loss: 0.316892, tar: 0.015668 \n",
      "It 173110\n",
      "l0: 0.013760, l1: 0.013758, l2: 0.015022, l3: 0.016271, l4: 0.021701, l5: 0.043193, l6: 0.232129\n",
      "\n",
      "[epoch: 21639/100000, batch:    28/   30, ite: 173111] train loss: 0.317243, tar: 0.015651 \n",
      "It 173111\n",
      "l0: 0.060945, l1: 0.060719, l2: 0.063376, l3: 0.069368, l4: 0.076094, l5: 0.119751, l6: 0.363079\n",
      "\n",
      "[epoch: 21639/100000, batch:    32/   30, ite: 173112] train loss: 0.321672, tar: 0.016056 \n",
      "It 173112\n",
      "l0: 0.010569, l1: 0.010572, l2: 0.011492, l3: 0.012494, l4: 0.015093, l5: 0.032584, l6: 0.167633\n",
      "\n",
      "[epoch: 21640/100000, batch:     4/   30, ite: 173113] train loss: 0.321130, tar: 0.016007 \n",
      "It 173113\n",
      "l0: 0.007493, l1: 0.007495, l2: 0.008344, l3: 0.008860, l4: 0.010735, l5: 0.025877, l6: 0.120760\n",
      "\n",
      "[epoch: 21640/100000, batch:     8/   30, ite: 173114] train loss: 0.319976, tar: 0.015932 \n",
      "It 173114\n",
      "l0: 0.035362, l1: 0.035277, l2: 0.036879, l3: 0.039136, l4: 0.042527, l5: 0.107754, l6: 0.286727\n",
      "\n",
      "[epoch: 21640/100000, batch:    12/   30, ite: 173115] train loss: 0.322269, tar: 0.016101 \n",
      "It 173115\n",
      "l0: 0.012389, l1: 0.012394, l2: 0.013467, l3: 0.014411, l4: 0.020497, l5: 0.045048, l6: 0.199238\n",
      "\n",
      "[epoch: 21640/100000, batch:    16/   30, ite: 173116] train loss: 0.322227, tar: 0.016069 \n",
      "It 173116\n",
      "l0: 0.012313, l1: 0.012318, l2: 0.013256, l3: 0.014274, l4: 0.018651, l5: 0.092930, l6: 0.229304\n",
      "\n",
      "[epoch: 21640/100000, batch:    20/   30, ite: 173117] train loss: 0.322833, tar: 0.016037 \n",
      "It 173117\n",
      "l0: 0.033058, l1: 0.033159, l2: 0.034100, l3: 0.037395, l4: 0.044959, l5: 0.081896, l6: 0.197112\n",
      "\n",
      "[epoch: 21640/100000, batch:    24/   30, ite: 173118] train loss: 0.324009, tar: 0.016182 \n",
      "It 173118\n",
      "l0: 0.005965, l1: 0.005963, l2: 0.006592, l3: 0.006897, l4: 0.008783, l5: 0.026508, l6: 0.088686\n",
      "\n",
      "[epoch: 21640/100000, batch:    28/   30, ite: 173119] train loss: 0.322542, tar: 0.016096 \n",
      "It 173119\n",
      "l0: 0.005500, l1: 0.005497, l2: 0.006160, l3: 0.006449, l4: 0.007411, l5: 0.012986, l6: 0.082456\n",
      "\n",
      "[epoch: 21640/100000, batch:    32/   30, ite: 173120] train loss: 0.320908, tar: 0.016007 \n",
      "It 173120\n",
      "l0: 0.006100, l1: 0.006096, l2: 0.006752, l3: 0.007166, l4: 0.009025, l5: 0.015804, l6: 0.080336\n",
      "\n",
      "[epoch: 21641/100000, batch:     4/   30, ite: 173121] train loss: 0.319341, tar: 0.015925 \n",
      "It 173121\n",
      "l0: 0.038234, l1: 0.038321, l2: 0.039513, l3: 0.042413, l4: 0.050885, l5: 0.107915, l6: 0.325516\n",
      "\n",
      "[epoch: 21641/100000, batch:     8/   30, ite: 173122] train loss: 0.321992, tar: 0.016108 \n",
      "It 173122\n",
      "l0: 0.006899, l1: 0.006893, l2: 0.007512, l3: 0.007880, l4: 0.009939, l5: 0.025397, l6: 0.125986\n",
      "\n",
      "[epoch: 21641/100000, batch:    12/   30, ite: 173123] train loss: 0.320923, tar: 0.016033 \n",
      "It 173123\n",
      "l0: 0.013036, l1: 0.013024, l2: 0.014097, l3: 0.014883, l4: 0.018970, l5: 0.057665, l6: 0.218925\n",
      "\n",
      "[epoch: 21641/100000, batch:    16/   30, ite: 173124] train loss: 0.321162, tar: 0.016009 \n",
      "It 173124\n",
      "l0: 0.011484, l1: 0.011477, l2: 0.012545, l3: 0.013584, l4: 0.018163, l5: 0.064596, l6: 0.197011\n",
      "\n",
      "[epoch: 21641/100000, batch:    20/   30, ite: 173125] train loss: 0.321224, tar: 0.015973 \n",
      "It 173125\n",
      "l0: 0.004744, l1: 0.004743, l2: 0.005329, l3: 0.005706, l4: 0.006862, l5: 0.010298, l6: 0.047749\n",
      "\n",
      "[epoch: 21641/100000, batch:    24/   30, ite: 173126] train loss: 0.319353, tar: 0.015884 \n",
      "It 173126\n",
      "l0: 0.029283, l1: 0.029187, l2: 0.030417, l3: 0.032197, l4: 0.034676, l5: 0.096037, l6: 0.185356\n",
      "\n",
      "[epoch: 21641/100000, batch:    28/   30, ite: 173127] train loss: 0.320280, tar: 0.015989 \n",
      "It 173127\n",
      "l0: 0.013174, l1: 0.013180, l2: 0.014557, l3: 0.015388, l4: 0.019724, l5: 0.038353, l6: 0.216955\n",
      "\n",
      "[epoch: 21641/100000, batch:    32/   30, ite: 173128] train loss: 0.320366, tar: 0.015967 \n",
      "It 173128\n",
      "l0: 0.012012, l1: 0.012021, l2: 0.013069, l3: 0.013806, l4: 0.016726, l5: 0.058114, l6: 0.176558\n",
      "\n",
      "[epoch: 21642/100000, batch:     4/   30, ite: 173129] train loss: 0.320226, tar: 0.015937 \n",
      "It 173129\n",
      "l0: 0.034832, l1: 0.034934, l2: 0.036197, l3: 0.038909, l4: 0.049399, l5: 0.116129, l6: 0.309354\n",
      "\n",
      "[epoch: 21642/100000, batch:     8/   30, ite: 173130] train loss: 0.322531, tar: 0.016082 \n",
      "It 173130\n",
      "l0: 0.006736, l1: 0.006735, l2: 0.007354, l3: 0.007949, l4: 0.010829, l5: 0.031302, l6: 0.108154\n",
      "\n",
      "[epoch: 21642/100000, batch:    12/   30, ite: 173131] train loss: 0.321435, tar: 0.016011 \n",
      "It 173131\n",
      "l0: 0.008055, l1: 0.008057, l2: 0.008913, l3: 0.009526, l4: 0.012866, l5: 0.067078, l6: 0.101755\n",
      "\n",
      "[epoch: 21642/100000, batch:    16/   30, ite: 173132] train loss: 0.320638, tar: 0.015951 \n",
      "It 173132\n",
      "l0: 0.010004, l1: 0.010002, l2: 0.010910, l3: 0.011543, l4: 0.014662, l5: 0.042863, l6: 0.170704\n",
      "\n",
      "[epoch: 21642/100000, batch:    20/   30, ite: 173133] train loss: 0.320263, tar: 0.015906 \n",
      "It 173133\n",
      "l0: 0.033153, l1: 0.033048, l2: 0.034207, l3: 0.036743, l4: 0.042618, l5: 0.082211, l6: 0.259083\n",
      "\n",
      "[epoch: 21642/100000, batch:    24/   30, ite: 173134] train loss: 0.321761, tar: 0.016035 \n",
      "It 173134\n",
      "l0: 0.005085, l1: 0.005084, l2: 0.005670, l3: 0.006117, l4: 0.007438, l5: 0.011881, l6: 0.041464\n",
      "\n",
      "[epoch: 21642/100000, batch:    28/   30, ite: 173135] train loss: 0.319991, tar: 0.015953 \n",
      "It 173135\n",
      "l0: 0.010955, l1: 0.010946, l2: 0.012036, l3: 0.012913, l4: 0.016210, l5: 0.046957, l6: 0.180215\n",
      "\n",
      "[epoch: 21642/100000, batch:    32/   30, ite: 173136] train loss: 0.319772, tar: 0.015917 \n",
      "It 173136\n",
      "l0: 0.012688, l1: 0.012676, l2: 0.013973, l3: 0.015095, l4: 0.017997, l5: 0.073936, l6: 0.212856\n",
      "\n",
      "[epoch: 21643/100000, batch:     4/   30, ite: 173137] train loss: 0.320060, tar: 0.015893 \n",
      "It 173137\n",
      "l0: 0.008162, l1: 0.008161, l2: 0.008948, l3: 0.009513, l4: 0.012290, l5: 0.034315, l6: 0.113295\n",
      "\n",
      "[epoch: 21643/100000, batch:     8/   30, ite: 173138] train loss: 0.319151, tar: 0.015837 \n",
      "It 173138\n",
      "l0: 0.033124, l1: 0.033219, l2: 0.034656, l3: 0.036769, l4: 0.042607, l5: 0.085594, l6: 0.249461\n",
      "\n",
      "[epoch: 21643/100000, batch:    12/   30, ite: 173139] train loss: 0.320564, tar: 0.015961 \n",
      "It 173139\n",
      "l0: 0.007775, l1: 0.007774, l2: 0.008611, l3: 0.009219, l4: 0.011013, l5: 0.032324, l6: 0.111902\n",
      "\n",
      "[epoch: 21643/100000, batch:    16/   30, ite: 173140] train loss: 0.319621, tar: 0.015903 \n",
      "It 173140\n",
      "l0: 0.007135, l1: 0.007138, l2: 0.007638, l3: 0.008091, l4: 0.011766, l5: 0.031688, l6: 0.103366\n",
      "\n",
      "[epoch: 21643/100000, batch:    20/   30, ite: 173141] train loss: 0.318608, tar: 0.015841 \n",
      "It 173141\n",
      "l0: 0.037065, l1: 0.036947, l2: 0.038452, l3: 0.041048, l4: 0.047450, l5: 0.103862, l6: 0.309590\n",
      "\n",
      "[epoch: 21643/100000, batch:    24/   30, ite: 173142] train loss: 0.320691, tar: 0.015990 \n",
      "It 173142\n",
      "l0: 0.007647, l1: 0.007652, l2: 0.008303, l3: 0.008710, l4: 0.010449, l5: 0.031761, l6: 0.114572\n",
      "\n",
      "[epoch: 21643/100000, batch:    28/   30, ite: 173143] train loss: 0.319771, tar: 0.015932 \n",
      "It 173143\n",
      "l0: 0.007074, l1: 0.007078, l2: 0.007611, l3: 0.007918, l4: 0.009062, l5: 0.027492, l6: 0.119405\n",
      "\n",
      "[epoch: 21643/100000, batch:    32/   30, ite: 173144] train loss: 0.318840, tar: 0.015870 \n",
      "It 173144\n",
      "l0: 0.009571, l1: 0.009573, l2: 0.010357, l3: 0.011025, l4: 0.013897, l5: 0.066552, l6: 0.172314\n",
      "\n",
      "[epoch: 21644/100000, batch:     4/   30, ite: 173145] train loss: 0.318664, tar: 0.015827 \n",
      "It 173145\n",
      "l0: 0.012870, l1: 0.012871, l2: 0.013974, l3: 0.014800, l4: 0.017911, l5: 0.064390, l6: 0.235104\n",
      "\n",
      "[epoch: 21644/100000, batch:     8/   30, ite: 173146] train loss: 0.319028, tar: 0.015807 \n",
      "It 173146\n",
      "l0: 0.009951, l1: 0.009951, l2: 0.010890, l3: 0.011829, l4: 0.016018, l5: 0.056836, l6: 0.176265\n",
      "\n",
      "[epoch: 21644/100000, batch:    12/   30, ite: 173147] train loss: 0.318843, tar: 0.015767 \n",
      "It 173147\n",
      "l0: 0.007783, l1: 0.007783, l2: 0.008472, l3: 0.009059, l4: 0.012409, l5: 0.042479, l6: 0.126758\n",
      "\n",
      "[epoch: 21644/100000, batch:    16/   30, ite: 173148] train loss: 0.318139, tar: 0.015713 \n",
      "It 173148\n",
      "l0: 0.005712, l1: 0.005712, l2: 0.006369, l3: 0.006792, l4: 0.008640, l5: 0.026103, l6: 0.076770\n",
      "\n",
      "[epoch: 21644/100000, batch:    20/   30, ite: 173149] train loss: 0.316918, tar: 0.015646 \n",
      "It 173149\n",
      "l0: 0.033611, l1: 0.033713, l2: 0.034681, l3: 0.036785, l4: 0.041372, l5: 0.078227, l6: 0.257635\n",
      "\n",
      "[epoch: 21644/100000, batch:    24/   30, ite: 173150] train loss: 0.318245, tar: 0.015766 \n",
      "It 173150\n",
      "l0: 0.038010, l1: 0.037880, l2: 0.039760, l3: 0.042193, l4: 0.049725, l5: 0.087712, l6: 0.270671\n",
      "\n",
      "[epoch: 21644/100000, batch:    28/   30, ite: 173151] train loss: 0.319885, tar: 0.015913 \n",
      "It 173151\n",
      "l0: 0.002315, l1: 0.002314, l2: 0.002630, l3: 0.002773, l4: 0.003308, l5: 0.005062, l6: 0.008454\n",
      "\n",
      "[epoch: 21644/100000, batch:    32/   30, ite: 173152] train loss: 0.317958, tar: 0.015823 \n",
      "It 173152\n",
      "l0: 0.008537, l1: 0.008536, l2: 0.009474, l3: 0.010188, l4: 0.013163, l5: 0.023213, l6: 0.121949\n",
      "\n",
      "[epoch: 21645/100000, batch:     4/   30, ite: 173153] train loss: 0.317154, tar: 0.015776 \n",
      "It 173153\n",
      "l0: 0.012320, l1: 0.012323, l2: 0.013388, l3: 0.013994, l4: 0.016024, l5: 0.042626, l6: 0.225030\n",
      "\n",
      "[epoch: 21645/100000, batch:     8/   30, ite: 173154] train loss: 0.317275, tar: 0.015753 \n",
      "It 173154\n",
      "l0: 0.008577, l1: 0.008574, l2: 0.009379, l3: 0.009966, l4: 0.013085, l5: 0.036183, l6: 0.143473\n",
      "\n",
      "[epoch: 21645/100000, batch:    12/   30, ite: 173155] train loss: 0.316707, tar: 0.015707 \n",
      "It 173155\n",
      "l0: 0.011274, l1: 0.011272, l2: 0.012029, l3: 0.012857, l4: 0.017337, l5: 0.074758, l6: 0.214089\n",
      "\n",
      "[epoch: 21645/100000, batch:    16/   30, ite: 173156] train loss: 0.316943, tar: 0.015679 \n",
      "It 173156\n",
      "l0: 0.028959, l1: 0.028868, l2: 0.029823, l3: 0.031393, l4: 0.034038, l5: 0.062290, l6: 0.178326\n",
      "\n",
      "[epoch: 21645/100000, batch:    20/   30, ite: 173157] train loss: 0.317432, tar: 0.015763 \n",
      "It 173157\n",
      "l0: 0.033331, l1: 0.033427, l2: 0.034531, l3: 0.036791, l4: 0.043219, l5: 0.068208, l6: 0.179738\n",
      "\n",
      "[epoch: 21645/100000, batch:    24/   30, ite: 173158] train loss: 0.318140, tar: 0.015874 \n",
      "It 173158\n",
      "l0: 0.009932, l1: 0.009931, l2: 0.010915, l3: 0.011618, l4: 0.014676, l5: 0.040218, l6: 0.156106\n",
      "\n",
      "[epoch: 21645/100000, batch:    28/   30, ite: 173159] train loss: 0.317733, tar: 0.015837 \n",
      "It 173159\n",
      "l0: 0.009782, l1: 0.009785, l2: 0.010512, l3: 0.011397, l4: 0.015539, l5: 0.091608, l6: 0.164175\n",
      "\n",
      "[epoch: 21645/100000, batch:    32/   30, ite: 173160] train loss: 0.317702, tar: 0.015799 \n",
      "It 173160\n",
      "l0: 0.010968, l1: 0.010968, l2: 0.012020, l3: 0.012774, l4: 0.015786, l5: 0.073680, l6: 0.174259\n",
      "\n",
      "[epoch: 21646/100000, batch:     4/   30, ite: 173161] train loss: 0.317657, tar: 0.015769 \n",
      "It 173161\n",
      "l0: 0.033077, l1: 0.033148, l2: 0.034545, l3: 0.037216, l4: 0.043398, l5: 0.081227, l6: 0.249143\n",
      "\n",
      "[epoch: 21646/100000, batch:     8/   30, ite: 173162] train loss: 0.318855, tar: 0.015876 \n",
      "It 173162\n",
      "l0: 0.032494, l1: 0.032386, l2: 0.033747, l3: 0.035337, l4: 0.037843, l5: 0.080901, l6: 0.204826\n",
      "\n",
      "[epoch: 21646/100000, batch:    12/   30, ite: 173163] train loss: 0.319706, tar: 0.015978 \n",
      "It 173163\n",
      "l0: 0.008642, l1: 0.008641, l2: 0.009302, l3: 0.009830, l4: 0.011596, l5: 0.049640, l6: 0.149021\n",
      "\n",
      "[epoch: 21646/100000, batch:    16/   30, ite: 173164] train loss: 0.319260, tar: 0.015933 \n",
      "It 173164\n",
      "l0: 0.007694, l1: 0.007692, l2: 0.008532, l3: 0.008994, l4: 0.011425, l5: 0.029596, l6: 0.111634\n",
      "\n",
      "[epoch: 21646/100000, batch:    20/   30, ite: 173165] train loss: 0.318450, tar: 0.015883 \n",
      "It 173165\n",
      "l0: 0.009949, l1: 0.009948, l2: 0.010915, l3: 0.011988, l4: 0.016469, l5: 0.064744, l6: 0.170465\n",
      "\n",
      "[epoch: 21646/100000, batch:    24/   30, ite: 173166] train loss: 0.318306, tar: 0.015848 \n",
      "It 173166\n",
      "l0: 0.009737, l1: 0.009736, l2: 0.010581, l3: 0.011406, l4: 0.014337, l5: 0.030636, l6: 0.179805\n",
      "\n",
      "[epoch: 21646/100000, batch:    28/   30, ite: 173167] train loss: 0.317994, tar: 0.015811 \n",
      "It 173167\n",
      "l0: 0.005580, l1: 0.005580, l2: 0.006098, l3: 0.006421, l4: 0.008401, l5: 0.021616, l6: 0.087251\n",
      "\n",
      "[epoch: 21646/100000, batch:    32/   30, ite: 173168] train loss: 0.316940, tar: 0.015750 \n",
      "It 173168\n",
      "l0: 0.036509, l1: 0.036410, l2: 0.037791, l3: 0.038924, l4: 0.041264, l5: 0.081555, l6: 0.306676\n",
      "\n",
      "[epoch: 21647/100000, batch:     4/   30, ite: 173169] train loss: 0.318492, tar: 0.015873 \n",
      "It 173169\n",
      "l0: 0.005399, l1: 0.005402, l2: 0.006136, l3: 0.007222, l4: 0.010133, l5: 0.017174, l6: 0.068960\n",
      "\n",
      "[epoch: 21647/100000, batch:     8/   30, ite: 173170] train loss: 0.317326, tar: 0.015811 \n",
      "It 173170\n",
      "l0: 0.010732, l1: 0.010734, l2: 0.011882, l3: 0.012661, l4: 0.016246, l5: 0.053151, l6: 0.153990\n",
      "\n",
      "[epoch: 21647/100000, batch:    12/   30, ite: 173171] train loss: 0.317046, tar: 0.015782 \n",
      "It 173171\n",
      "l0: 0.013383, l1: 0.013389, l2: 0.014380, l3: 0.015254, l4: 0.019116, l5: 0.089400, l6: 0.247982\n",
      "\n",
      "[epoch: 21647/100000, batch:    16/   30, ite: 173172] train loss: 0.317603, tar: 0.015768 \n",
      "It 173172\n",
      "l0: 0.033077, l1: 0.033167, l2: 0.034412, l3: 0.037791, l4: 0.043597, l5: 0.081392, l6: 0.226800\n",
      "\n",
      "[epoch: 21647/100000, batch:    20/   30, ite: 173173] train loss: 0.318601, tar: 0.015868 \n",
      "It 173173\n",
      "l0: 0.010898, l1: 0.010899, l2: 0.011895, l3: 0.012797, l4: 0.016001, l5: 0.057816, l6: 0.187705\n",
      "\n",
      "[epoch: 21647/100000, batch:    24/   30, ite: 173174] train loss: 0.318540, tar: 0.015839 \n",
      "It 173174\n",
      "l0: 0.006733, l1: 0.006733, l2: 0.007385, l3: 0.007795, l4: 0.009615, l5: 0.022092, l6: 0.100538\n",
      "\n",
      "[epoch: 21647/100000, batch:    28/   30, ite: 173175] train loss: 0.317640, tar: 0.015787 \n",
      "It 173175\n",
      "l0: 0.003317, l1: 0.003318, l2: 0.003799, l3: 0.003978, l4: 0.004467, l5: 0.006457, l6: 0.016763\n",
      "\n",
      "[epoch: 21647/100000, batch:    32/   30, ite: 173176] train loss: 0.316074, tar: 0.015716 \n",
      "It 173176\n",
      "l0: 0.029729, l1: 0.029804, l2: 0.031111, l3: 0.032868, l4: 0.039066, l5: 0.103339, l6: 0.179929\n",
      "\n",
      "[epoch: 21648/100000, batch:     4/   30, ite: 173177] train loss: 0.316807, tar: 0.015795 \n",
      "It 173177\n",
      "l0: 0.010109, l1: 0.010105, l2: 0.011103, l3: 0.011886, l4: 0.014379, l5: 0.033352, l6: 0.171353\n",
      "\n",
      "[epoch: 21648/100000, batch:     8/   30, ite: 173178] train loss: 0.316501, tar: 0.015763 \n",
      "It 173178\n",
      "l0: 0.008295, l1: 0.008291, l2: 0.009031, l3: 0.009788, l4: 0.012401, l5: 0.037237, l6: 0.163395\n",
      "\n",
      "[epoch: 21648/100000, batch:    12/   30, ite: 173179] train loss: 0.316121, tar: 0.015722 \n",
      "It 173179\n",
      "l0: 0.007458, l1: 0.007455, l2: 0.008081, l3: 0.008528, l4: 0.010057, l5: 0.030817, l6: 0.140002\n",
      "\n",
      "[epoch: 21648/100000, batch:    16/   30, ite: 173180] train loss: 0.315544, tar: 0.015676 \n",
      "It 173180\n",
      "l0: 0.012035, l1: 0.012032, l2: 0.012979, l3: 0.013888, l4: 0.018334, l5: 0.078976, l6: 0.206026\n",
      "\n",
      "[epoch: 21648/100000, batch:    20/   30, ite: 173181] train loss: 0.315758, tar: 0.015656 \n",
      "It 173181\n",
      "l0: 0.034992, l1: 0.034838, l2: 0.036337, l3: 0.038817, l4: 0.045146, l5: 0.093846, l6: 0.183616\n",
      "\n",
      "[epoch: 21648/100000, batch:    24/   30, ite: 173182] train loss: 0.316593, tar: 0.015762 \n",
      "It 173182\n",
      "l0: 0.010451, l1: 0.010450, l2: 0.011456, l3: 0.012170, l4: 0.015485, l5: 0.040800, l6: 0.181949\n",
      "\n",
      "[epoch: 21648/100000, batch:    28/   30, ite: 173183] train loss: 0.316408, tar: 0.015733 \n",
      "It 173183\n",
      "l0: 0.011305, l1: 0.011315, l2: 0.012337, l3: 0.013258, l4: 0.016732, l5: 0.039333, l6: 0.168524\n",
      "\n",
      "[epoch: 21648/100000, batch:    32/   30, ite: 173184] train loss: 0.316171, tar: 0.015709 \n",
      "It 173184\n",
      "l0: 0.010654, l1: 0.010658, l2: 0.011545, l3: 0.012096, l4: 0.014148, l5: 0.034437, l6: 0.164615\n",
      "\n",
      "[epoch: 21649/100000, batch:     4/   30, ite: 173185] train loss: 0.315857, tar: 0.015682 \n",
      "It 173185\n",
      "l0: 0.006228, l1: 0.006233, l2: 0.006831, l3: 0.007136, l4: 0.008043, l5: 0.014677, l6: 0.083945\n",
      "\n",
      "[epoch: 21649/100000, batch:     8/   30, ite: 173186] train loss: 0.314875, tar: 0.015631 \n",
      "It 173186\n",
      "l0: 0.007549, l1: 0.007551, l2: 0.008432, l3: 0.009040, l4: 0.010972, l5: 0.025504, l6: 0.115235\n",
      "\n",
      "[epoch: 21649/100000, batch:    12/   30, ite: 173187] train loss: 0.314176, tar: 0.015588 \n",
      "It 173187\n",
      "l0: 0.010068, l1: 0.010069, l2: 0.011116, l3: 0.012002, l4: 0.016924, l5: 0.059161, l6: 0.148733\n",
      "\n",
      "[epoch: 21649/100000, batch:    16/   30, ite: 173188] train loss: 0.313931, tar: 0.015558 \n",
      "It 173188\n",
      "l0: 0.008553, l1: 0.008554, l2: 0.009247, l3: 0.009860, l4: 0.013520, l5: 0.047137, l6: 0.158211\n",
      "\n",
      "[epoch: 21649/100000, batch:    20/   30, ite: 173189] train loss: 0.313620, tar: 0.015521 \n",
      "It 173189\n",
      "l0: 0.033225, l1: 0.033123, l2: 0.034435, l3: 0.035513, l4: 0.038798, l5: 0.106775, l6: 0.258531\n",
      "\n",
      "[epoch: 21649/100000, batch:    24/   30, ite: 173190] train loss: 0.314813, tar: 0.015614 \n",
      "It 173190\n",
      "l0: 0.031833, l1: 0.031907, l2: 0.033353, l3: 0.035520, l4: 0.041622, l5: 0.087056, l6: 0.204522\n",
      "\n",
      "[epoch: 21649/100000, batch:    28/   30, ite: 173191] train loss: 0.315604, tar: 0.015699 \n",
      "It 173191\n",
      "l0: 0.016090, l1: 0.016083, l2: 0.017271, l3: 0.018354, l4: 0.022334, l5: 0.052529, l6: 0.318806\n",
      "\n",
      "[epoch: 21649/100000, batch:    32/   30, ite: 173192] train loss: 0.316363, tar: 0.015701 \n",
      "It 173192\n",
      "l0: 0.013073, l1: 0.013063, l2: 0.014040, l3: 0.014853, l4: 0.018204, l5: 0.076076, l6: 0.222210\n",
      "\n",
      "[epoch: 21650/100000, batch:     4/   30, ite: 173193] train loss: 0.316649, tar: 0.015688 \n",
      "It 173193\n",
      "l0: 0.008851, l1: 0.008847, l2: 0.009614, l3: 0.010225, l4: 0.013426, l5: 0.044437, l6: 0.149388\n",
      "\n",
      "[epoch: 21650/100000, batch:     8/   30, ite: 173194] train loss: 0.316279, tar: 0.015652 \n",
      "It 173194\n",
      "l0: 0.030507, l1: 0.030414, l2: 0.032037, l3: 0.033378, l4: 0.038755, l5: 0.081200, l6: 0.176557\n",
      "\n",
      "[epoch: 21650/100000, batch:    12/   30, ite: 173195] train loss: 0.316825, tar: 0.015729 \n",
      "It 173195\n",
      "l0: 0.007544, l1: 0.007542, l2: 0.008302, l3: 0.009423, l4: 0.012254, l5: 0.025942, l6: 0.119589\n",
      "\n",
      "[epoch: 21650/100000, batch:    16/   30, ite: 173196] train loss: 0.316181, tar: 0.015687 \n",
      "It 173196\n",
      "l0: 0.010553, l1: 0.010555, l2: 0.011381, l3: 0.012040, l4: 0.014133, l5: 0.044138, l6: 0.176551\n",
      "\n",
      "[epoch: 21650/100000, batch:    20/   30, ite: 173197] train loss: 0.315994, tar: 0.015661 \n",
      "It 173197\n",
      "l0: 0.010745, l1: 0.010751, l2: 0.011778, l3: 0.013014, l4: 0.018334, l5: 0.063443, l6: 0.158761\n",
      "\n",
      "[epoch: 21650/100000, batch:    24/   30, ite: 173198] train loss: 0.315847, tar: 0.015636 \n",
      "It 173198\n",
      "l0: 0.031001, l1: 0.031077, l2: 0.032616, l3: 0.035703, l4: 0.041295, l5: 0.076899, l6: 0.205276\n",
      "\n",
      "[epoch: 21650/100000, batch:    28/   30, ite: 173199] train loss: 0.316541, tar: 0.015713 \n",
      "It 173199\n",
      "l0: 0.007686, l1: 0.007688, l2: 0.008216, l3: 0.008627, l4: 0.009868, l5: 0.026364, l6: 0.143186\n",
      "\n",
      "[epoch: 21650/100000, batch:    32/   30, ite: 173200] train loss: 0.316016, tar: 0.015673 \n",
      "It 173200\n",
      "l0: 0.008942, l1: 0.008942, l2: 0.009741, l3: 0.010368, l4: 0.012324, l5: 0.025305, l6: 0.153607\n",
      "\n",
      "[epoch: 21651/100000, batch:     4/   30, ite: 173201] train loss: 0.315584, tar: 0.015639 \n",
      "It 173201\n",
      "l0: 0.011615, l1: 0.011615, l2: 0.012683, l3: 0.013527, l4: 0.017648, l5: 0.069511, l6: 0.195118\n",
      "\n",
      "[epoch: 21651/100000, batch:     8/   30, ite: 173202] train loss: 0.315664, tar: 0.015620 \n",
      "It 173202\n",
      "l0: 0.007077, l1: 0.007077, l2: 0.007863, l3: 0.008345, l4: 0.010323, l5: 0.026756, l6: 0.099692\n",
      "\n",
      "[epoch: 21651/100000, batch:    12/   30, ite: 173203] train loss: 0.314932, tar: 0.015577 \n",
      "It 173203\n",
      "l0: 0.006174, l1: 0.006173, l2: 0.006684, l3: 0.007110, l4: 0.008603, l5: 0.027164, l6: 0.110766\n",
      "\n",
      "[epoch: 21651/100000, batch:    16/   30, ite: 173204] train loss: 0.314235, tar: 0.015531 \n",
      "It 173204\n",
      "l0: 0.030995, l1: 0.031080, l2: 0.032270, l3: 0.034813, l4: 0.040076, l5: 0.069929, l6: 0.147854\n",
      "\n",
      "[epoch: 21651/100000, batch:    20/   30, ite: 173205] train loss: 0.314590, tar: 0.015607 \n",
      "It 173205\n",
      "l0: 0.035789, l1: 0.035705, l2: 0.037256, l3: 0.039464, l4: 0.046280, l5: 0.145345, l6: 0.301060\n",
      "\n",
      "[epoch: 21651/100000, batch:    24/   30, ite: 173206] train loss: 0.316174, tar: 0.015705 \n",
      "It 173206\n",
      "l0: 0.010346, l1: 0.010346, l2: 0.011131, l3: 0.011930, l4: 0.016460, l5: 0.048149, l6: 0.176699\n",
      "\n",
      "[epoch: 21651/100000, batch:    28/   30, ite: 173207] train loss: 0.316024, tar: 0.015679 \n",
      "It 173207\n",
      "l0: 0.012322, l1: 0.012319, l2: 0.013548, l3: 0.014582, l4: 0.017306, l5: 0.034410, l6: 0.196229\n",
      "\n",
      "[epoch: 21651/100000, batch:    32/   30, ite: 173208] train loss: 0.315950, tar: 0.015663 \n",
      "It 173208\n",
      "l0: 0.007448, l1: 0.007447, l2: 0.008166, l3: 0.008693, l4: 0.010494, l5: 0.022265, l6: 0.128212\n",
      "\n",
      "[epoch: 21652/100000, batch:     4/   30, ite: 173209] train loss: 0.315361, tar: 0.015623 \n",
      "It 173209\n",
      "l0: 0.009785, l1: 0.009785, l2: 0.010516, l3: 0.010978, l4: 0.013213, l5: 0.056036, l6: 0.190728\n",
      "\n",
      "[epoch: 21652/100000, batch:     8/   30, ite: 173210] train loss: 0.315293, tar: 0.015596 \n",
      "It 173210\n",
      "l0: 0.029334, l1: 0.029245, l2: 0.030595, l3: 0.032263, l4: 0.035390, l5: 0.057121, l6: 0.155722\n",
      "\n",
      "[epoch: 21652/100000, batch:    12/   30, ite: 173211] train loss: 0.315550, tar: 0.015661 \n",
      "It 173211\n",
      "l0: 0.008852, l1: 0.008853, l2: 0.009644, l3: 0.010068, l4: 0.013319, l5: 0.064662, l6: 0.121945\n",
      "\n",
      "[epoch: 21652/100000, batch:    16/   30, ite: 173212] train loss: 0.315181, tar: 0.015629 \n",
      "It 173212\n",
      "l0: 0.007073, l1: 0.007076, l2: 0.007832, l3: 0.008434, l4: 0.010137, l5: 0.022044, l6: 0.114763\n",
      "\n",
      "[epoch: 21652/100000, batch:    20/   30, ite: 173213] train loss: 0.314534, tar: 0.015588 \n",
      "It 173213\n",
      "l0: 0.039947, l1: 0.040042, l2: 0.041675, l3: 0.045283, l4: 0.057966, l5: 0.146792, l6: 0.346683\n",
      "\n",
      "[epoch: 21652/100000, batch:    24/   30, ite: 173214] train loss: 0.316421, tar: 0.015702 \n",
      "It 173214\n",
      "l0: 0.010535, l1: 0.010532, l2: 0.011419, l3: 0.012071, l4: 0.013855, l5: 0.040353, l6: 0.213244\n",
      "\n",
      "[epoch: 21652/100000, batch:    28/   30, ite: 173215] train loss: 0.316401, tar: 0.015678 \n",
      "It 173215\n",
      "l0: 0.010551, l1: 0.010549, l2: 0.011750, l3: 0.012545, l4: 0.015597, l5: 0.030619, l6: 0.123928\n",
      "\n",
      "[epoch: 21652/100000, batch:    32/   30, ite: 173216] train loss: 0.315934, tar: 0.015655 \n",
      "It 173216\n",
      "l0: 0.004959, l1: 0.004958, l2: 0.005586, l3: 0.005867, l4: 0.006921, l5: 0.012285, l6: 0.035772\n",
      "\n",
      "[epoch: 21653/100000, batch:     4/   30, ite: 173217] train loss: 0.314830, tar: 0.015605 \n",
      "It 173217\n",
      "l0: 0.008028, l1: 0.008025, l2: 0.008855, l3: 0.009535, l4: 0.013075, l5: 0.040367, l6: 0.119628\n",
      "\n",
      "[epoch: 21653/100000, batch:     8/   30, ite: 173218] train loss: 0.314338, tar: 0.015570 \n",
      "It 173218\n",
      "l0: 0.029494, l1: 0.029574, l2: 0.030517, l3: 0.033201, l4: 0.038538, l5: 0.058380, l6: 0.155634\n",
      "\n",
      "[epoch: 21653/100000, batch:    12/   30, ite: 173219] train loss: 0.314616, tar: 0.015634 \n",
      "It 173219\n",
      "l0: 0.012624, l1: 0.012618, l2: 0.013440, l3: 0.014222, l4: 0.019448, l5: 0.051891, l6: 0.234626\n",
      "\n",
      "[epoch: 21653/100000, batch:    16/   30, ite: 173220] train loss: 0.314817, tar: 0.015620 \n",
      "It 173220\n",
      "l0: 0.006033, l1: 0.006033, l2: 0.006636, l3: 0.007181, l4: 0.009369, l5: 0.018535, l6: 0.112444\n",
      "\n",
      "[epoch: 21653/100000, batch:    20/   30, ite: 173221] train loss: 0.314145, tar: 0.015577 \n",
      "It 173221\n",
      "l0: 0.031032, l1: 0.030924, l2: 0.032193, l3: 0.033533, l4: 0.035266, l5: 0.106161, l6: 0.235024\n",
      "\n",
      "[epoch: 21653/100000, batch:    24/   30, ite: 173222] train loss: 0.315001, tar: 0.015647 \n",
      "It 173222\n",
      "l0: 0.013972, l1: 0.013970, l2: 0.015233, l3: 0.016424, l4: 0.022181, l5: 0.062462, l6: 0.232896\n",
      "\n",
      "[epoch: 21653/100000, batch:    28/   30, ite: 173223] train loss: 0.315279, tar: 0.015639 \n",
      "It 173223\n",
      "l0: 0.013831, l1: 0.013827, l2: 0.015018, l3: 0.015836, l4: 0.018055, l5: 0.047021, l6: 0.251857\n",
      "\n",
      "[epoch: 21653/100000, batch:    32/   30, ite: 173224] train loss: 0.315548, tar: 0.015631 \n",
      "It 173224\n",
      "l0: 0.006095, l1: 0.006098, l2: 0.006786, l3: 0.007251, l4: 0.009427, l5: 0.017091, l6: 0.088754\n",
      "\n",
      "[epoch: 21654/100000, batch:     4/   30, ite: 173225] train loss: 0.314775, tar: 0.015589 \n",
      "It 173225\n",
      "l0: 0.030375, l1: 0.030458, l2: 0.031709, l3: 0.033691, l4: 0.039233, l5: 0.066702, l6: 0.222754\n",
      "\n",
      "[epoch: 21654/100000, batch:     8/   30, ite: 173226] train loss: 0.315395, tar: 0.015654 \n",
      "It 173226\n",
      "l0: 0.011146, l1: 0.011151, l2: 0.012107, l3: 0.012797, l4: 0.015438, l5: 0.053615, l6: 0.177266\n",
      "\n",
      "[epoch: 21654/100000, batch:    12/   30, ite: 173227] train loss: 0.315298, tar: 0.015634 \n",
      "It 173227\n",
      "l0: 0.014382, l1: 0.014385, l2: 0.015475, l3: 0.016511, l4: 0.022659, l5: 0.087378, l6: 0.253546\n",
      "\n",
      "[epoch: 21654/100000, batch:    16/   30, ite: 173228] train loss: 0.315776, tar: 0.015629 \n",
      "It 173228\n",
      "l0: 0.009088, l1: 0.009088, l2: 0.009971, l3: 0.010513, l4: 0.013465, l5: 0.062771, l6: 0.122645\n",
      "\n",
      "[epoch: 21654/100000, batch:    20/   30, ite: 173229] train loss: 0.315435, tar: 0.015600 \n",
      "It 173229\n",
      "l0: 0.008002, l1: 0.008001, l2: 0.008650, l3: 0.009068, l4: 0.010923, l5: 0.032579, l6: 0.149399\n",
      "\n",
      "[epoch: 21654/100000, batch:    24/   30, ite: 173230] train loss: 0.315049, tar: 0.015567 \n",
      "It 173230\n",
      "l0: 0.009744, l1: 0.009741, l2: 0.010548, l3: 0.011374, l4: 0.014105, l5: 0.046063, l6: 0.171805\n",
      "\n",
      "[epoch: 21654/100000, batch:    28/   30, ite: 173231] train loss: 0.314868, tar: 0.015542 \n",
      "It 173231\n",
      "l0: 0.058271, l1: 0.058010, l2: 0.060693, l3: 0.064226, l4: 0.070293, l5: 0.096855, l6: 0.209966\n",
      "\n",
      "[epoch: 21654/100000, batch:    32/   30, ite: 173232] train loss: 0.316176, tar: 0.015726 \n",
      "It 173232\n",
      "l0: 0.009362, l1: 0.009361, l2: 0.010126, l3: 0.010917, l4: 0.013631, l5: 0.037189, l6: 0.176827\n",
      "\n",
      "[epoch: 21655/100000, batch:     4/   30, ite: 173233] train loss: 0.315967, tar: 0.015699 \n",
      "It 173233\n",
      "l0: 0.010423, l1: 0.010425, l2: 0.011545, l3: 0.012453, l4: 0.015668, l5: 0.028544, l6: 0.140123\n",
      "\n",
      "[epoch: 21655/100000, batch:     8/   30, ite: 173234] train loss: 0.315596, tar: 0.015676 \n",
      "It 173234\n",
      "l0: 0.005526, l1: 0.005527, l2: 0.006274, l3: 0.006711, l4: 0.008204, l5: 0.012439, l6: 0.037429\n",
      "\n",
      "[epoch: 21655/100000, batch:    12/   30, ite: 173235] train loss: 0.314603, tar: 0.015633 \n",
      "It 173235\n",
      "l0: 0.034707, l1: 0.034613, l2: 0.036054, l3: 0.037409, l4: 0.042785, l5: 0.128150, l6: 0.247854\n",
      "\n",
      "[epoch: 21655/100000, batch:    16/   30, ite: 173236] train loss: 0.315649, tar: 0.015714 \n",
      "It 173236\n",
      "l0: 0.012574, l1: 0.012577, l2: 0.013478, l3: 0.014383, l4: 0.019760, l5: 0.074709, l6: 0.231689\n",
      "\n",
      "[epoch: 21655/100000, batch:    20/   30, ite: 173237] train loss: 0.315917, tar: 0.015701 \n",
      "It 173237\n",
      "l0: 0.010868, l1: 0.010871, l2: 0.011798, l3: 0.012600, l4: 0.017958, l5: 0.043428, l6: 0.179439\n",
      "\n",
      "[epoch: 21655/100000, batch:    24/   30, ite: 173238] train loss: 0.315795, tar: 0.015680 \n",
      "It 173238\n",
      "l0: 0.031234, l1: 0.031325, l2: 0.032605, l3: 0.035650, l4: 0.040846, l5: 0.073318, l6: 0.209281\n",
      "\n",
      "[epoch: 21655/100000, batch:    28/   30, ite: 173239] train loss: 0.316375, tar: 0.015745 \n",
      "It 173239\n",
      "l0: 0.007203, l1: 0.007202, l2: 0.007755, l3: 0.008084, l4: 0.008826, l5: 0.038267, l6: 0.120430\n",
      "\n",
      "[epoch: 21655/100000, batch:    32/   30, ite: 173240] train loss: 0.315881, tar: 0.015710 \n",
      "It 173240\n",
      "l0: 0.013668, l1: 0.013669, l2: 0.014746, l3: 0.015711, l4: 0.021828, l5: 0.075113, l6: 0.256344\n",
      "\n",
      "[epoch: 21656/100000, batch:     4/   30, ite: 173241] train loss: 0.316276, tar: 0.015701 \n",
      "It 173241\n",
      "l0: 0.033931, l1: 0.033851, l2: 0.035168, l3: 0.036668, l4: 0.041219, l5: 0.082721, l6: 0.243074\n",
      "\n",
      "[epoch: 21656/100000, batch:     8/   30, ite: 173242] train loss: 0.317062, tar: 0.015777 \n",
      "It 173242\n",
      "l0: 0.007701, l1: 0.007698, l2: 0.008540, l3: 0.009253, l4: 0.011224, l5: 0.022339, l6: 0.103251\n",
      "\n",
      "[epoch: 21656/100000, batch:    12/   30, ite: 173243] train loss: 0.316457, tar: 0.015743 \n",
      "It 173243\n",
      "l0: 0.031807, l1: 0.031881, l2: 0.033125, l3: 0.036068, l4: 0.041909, l5: 0.107482, l6: 0.191564\n",
      "\n",
      "[epoch: 21656/100000, batch:    16/   30, ite: 173244] train loss: 0.317102, tar: 0.015809 \n",
      "It 173244\n",
      "l0: 0.009691, l1: 0.009688, l2: 0.010479, l3: 0.011150, l4: 0.015127, l5: 0.031206, l6: 0.163306\n",
      "\n",
      "[epoch: 21656/100000, batch:    20/   30, ite: 173245] train loss: 0.316831, tar: 0.015784 \n",
      "It 173245\n",
      "l0: 0.010042, l1: 0.010036, l2: 0.010766, l3: 0.011385, l4: 0.014199, l5: 0.063122, l6: 0.170739\n",
      "\n",
      "[epoch: 21656/100000, batch:    24/   30, ite: 173246] train loss: 0.316723, tar: 0.015761 \n",
      "It 173246\n",
      "l0: 0.010023, l1: 0.010021, l2: 0.010936, l3: 0.011788, l4: 0.014229, l5: 0.029897, l6: 0.152500\n",
      "\n",
      "[epoch: 21656/100000, batch:    28/   30, ite: 173247] train loss: 0.316410, tar: 0.015738 \n",
      "It 173247\n",
      "l0: 0.004420, l1: 0.004420, l2: 0.004917, l3: 0.005228, l4: 0.006355, l5: 0.011451, l6: 0.060030\n",
      "\n",
      "[epoch: 21656/100000, batch:    32/   30, ite: 173248] train loss: 0.315524, tar: 0.015692 \n",
      "It 173248\n",
      "l0: 0.009352, l1: 0.009351, l2: 0.010043, l3: 0.010635, l4: 0.014405, l5: 0.048843, l6: 0.150920\n",
      "\n",
      "[epoch: 21657/100000, batch:     4/   30, ite: 173249] train loss: 0.315275, tar: 0.015667 \n",
      "It 173249\n",
      "l0: 0.011932, l1: 0.011926, l2: 0.012917, l3: 0.013765, l4: 0.018539, l5: 0.075332, l6: 0.207900\n",
      "\n",
      "[epoch: 21657/100000, batch:     8/   30, ite: 173250] train loss: 0.315424, tar: 0.015652 \n",
      "It 173250\n",
      "l0: 0.012739, l1: 0.012729, l2: 0.013794, l3: 0.014663, l4: 0.018361, l5: 0.075808, l6: 0.235516\n",
      "\n",
      "[epoch: 21657/100000, batch:    12/   30, ite: 173251] train loss: 0.315695, tar: 0.015640 \n",
      "It 173251\n",
      "l0: 0.008483, l1: 0.008483, l2: 0.009577, l3: 0.010367, l4: 0.013069, l5: 0.028109, l6: 0.107312\n",
      "\n",
      "[epoch: 21657/100000, batch:    16/   30, ite: 173252] train loss: 0.315178, tar: 0.015612 \n",
      "It 173252\n",
      "l0: 0.039420, l1: 0.039517, l2: 0.041150, l3: 0.045018, l4: 0.051530, l5: 0.093959, l6: 0.289099\n",
      "\n",
      "[epoch: 21657/100000, batch:    20/   30, ite: 173253] train loss: 0.316303, tar: 0.015706 \n",
      "It 173253\n",
      "l0: 0.010856, l1: 0.010857, l2: 0.011840, l3: 0.012748, l4: 0.016865, l5: 0.045703, l6: 0.189902\n",
      "\n",
      "[epoch: 21657/100000, batch:    24/   30, ite: 173254] train loss: 0.316234, tar: 0.015687 \n",
      "It 173254\n",
      "l0: 0.003535, l1: 0.003535, l2: 0.003960, l3: 0.004210, l4: 0.004820, l5: 0.008447, l6: 0.037537\n",
      "\n",
      "[epoch: 21657/100000, batch:    28/   30, ite: 173255] train loss: 0.315253, tar: 0.015639 \n",
      "It 173255\n",
      "l0: 0.056406, l1: 0.056219, l2: 0.058110, l3: 0.060819, l4: 0.064110, l5: 0.115721, l6: 0.229982\n",
      "\n",
      "[epoch: 21657/100000, batch:    32/   30, ite: 173256] train loss: 0.316527, tar: 0.015798 \n",
      "It 173256\n",
      "l0: 0.010177, l1: 0.010174, l2: 0.011094, l3: 0.011819, l4: 0.013560, l5: 0.030527, l6: 0.158131\n",
      "\n",
      "[epoch: 21658/100000, batch:     4/   30, ite: 173257] train loss: 0.316250, tar: 0.015776 \n",
      "It 173257\n",
      "l0: 0.025885, l1: 0.025824, l2: 0.026750, l3: 0.028057, l4: 0.030960, l5: 0.047958, l6: 0.117005\n",
      "\n",
      "[epoch: 21658/100000, batch:     8/   30, ite: 173258] train loss: 0.316197, tar: 0.015816 \n",
      "It 173258\n",
      "l0: 0.006619, l1: 0.006618, l2: 0.007203, l3: 0.007715, l4: 0.009939, l5: 0.023723, l6: 0.118029\n",
      "\n",
      "[epoch: 21658/100000, batch:    12/   30, ite: 173259] train loss: 0.315670, tar: 0.015780 \n",
      "It 173259\n",
      "l0: 0.009092, l1: 0.009089, l2: 0.009996, l3: 0.010916, l4: 0.013920, l5: 0.035832, l6: 0.145567\n",
      "\n",
      "[epoch: 21658/100000, batch:    16/   30, ite: 173260] train loss: 0.315358, tar: 0.015754 \n",
      "It 173260\n",
      "l0: 0.012608, l1: 0.012608, l2: 0.013907, l3: 0.014828, l4: 0.018396, l5: 0.064387, l6: 0.196964\n",
      "\n",
      "[epoch: 21658/100000, batch:    20/   30, ite: 173261] train loss: 0.315428, tar: 0.015742 \n",
      "It 173261\n",
      "l0: 0.036841, l1: 0.036892, l2: 0.038788, l3: 0.042699, l4: 0.050381, l5: 0.098448, l6: 0.257807\n",
      "\n",
      "[epoch: 21658/100000, batch:    24/   30, ite: 173262] train loss: 0.316368, tar: 0.015823 \n",
      "It 173262\n",
      "l0: 0.007163, l1: 0.007163, l2: 0.007780, l3: 0.008305, l4: 0.010533, l5: 0.034769, l6: 0.113160\n",
      "\n",
      "[epoch: 21658/100000, batch:    28/   30, ite: 173263] train loss: 0.315884, tar: 0.015790 \n",
      "It 173263\n",
      "l0: 0.017296, l1: 0.017296, l2: 0.018359, l3: 0.019850, l4: 0.025273, l5: 0.109953, l6: 0.307128\n",
      "\n",
      "[epoch: 21658/100000, batch:    32/   30, ite: 173264] train loss: 0.316638, tar: 0.015796 \n",
      "It 173264\n",
      "l0: 0.009028, l1: 0.009029, l2: 0.009704, l3: 0.010295, l4: 0.012289, l5: 0.046449, l6: 0.152869\n",
      "\n",
      "[epoch: 21659/100000, batch:     4/   30, ite: 173265] train loss: 0.316386, tar: 0.015770 \n",
      "It 173265\n",
      "l0: 0.013559, l1: 0.013559, l2: 0.014623, l3: 0.015482, l4: 0.018767, l5: 0.049750, l6: 0.250595\n",
      "\n",
      "[epoch: 21659/100000, batch:     8/   30, ite: 173266] train loss: 0.316611, tar: 0.015762 \n",
      "It 173266\n",
      "l0: 0.007234, l1: 0.007234, l2: 0.008058, l3: 0.008614, l4: 0.010704, l5: 0.026029, l6: 0.111415\n",
      "\n",
      "[epoch: 21659/100000, batch:    12/   30, ite: 173267] train loss: 0.316097, tar: 0.015730 \n",
      "It 173267\n",
      "l0: 0.011577, l1: 0.011576, l2: 0.012500, l3: 0.013471, l4: 0.019274, l5: 0.077458, l6: 0.208305\n",
      "\n",
      "[epoch: 21659/100000, batch:    16/   30, ite: 173268] train loss: 0.316239, tar: 0.015714 \n",
      "It 173268\n",
      "l0: 0.005057, l1: 0.005057, l2: 0.005634, l3: 0.005965, l4: 0.007333, l5: 0.011548, l6: 0.064567\n",
      "\n",
      "[epoch: 21659/100000, batch:    20/   30, ite: 173269] train loss: 0.315454, tar: 0.015675 \n",
      "It 173269\n",
      "l0: 0.034911, l1: 0.034974, l2: 0.036623, l3: 0.040195, l4: 0.044275, l5: 0.076634, l6: 0.236775\n",
      "\n",
      "[epoch: 21659/100000, batch:    24/   30, ite: 173270] train loss: 0.316154, tar: 0.015746 \n",
      "It 173270\n",
      "l0: 0.009912, l1: 0.009912, l2: 0.010892, l3: 0.011574, l4: 0.015209, l5: 0.040606, l6: 0.171454\n",
      "\n",
      "[epoch: 21659/100000, batch:    28/   30, ite: 173271] train loss: 0.315982, tar: 0.015724 \n",
      "It 173271\n",
      "l0: 0.061254, l1: 0.061137, l2: 0.062248, l3: 0.065727, l4: 0.073457, l5: 0.124072, l6: 0.287594\n",
      "\n",
      "[epoch: 21659/100000, batch:    32/   30, ite: 173272] train loss: 0.317524, tar: 0.015892 \n",
      "It 173272\n",
      "l0: 0.007602, l1: 0.007603, l2: 0.008275, l3: 0.009135, l4: 0.013000, l5: 0.047803, l6: 0.105509\n",
      "\n",
      "[epoch: 21660/100000, batch:     4/   30, ite: 173273] train loss: 0.317090, tar: 0.015861 \n",
      "It 173273\n",
      "l0: 0.006829, l1: 0.006830, l2: 0.007638, l3: 0.008293, l4: 0.009977, l5: 0.018759, l6: 0.071154\n",
      "\n",
      "[epoch: 21660/100000, batch:     8/   30, ite: 173274] train loss: 0.316405, tar: 0.015828 \n",
      "It 173274\n",
      "l0: 0.049336, l1: 0.049333, l2: 0.050704, l3: 0.053705, l4: 0.058456, l5: 0.107838, l6: 0.232615\n",
      "\n",
      "[epoch: 21660/100000, batch:    12/   30, ite: 173275] train loss: 0.317444, tar: 0.015950 \n",
      "It 173275\n",
      "l0: 0.011392, l1: 0.011393, l2: 0.012390, l3: 0.013125, l4: 0.015712, l5: 0.041830, l6: 0.185435\n",
      "\n",
      "[epoch: 21660/100000, batch:    16/   30, ite: 173276] train loss: 0.317349, tar: 0.015934 \n",
      "It 173276\n",
      "l0: 0.013118, l1: 0.013121, l2: 0.014043, l3: 0.014963, l4: 0.021412, l5: 0.091935, l6: 0.241506\n",
      "\n",
      "[epoch: 21660/100000, batch:    20/   30, ite: 173277] train loss: 0.317684, tar: 0.015924 \n",
      "It 173277\n",
      "l0: 0.012515, l1: 0.012516, l2: 0.013285, l3: 0.014059, l4: 0.017789, l5: 0.059502, l6: 0.239439\n",
      "\n",
      "[epoch: 21660/100000, batch:    24/   30, ite: 173278] train loss: 0.317869, tar: 0.015911 \n",
      "It 173278\n",
      "l0: 0.007224, l1: 0.007225, l2: 0.007870, l3: 0.008407, l4: 0.010807, l5: 0.034324, l6: 0.117206\n",
      "\n",
      "[epoch: 21660/100000, batch:    28/   30, ite: 173279] train loss: 0.317421, tar: 0.015880 \n",
      "It 173279\n",
      "l0: 0.009473, l1: 0.009474, l2: 0.010495, l3: 0.011204, l4: 0.013638, l5: 0.022411, l6: 0.126351\n",
      "\n",
      "[epoch: 21660/100000, batch:    32/   30, ite: 173280] train loss: 0.317013, tar: 0.015857 \n",
      "It 173280\n",
      "l0: 0.006678, l1: 0.006675, l2: 0.007581, l3: 0.008147, l4: 0.010122, l5: 0.019255, l6: 0.077077\n",
      "\n",
      "[epoch: 21661/100000, batch:     4/   30, ite: 173281] train loss: 0.316367, tar: 0.015825 \n",
      "It 173281\n",
      "l0: 0.035123, l1: 0.035082, l2: 0.036025, l3: 0.036734, l4: 0.040274, l5: 0.084154, l6: 0.284821\n",
      "\n",
      "[epoch: 21661/100000, batch:     8/   30, ite: 173282] train loss: 0.317203, tar: 0.015893 \n",
      "It 173282\n",
      "l0: 0.011824, l1: 0.011820, l2: 0.012701, l3: 0.013379, l4: 0.016458, l5: 0.080220, l6: 0.214462\n",
      "\n",
      "[epoch: 21661/100000, batch:    12/   30, ite: 173283] train loss: 0.317358, tar: 0.015879 \n",
      "It 173283\n",
      "l0: 0.014140, l1: 0.014137, l2: 0.015035, l3: 0.015753, l4: 0.018791, l5: 0.058744, l6: 0.246581\n",
      "\n",
      "[epoch: 21661/100000, batch:    16/   30, ite: 173284] train loss: 0.317589, tar: 0.015873 \n",
      "It 173284\n",
      "l0: 0.026262, l1: 0.026281, l2: 0.027531, l3: 0.029775, l4: 0.033108, l5: 0.049668, l6: 0.094644\n",
      "\n",
      "[epoch: 21661/100000, batch:    20/   30, ite: 173285] train loss: 0.317483, tar: 0.015909 \n",
      "It 173285\n",
      "l0: 0.012853, l1: 0.012852, l2: 0.014031, l3: 0.015126, l4: 0.021340, l5: 0.075856, l6: 0.215401\n",
      "\n",
      "[epoch: 21661/100000, batch:    24/   30, ite: 173286] train loss: 0.317658, tar: 0.015898 \n",
      "It 173286\n",
      "l0: 0.005133, l1: 0.005133, l2: 0.005679, l3: 0.006023, l4: 0.007697, l5: 0.016047, l6: 0.066488\n",
      "\n",
      "[epoch: 21661/100000, batch:    28/   30, ite: 173287] train loss: 0.316942, tar: 0.015861 \n",
      "It 173287\n",
      "l0: 0.004996, l1: 0.004996, l2: 0.005444, l3: 0.005840, l4: 0.007172, l5: 0.021236, l6: 0.081094\n",
      "\n",
      "[epoch: 21661/100000, batch:    32/   30, ite: 173288] train loss: 0.316295, tar: 0.015823 \n",
      "It 173288\n",
      "l0: 0.033447, l1: 0.033449, l2: 0.035057, l3: 0.038319, l4: 0.042851, l5: 0.080230, l6: 0.237183\n",
      "\n",
      "[epoch: 21662/100000, batch:     4/   30, ite: 173289] train loss: 0.316933, tar: 0.015884 \n",
      "It 173289\n",
      "l0: 0.032913, l1: 0.032918, l2: 0.033705, l3: 0.034440, l4: 0.038199, l5: 0.062496, l6: 0.245393\n",
      "\n",
      "[epoch: 21662/100000, batch:     8/   30, ite: 173290] train loss: 0.317495, tar: 0.015943 \n",
      "It 173290\n",
      "l0: 0.009290, l1: 0.009290, l2: 0.009960, l3: 0.010573, l4: 0.013229, l5: 0.073117, l6: 0.170085\n",
      "\n",
      "[epoch: 21662/100000, batch:    12/   30, ite: 173291] train loss: 0.317420, tar: 0.015920 \n",
      "It 173291\n",
      "l0: 0.008617, l1: 0.008617, l2: 0.009418, l3: 0.010077, l4: 0.013607, l5: 0.030701, l6: 0.146489\n",
      "\n",
      "[epoch: 21662/100000, batch:    16/   30, ite: 173292] train loss: 0.317112, tar: 0.015895 \n",
      "It 173292\n",
      "l0: 0.007053, l1: 0.007054, l2: 0.007848, l3: 0.008507, l4: 0.010223, l5: 0.019096, l6: 0.097419\n",
      "\n",
      "[epoch: 21662/100000, batch:    20/   30, ite: 173293] train loss: 0.316566, tar: 0.015865 \n",
      "It 173293\n",
      "l0: 0.006650, l1: 0.006650, l2: 0.007364, l3: 0.007874, l4: 0.009600, l5: 0.028295, l6: 0.090043\n",
      "\n",
      "[epoch: 21662/100000, batch:    24/   30, ite: 173294] train loss: 0.316022, tar: 0.015833 \n",
      "It 173294\n",
      "l0: 0.013568, l1: 0.013567, l2: 0.014552, l3: 0.015555, l4: 0.021909, l5: 0.090243, l6: 0.251556\n",
      "\n",
      "[epoch: 21662/100000, batch:    28/   30, ite: 173295] train loss: 0.316378, tar: 0.015826 \n",
      "It 173295\n",
      "l0: 0.010239, l1: 0.010235, l2: 0.011569, l3: 0.012430, l4: 0.015725, l5: 0.033248, l6: 0.128878\n",
      "\n",
      "[epoch: 21662/100000, batch:    32/   30, ite: 173296] train loss: 0.316060, tar: 0.015807 \n",
      "It 173296\n",
      "l0: 0.008911, l1: 0.008911, l2: 0.009589, l3: 0.010132, l4: 0.012351, l5: 0.058159, l6: 0.146379\n",
      "\n",
      "[epoch: 21663/100000, batch:     4/   30, ite: 173297] train loss: 0.315852, tar: 0.015784 \n",
      "It 173297\n",
      "l0: 0.010703, l1: 0.010702, l2: 0.011842, l3: 0.012761, l4: 0.016509, l5: 0.043247, l6: 0.149022\n",
      "\n",
      "[epoch: 21663/100000, batch:     8/   30, ite: 173298] train loss: 0.315647, tar: 0.015767 \n",
      "It 173298\n",
      "l0: 0.034561, l1: 0.034537, l2: 0.035549, l3: 0.037220, l4: 0.042586, l5: 0.084668, l6: 0.220419\n",
      "\n",
      "[epoch: 21663/100000, batch:    12/   30, ite: 173299] train loss: 0.316229, tar: 0.015830 \n",
      "It 173299\n",
      "l0: 0.011194, l1: 0.011194, l2: 0.012039, l3: 0.012722, l4: 0.016248, l5: 0.045817, l6: 0.200136\n",
      "\n",
      "[epoch: 21663/100000, batch:    16/   30, ite: 173300] train loss: 0.316206, tar: 0.015814 \n",
      "It 173300\n",
      "l0: 0.009161, l1: 0.009158, l2: 0.010061, l3: 0.010721, l4: 0.013963, l5: 0.030577, l6: 0.139488\n",
      "\n",
      "[epoch: 21663/100000, batch:    20/   30, ite: 173301] train loss: 0.315897, tar: 0.015792 \n",
      "It 173301\n",
      "l0: 0.030533, l1: 0.030517, l2: 0.032024, l3: 0.035781, l4: 0.043908, l5: 0.093527, l6: 0.198248\n",
      "\n",
      "[epoch: 21663/100000, batch:    24/   30, ite: 173302] train loss: 0.316389, tar: 0.015841 \n",
      "It 173302\n",
      "l0: 0.009684, l1: 0.009685, l2: 0.010401, l3: 0.011131, l4: 0.013597, l5: 0.072406, l6: 0.184218\n",
      "\n",
      "[epoch: 21663/100000, batch:    28/   30, ite: 173303] train loss: 0.316372, tar: 0.015820 \n",
      "It 173303\n",
      "l0: 0.007335, l1: 0.007336, l2: 0.008108, l3: 0.008546, l4: 0.010327, l5: 0.019528, l6: 0.106472\n",
      "\n",
      "[epoch: 21663/100000, batch:    32/   30, ite: 173304] train loss: 0.315882, tar: 0.015793 \n",
      "It 173304\n",
      "l0: 0.034394, l1: 0.034384, l2: 0.036251, l3: 0.039655, l4: 0.047192, l5: 0.069492, l6: 0.195687\n",
      "\n",
      "[epoch: 21664/100000, batch:     4/   30, ite: 173305] train loss: 0.316345, tar: 0.015854 \n",
      "It 173305\n",
      "l0: 0.033997, l1: 0.034000, l2: 0.034900, l3: 0.036344, l4: 0.039345, l5: 0.075218, l6: 0.253567\n",
      "\n",
      "[epoch: 21664/100000, batch:     8/   30, ite: 173306] train loss: 0.316970, tar: 0.015913 \n",
      "It 173306\n",
      "l0: 0.008226, l1: 0.008225, l2: 0.008894, l3: 0.009455, l4: 0.012465, l5: 0.033112, l6: 0.145741\n",
      "\n",
      "[epoch: 21664/100000, batch:    12/   30, ite: 173307] train loss: 0.316674, tar: 0.015888 \n",
      "It 173307\n",
      "l0: 0.005437, l1: 0.005437, l2: 0.005989, l3: 0.006415, l4: 0.007996, l5: 0.014945, l6: 0.090380\n",
      "\n",
      "[epoch: 21664/100000, batch:    16/   30, ite: 173308] train loss: 0.316089, tar: 0.015854 \n",
      "It 173308\n",
      "l0: 0.009911, l1: 0.009908, l2: 0.010792, l3: 0.011527, l4: 0.013635, l5: 0.066658, l6: 0.179443\n",
      "\n",
      "[epoch: 21664/100000, batch:    20/   30, ite: 173309] train loss: 0.316043, tar: 0.015835 \n",
      "It 173309\n",
      "l0: 0.008238, l1: 0.008239, l2: 0.009036, l3: 0.009671, l4: 0.012334, l5: 0.031123, l6: 0.115969\n",
      "\n",
      "[epoch: 21664/100000, batch:    24/   30, ite: 173310] train loss: 0.315651, tar: 0.015810 \n",
      "It 173310\n",
      "l0: 0.011916, l1: 0.011913, l2: 0.012933, l3: 0.013683, l4: 0.017087, l5: 0.047371, l6: 0.209224\n",
      "\n",
      "[epoch: 21664/100000, batch:    28/   30, ite: 173311] train loss: 0.315678, tar: 0.015798 \n",
      "It 173311\n",
      "l0: 0.016595, l1: 0.016590, l2: 0.017659, l3: 0.019165, l4: 0.027417, l5: 0.128657, l6: 0.316316\n",
      "\n",
      "[epoch: 21664/100000, batch:    32/   30, ite: 173312] train loss: 0.316405, tar: 0.015800 \n",
      "It 173312\n",
      "l0: 0.030125, l1: 0.030149, l2: 0.031657, l3: 0.034877, l4: 0.042372, l5: 0.082905, l6: 0.201796\n",
      "\n",
      "[epoch: 21665/100000, batch:     4/   30, ite: 173313] train loss: 0.316844, tar: 0.015846 \n",
      "It 173313\n",
      "l0: 0.007061, l1: 0.007059, l2: 0.007593, l3: 0.007944, l4: 0.009877, l5: 0.034029, l6: 0.113408\n",
      "\n",
      "[epoch: 21665/100000, batch:     8/   30, ite: 173314] train loss: 0.316431, tar: 0.015818 \n",
      "It 173314\n",
      "l0: 0.034608, l1: 0.034560, l2: 0.035551, l3: 0.037347, l4: 0.045353, l5: 0.115657, l6: 0.246758\n",
      "\n",
      "[epoch: 21665/100000, batch:    12/   30, ite: 173315] train loss: 0.317172, tar: 0.015878 \n",
      "It 173315\n",
      "l0: 0.009225, l1: 0.009223, l2: 0.010083, l3: 0.010734, l4: 0.012753, l5: 0.033270, l6: 0.181177\n",
      "\n",
      "[epoch: 21665/100000, batch:    16/   30, ite: 173316] train loss: 0.317011, tar: 0.015857 \n",
      "It 173316\n",
      "l0: 0.008538, l1: 0.008537, l2: 0.009404, l3: 0.010104, l4: 0.012596, l5: 0.032893, l6: 0.157632\n",
      "\n",
      "[epoch: 21665/100000, batch:    20/   30, ite: 173317] train loss: 0.316767, tar: 0.015833 \n",
      "It 173317\n",
      "l0: 0.008287, l1: 0.008287, l2: 0.008997, l3: 0.009490, l4: 0.011099, l5: 0.022617, l6: 0.149243\n",
      "\n",
      "[epoch: 21665/100000, batch:    24/   30, ite: 173318] train loss: 0.316457, tar: 0.015810 \n",
      "It 173318\n",
      "l0: 0.009191, l1: 0.009191, l2: 0.010218, l3: 0.010949, l4: 0.013516, l5: 0.037599, l6: 0.123432\n",
      "\n",
      "[epoch: 21665/100000, batch:    28/   30, ite: 173319] train loss: 0.316136, tar: 0.015789 \n",
      "It 173319\n",
      "l0: 0.014037, l1: 0.014038, l2: 0.015101, l3: 0.016044, l4: 0.019895, l5: 0.047139, l6: 0.254839\n",
      "\n",
      "[epoch: 21665/100000, batch:    32/   30, ite: 173320] train loss: 0.316339, tar: 0.015783 \n",
      "It 173320\n",
      "l0: 0.010072, l1: 0.010073, l2: 0.011226, l3: 0.011983, l4: 0.014352, l5: 0.035268, l6: 0.154086\n",
      "\n",
      "[epoch: 21666/100000, batch:     4/   30, ite: 173321] train loss: 0.316123, tar: 0.015766 \n",
      "It 173321\n",
      "l0: 0.012414, l1: 0.012414, l2: 0.013312, l3: 0.014142, l4: 0.017600, l5: 0.056195, l6: 0.246870\n",
      "\n",
      "[epoch: 21666/100000, batch:     8/   30, ite: 173322] train loss: 0.316300, tar: 0.015755 \n",
      "It 173322\n",
      "l0: 0.012932, l1: 0.012932, l2: 0.013935, l3: 0.014923, l4: 0.021489, l5: 0.073200, l6: 0.221377\n",
      "\n",
      "[epoch: 21666/100000, batch:    12/   30, ite: 173323] train loss: 0.316468, tar: 0.015747 \n",
      "It 173323\n",
      "l0: 0.028441, l1: 0.028461, l2: 0.029854, l3: 0.032596, l4: 0.038050, l5: 0.060872, l6: 0.120197\n",
      "\n",
      "[epoch: 21666/100000, batch:    16/   30, ite: 173324] train loss: 0.316536, tar: 0.015786 \n",
      "It 173324\n",
      "l0: 0.010528, l1: 0.010528, l2: 0.011303, l3: 0.011967, l4: 0.014916, l5: 0.073183, l6: 0.186790\n",
      "\n",
      "[epoch: 21666/100000, batch:    20/   30, ite: 173325] train loss: 0.316544, tar: 0.015770 \n",
      "It 173325\n",
      "l0: 0.030785, l1: 0.030751, l2: 0.031696, l3: 0.033020, l4: 0.035489, l5: 0.057778, l6: 0.149225\n",
      "\n",
      "[epoch: 21666/100000, batch:    24/   30, ite: 173326] train loss: 0.316705, tar: 0.015816 \n",
      "It 173326\n",
      "l0: 0.007248, l1: 0.007247, l2: 0.007910, l3: 0.008468, l4: 0.010471, l5: 0.028193, l6: 0.104285\n",
      "\n",
      "[epoch: 21666/100000, batch:    28/   30, ite: 173327] train loss: 0.316268, tar: 0.015789 \n",
      "It 173327\n",
      "l0: 0.008804, l1: 0.008808, l2: 0.009336, l3: 0.009856, l4: 0.011843, l5: 0.032490, l6: 0.193675\n",
      "\n",
      "[epoch: 21666/100000, batch:    32/   30, ite: 173328] train loss: 0.316141, tar: 0.015768 \n",
      "It 173328\n",
      "l0: 0.012166, l1: 0.012165, l2: 0.013522, l3: 0.014451, l4: 0.017836, l5: 0.047675, l6: 0.175074\n",
      "\n",
      "[epoch: 21667/100000, batch:     4/   30, ite: 173329] train loss: 0.316071, tar: 0.015757 \n",
      "It 173329\n",
      "l0: 0.005048, l1: 0.005048, l2: 0.005559, l3: 0.005887, l4: 0.007081, l5: 0.014862, l6: 0.070477\n",
      "\n",
      "[epoch: 21667/100000, batch:     8/   30, ite: 173330] train loss: 0.315458, tar: 0.015725 \n",
      "It 173330\n",
      "l0: 0.013138, l1: 0.013138, l2: 0.014138, l3: 0.014929, l4: 0.018426, l5: 0.084922, l6: 0.219393\n",
      "\n",
      "[epoch: 21667/100000, batch:    12/   30, ite: 173331] train loss: 0.315647, tar: 0.015717 \n",
      "It 173331\n",
      "l0: 0.039426, l1: 0.039441, l2: 0.041300, l3: 0.044118, l4: 0.051328, l5: 0.115968, l6: 0.306094\n",
      "\n",
      "[epoch: 21667/100000, batch:    16/   30, ite: 173332] train loss: 0.316617, tar: 0.015788 \n",
      "It 173332\n",
      "l0: 0.035916, l1: 0.035903, l2: 0.036825, l3: 0.038428, l4: 0.043983, l5: 0.098344, l6: 0.269567\n",
      "\n",
      "[epoch: 21667/100000, batch:    20/   30, ite: 173333] train loss: 0.317345, tar: 0.015849 \n",
      "It 173333\n",
      "l0: 0.009944, l1: 0.009944, l2: 0.010741, l3: 0.011379, l4: 0.013832, l5: 0.037554, l6: 0.181052\n",
      "\n",
      "[epoch: 21667/100000, batch:    24/   30, ite: 173334] train loss: 0.317217, tar: 0.015831 \n",
      "It 173334\n",
      "l0: 0.003710, l1: 0.003710, l2: 0.004172, l3: 0.004372, l4: 0.005216, l5: 0.009539, l6: 0.035269\n",
      "\n",
      "[epoch: 21667/100000, batch:    28/   30, ite: 173335] train loss: 0.316467, tar: 0.015795 \n",
      "It 173335\n",
      "l0: 0.008288, l1: 0.008288, l2: 0.009275, l3: 0.009854, l4: 0.012344, l5: 0.027691, l6: 0.121993\n",
      "\n",
      "[epoch: 21667/100000, batch:    32/   30, ite: 173336] train loss: 0.316113, tar: 0.015773 \n",
      "It 173336\n",
      "l0: 0.028761, l1: 0.028760, l2: 0.029620, l3: 0.030839, l4: 0.033913, l5: 0.090518, l6: 0.183413\n",
      "\n",
      "[epoch: 21668/100000, batch:     4/   30, ite: 173337] train loss: 0.316439, tar: 0.015811 \n",
      "It 173337\n",
      "l0: 0.005896, l1: 0.005896, l2: 0.006535, l3: 0.006910, l4: 0.008139, l5: 0.022610, l6: 0.098868\n",
      "\n",
      "[epoch: 21668/100000, batch:     8/   30, ite: 173338] train loss: 0.315961, tar: 0.015782 \n",
      "It 173338\n",
      "l0: 0.034589, l1: 0.034606, l2: 0.036207, l3: 0.039476, l4: 0.045165, l5: 0.090385, l6: 0.249213\n",
      "\n",
      "[epoch: 21668/100000, batch:    12/   30, ite: 173339] train loss: 0.316591, tar: 0.015837 \n",
      "It 173339\n",
      "l0: 0.010301, l1: 0.010300, l2: 0.011023, l3: 0.011760, l4: 0.014327, l5: 0.044631, l6: 0.203131\n",
      "\n",
      "[epoch: 21668/100000, batch:    16/   30, ite: 173340] train loss: 0.316558, tar: 0.015821 \n",
      "It 173340\n",
      "l0: 0.010333, l1: 0.010332, l2: 0.011249, l3: 0.011952, l4: 0.015245, l5: 0.041422, l6: 0.192114\n",
      "\n",
      "[epoch: 21668/100000, batch:    20/   30, ite: 173341] train loss: 0.316488, tar: 0.015805 \n",
      "It 173341\n",
      "l0: 0.009297, l1: 0.009295, l2: 0.010142, l3: 0.010854, l4: 0.015792, l5: 0.049126, l6: 0.142406\n",
      "\n",
      "[epoch: 21668/100000, batch:    24/   30, ite: 173342] train loss: 0.316285, tar: 0.015786 \n",
      "It 173342\n",
      "l0: 0.010187, l1: 0.010187, l2: 0.011109, l3: 0.011825, l4: 0.014177, l5: 0.033781, l6: 0.164430\n",
      "\n",
      "[epoch: 21668/100000, batch:    28/   30, ite: 173343] train loss: 0.316108, tar: 0.015770 \n",
      "It 173343\n",
      "l0: 0.008345, l1: 0.008343, l2: 0.009030, l3: 0.009335, l4: 0.010424, l5: 0.017862, l6: 0.107172\n",
      "\n",
      "[epoch: 21668/100000, batch:    32/   30, ite: 173344] train loss: 0.315685, tar: 0.015748 \n",
      "It 173344\n",
      "l0: 0.005298, l1: 0.005299, l2: 0.005809, l3: 0.006055, l4: 0.007066, l5: 0.017306, l6: 0.075793\n",
      "\n",
      "[epoch: 21669/100000, batch:     4/   30, ite: 173345] train loss: 0.315125, tar: 0.015718 \n",
      "It 173345\n",
      "l0: 0.007116, l1: 0.007112, l2: 0.007890, l3: 0.008392, l4: 0.010799, l5: 0.021276, l6: 0.097391\n",
      "\n",
      "[epoch: 21669/100000, batch:     8/   30, ite: 173346] train loss: 0.314677, tar: 0.015693 \n",
      "It 173346\n",
      "l0: 0.041888, l1: 0.041818, l2: 0.043811, l3: 0.047878, l4: 0.055526, l5: 0.124342, l6: 0.268877\n",
      "\n",
      "[epoch: 21669/100000, batch:    12/   30, ite: 173347] train loss: 0.315569, tar: 0.015768 \n",
      "It 173347\n",
      "l0: 0.012829, l1: 0.012815, l2: 0.013789, l3: 0.014750, l4: 0.020112, l5: 0.086102, l6: 0.263118\n",
      "\n",
      "[epoch: 21669/100000, batch:    16/   30, ite: 173348] train loss: 0.315879, tar: 0.015760 \n",
      "It 173348\n",
      "l0: 0.034794, l1: 0.034782, l2: 0.035761, l3: 0.038133, l4: 0.044856, l5: 0.097353, l6: 0.263319\n",
      "\n",
      "[epoch: 21669/100000, batch:    20/   30, ite: 173349] train loss: 0.316547, tar: 0.015814 \n",
      "It 173349\n",
      "l0: 0.005065, l1: 0.005065, l2: 0.005630, l3: 0.005946, l4: 0.006788, l5: 0.012722, l6: 0.051914\n",
      "\n",
      "[epoch: 21669/100000, batch:    24/   30, ite: 173350] train loss: 0.315909, tar: 0.015784 \n",
      "It 173350\n",
      "l0: 0.011181, l1: 0.011180, l2: 0.012125, l3: 0.013061, l4: 0.017842, l5: 0.066001, l6: 0.186244\n",
      "\n",
      "[epoch: 21669/100000, batch:    28/   30, ite: 173351] train loss: 0.315914, tar: 0.015771 \n",
      "It 173351\n",
      "l0: 0.011117, l1: 0.011117, l2: 0.011977, l3: 0.012430, l4: 0.014648, l5: 0.031319, l6: 0.224377\n",
      "\n",
      "[epoch: 21669/100000, batch:    32/   30, ite: 173352] train loss: 0.315917, tar: 0.015757 \n",
      "It 173352\n",
      "l0: 0.010441, l1: 0.010440, l2: 0.011333, l3: 0.012366, l4: 0.016515, l5: 0.058571, l6: 0.188815\n",
      "\n",
      "[epoch: 21670/100000, batch:     4/   30, ite: 173353] train loss: 0.315895, tar: 0.015742 \n",
      "It 173353\n",
      "l0: 0.011743, l1: 0.011742, l2: 0.012814, l3: 0.013605, l4: 0.015900, l5: 0.039003, l6: 0.219691\n",
      "\n",
      "[epoch: 21670/100000, batch:     8/   30, ite: 173354] train loss: 0.315920, tar: 0.015731 \n",
      "It 173354\n",
      "l0: 0.008362, l1: 0.008359, l2: 0.009239, l3: 0.010055, l4: 0.013851, l5: 0.068406, l6: 0.125466\n",
      "\n",
      "[epoch: 21670/100000, batch:    12/   30, ite: 173355] train loss: 0.315716, tar: 0.015710 \n",
      "It 173355\n",
      "l0: 0.011706, l1: 0.011704, l2: 0.012550, l3: 0.013435, l4: 0.016600, l5: 0.050842, l6: 0.219080\n",
      "\n",
      "[epoch: 21670/100000, batch:    16/   30, ite: 173356] train loss: 0.315773, tar: 0.015699 \n",
      "It 173356\n",
      "l0: 0.032416, l1: 0.032437, l2: 0.033832, l3: 0.036141, l4: 0.039956, l5: 0.064562, l6: 0.176443\n",
      "\n",
      "[epoch: 21670/100000, batch:    20/   30, ite: 173357] train loss: 0.316053, tar: 0.015746 \n",
      "It 173357\n",
      "l0: 0.011871, l1: 0.011869, l2: 0.012895, l3: 0.013686, l4: 0.018586, l5: 0.045082, l6: 0.171981\n",
      "\n",
      "[epoch: 21670/100000, batch:    24/   30, ite: 173358] train loss: 0.315969, tar: 0.015735 \n",
      "It 173358\n",
      "l0: 0.032151, l1: 0.032106, l2: 0.032728, l3: 0.033916, l4: 0.039086, l5: 0.059608, l6: 0.162227\n",
      "\n",
      "[epoch: 21670/100000, batch:    28/   30, ite: 173359] train loss: 0.316181, tar: 0.015781 \n",
      "It 173359\n",
      "l0: 0.007471, l1: 0.007471, l2: 0.008263, l3: 0.009050, l4: 0.012840, l5: 0.045809, l6: 0.124759\n",
      "\n",
      "[epoch: 21670/100000, batch:    32/   30, ite: 173360] train loss: 0.315901, tar: 0.015758 \n",
      "It 173360\n",
      "l0: 0.009346, l1: 0.009347, l2: 0.010171, l3: 0.010872, l4: 0.015406, l5: 0.057972, l6: 0.151047\n",
      "\n",
      "[epoch: 21671/100000, batch:     4/   30, ite: 173361] train loss: 0.315758, tar: 0.015740 \n",
      "It 173361\n",
      "l0: 0.006680, l1: 0.006680, l2: 0.007537, l3: 0.008125, l4: 0.009988, l5: 0.017159, l6: 0.061311\n",
      "\n",
      "[epoch: 21671/100000, batch:     8/   30, ite: 173362] train loss: 0.315210, tar: 0.015715 \n",
      "It 173362\n",
      "l0: 0.011597, l1: 0.011597, l2: 0.012485, l3: 0.013224, l4: 0.016040, l5: 0.048603, l6: 0.237871\n",
      "\n",
      "[epoch: 21671/100000, batch:    12/   30, ite: 173363] train loss: 0.315310, tar: 0.015703 \n",
      "It 173363\n",
      "l0: 0.036537, l1: 0.036511, l2: 0.037351, l3: 0.038975, l4: 0.044382, l5: 0.092643, l6: 0.249565\n",
      "\n",
      "[epoch: 21671/100000, batch:    16/   30, ite: 173364] train loss: 0.315916, tar: 0.015761 \n",
      "It 173364\n",
      "l0: 0.013188, l1: 0.013191, l2: 0.014244, l3: 0.015069, l4: 0.018243, l5: 0.047040, l6: 0.231591\n",
      "\n",
      "[epoch: 21671/100000, batch:    20/   30, ite: 173365] train loss: 0.316017, tar: 0.015754 \n",
      "It 173365\n",
      "l0: 0.032387, l1: 0.032390, l2: 0.033750, l3: 0.036718, l4: 0.044422, l5: 0.073266, l6: 0.196649\n",
      "\n",
      "[epoch: 21671/100000, batch:    24/   30, ite: 173366] train loss: 0.316382, tar: 0.015799 \n",
      "It 173366\n",
      "l0: 0.009062, l1: 0.009054, l2: 0.009961, l3: 0.010537, l4: 0.014611, l5: 0.066231, l6: 0.145778\n",
      "\n",
      "[epoch: 21671/100000, batch:    28/   30, ite: 173367] train loss: 0.316242, tar: 0.015781 \n",
      "It 173367\n",
      "l0: 0.008794, l1: 0.008793, l2: 0.009464, l3: 0.009827, l4: 0.010867, l5: 0.051400, l6: 0.155498\n",
      "\n",
      "[epoch: 21671/100000, batch:    32/   30, ite: 173368] train loss: 0.316075, tar: 0.015762 \n",
      "It 173368\n",
      "l0: 0.011183, l1: 0.011181, l2: 0.012189, l3: 0.013036, l4: 0.015748, l5: 0.033384, l6: 0.207752\n",
      "\n",
      "[epoch: 21672/100000, batch:     4/   30, ite: 173369] train loss: 0.316043, tar: 0.015749 \n",
      "It 173369\n",
      "l0: 0.005393, l1: 0.005392, l2: 0.005854, l3: 0.006134, l4: 0.007337, l5: 0.026395, l6: 0.077919\n",
      "\n",
      "[epoch: 21672/100000, batch:     8/   30, ite: 173370] train loss: 0.315553, tar: 0.015721 \n",
      "It 173370\n",
      "l0: 0.012183, l1: 0.012181, l2: 0.013419, l3: 0.014388, l4: 0.018803, l5: 0.062813, l6: 0.203315\n",
      "\n",
      "[epoch: 21672/100000, batch:    12/   30, ite: 173371] train loss: 0.315611, tar: 0.015712 \n",
      "It 173371\n",
      "l0: 0.031886, l1: 0.031897, l2: 0.033316, l3: 0.036379, l4: 0.040843, l5: 0.090264, l6: 0.211445\n",
      "\n",
      "[epoch: 21672/100000, batch:    16/   30, ite: 173372] train loss: 0.316042, tar: 0.015755 \n",
      "It 173372\n",
      "l0: 0.036529, l1: 0.036504, l2: 0.037955, l3: 0.040289, l4: 0.045739, l5: 0.090515, l6: 0.299999\n",
      "\n",
      "[epoch: 21672/100000, batch:    20/   30, ite: 173373] train loss: 0.316770, tar: 0.015811 \n",
      "It 173373\n",
      "l0: 0.008406, l1: 0.008406, l2: 0.009131, l3: 0.009731, l4: 0.012451, l5: 0.039818, l6: 0.129892\n",
      "\n",
      "[epoch: 21672/100000, batch:    24/   30, ite: 173374] train loss: 0.316505, tar: 0.015791 \n",
      "It 173374\n",
      "l0: 0.008850, l1: 0.008851, l2: 0.009616, l3: 0.010317, l4: 0.013242, l5: 0.045834, l6: 0.137369\n",
      "\n",
      "[epoch: 21672/100000, batch:    28/   30, ite: 173375] train loss: 0.316285, tar: 0.015773 \n",
      "It 173375\n",
      "l0: 0.006434, l1: 0.006433, l2: 0.007030, l3: 0.007303, l4: 0.008231, l5: 0.017575, l6: 0.082734\n",
      "\n",
      "[epoch: 21672/100000, batch:    32/   30, ite: 173376] train loss: 0.315805, tar: 0.015748 \n",
      "It 173376\n",
      "l0: 0.036881, l1: 0.036893, l2: 0.038398, l3: 0.040494, l4: 0.048314, l5: 0.080182, l6: 0.275374\n",
      "\n",
      "[epoch: 21673/100000, batch:     4/   30, ite: 173377] train loss: 0.316444, tar: 0.015804 \n",
      "It 173377\n",
      "l0: 0.009276, l1: 0.009275, l2: 0.009963, l3: 0.010989, l4: 0.014468, l5: 0.083858, l6: 0.160227\n",
      "\n",
      "[epoch: 21673/100000, batch:     8/   30, ite: 173378] train loss: 0.316395, tar: 0.015787 \n",
      "It 173378\n",
      "l0: 0.009236, l1: 0.009235, l2: 0.010073, l3: 0.010499, l4: 0.012308, l5: 0.030611, l6: 0.145189\n",
      "\n",
      "[epoch: 21673/100000, batch:    12/   30, ite: 173379] train loss: 0.316160, tar: 0.015769 \n",
      "It 173379\n",
      "l0: 0.032455, l1: 0.032435, l2: 0.033649, l3: 0.036037, l4: 0.040700, l5: 0.079635, l6: 0.196767\n",
      "\n",
      "[epoch: 21673/100000, batch:    16/   30, ite: 173380] train loss: 0.316516, tar: 0.015813 \n",
      "It 173380\n",
      "l0: 0.006041, l1: 0.006040, l2: 0.006688, l3: 0.007156, l4: 0.009489, l5: 0.025477, l6: 0.082124\n",
      "\n",
      "[epoch: 21673/100000, batch:    20/   30, ite: 173381] train loss: 0.316061, tar: 0.015788 \n",
      "It 173381\n",
      "l0: 0.007839, l1: 0.007839, l2: 0.008742, l3: 0.009399, l4: 0.011225, l5: 0.020273, l6: 0.098410\n",
      "\n",
      "[epoch: 21673/100000, batch:    24/   30, ite: 173382] train loss: 0.315662, tar: 0.015767 \n",
      "It 173382\n",
      "l0: 0.012982, l1: 0.012982, l2: 0.014018, l3: 0.014819, l4: 0.019001, l5: 0.059729, l6: 0.246707\n",
      "\n",
      "[epoch: 21673/100000, batch:    28/   30, ite: 173383] train loss: 0.315831, tar: 0.015760 \n",
      "It 173383\n",
      "l0: 0.011230, l1: 0.011230, l2: 0.012370, l3: 0.013000, l4: 0.015994, l5: 0.050209, l6: 0.199869\n",
      "\n",
      "[epoch: 21673/100000, batch:    32/   30, ite: 173384] train loss: 0.315826, tar: 0.015748 \n",
      "It 173384\n",
      "l0: 0.011899, l1: 0.011897, l2: 0.012970, l3: 0.013896, l4: 0.019490, l5: 0.103034, l6: 0.196945\n",
      "\n",
      "[epoch: 21674/100000, batch:     4/   30, ite: 173385] train loss: 0.315967, tar: 0.015738 \n",
      "It 173385\n",
      "l0: 0.011771, l1: 0.011770, l2: 0.012790, l3: 0.013620, l4: 0.016977, l5: 0.058309, l6: 0.197846\n",
      "\n",
      "[epoch: 21674/100000, batch:     8/   30, ite: 173386] train loss: 0.315985, tar: 0.015727 \n",
      "It 173386\n",
      "l0: 0.035045, l1: 0.035027, l2: 0.036437, l3: 0.038990, l4: 0.045863, l5: 0.089801, l6: 0.255563\n",
      "\n",
      "[epoch: 21674/100000, batch:    12/   30, ite: 173387] train loss: 0.316556, tar: 0.015777 \n",
      "It 173387\n",
      "l0: 0.011951, l1: 0.011952, l2: 0.012833, l3: 0.013919, l4: 0.019606, l5: 0.038523, l6: 0.215988\n",
      "\n",
      "[epoch: 21674/100000, batch:    16/   30, ite: 173388] train loss: 0.316577, tar: 0.015768 \n",
      "It 173388\n",
      "l0: 0.008988, l1: 0.008988, l2: 0.009746, l3: 0.010721, l4: 0.013757, l5: 0.054537, l6: 0.132728\n",
      "\n",
      "[epoch: 21674/100000, batch:    20/   30, ite: 173389] train loss: 0.316379, tar: 0.015750 \n",
      "It 173389\n",
      "l0: 0.002829, l1: 0.002829, l2: 0.003325, l3: 0.003740, l4: 0.004517, l5: 0.006541, l6: 0.015223\n",
      "\n",
      "[epoch: 21674/100000, batch:    24/   30, ite: 173390] train loss: 0.315667, tar: 0.015717 \n",
      "It 173390\n",
      "l0: 0.035059, l1: 0.035081, l2: 0.036824, l3: 0.040528, l4: 0.046332, l5: 0.085335, l6: 0.259804\n",
      "\n",
      "[epoch: 21674/100000, batch:    28/   30, ite: 173391] train loss: 0.316238, tar: 0.015766 \n",
      "It 173391\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/U-2-Net/\n",
    "!python3 u2net_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmncsGSmCmiX"
   },
   "source": [
    "**Preprocess test samples and inferece the U2Net model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1uitrgxAkR68"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show(image, **kwargs):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(image, cmap=plt.cm.gray, **kwargs)\n",
    "    plt.gca().get_xaxis().set_ticks([])\n",
    "    plt.gca().get_yaxis().set_ticks([])\n",
    "\n",
    "def get_contour(mask,change_color=True, thresh_value=[10,255]):\n",
    "    \n",
    "    if  change_color:\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    (thresh, mask) = cv2.threshold(mask, thresh_value[0], thresh_value[1], 0)\n",
    "    try:\n",
    "        (_, contours, hierarchy) = cv2.findContours(image = mask, \n",
    "            mode = cv2.RETR_TREE,\n",
    "            method = cv2.CHAIN_APPROX_SIMPLE)\n",
    "    except:\n",
    "        (contours, hierarchy) = cv2.findContours(image = mask, \n",
    "            mode = cv2.RETR_TREE,\n",
    "            method = cv2.CHAIN_APPROX_SIMPLE)        \n",
    "\n",
    "    contours_sizes= [(cv2.contourArea(cnt), cnt) for cnt in contours]\n",
    "    biggest_contour = max(contours_sizes, key=lambda x: x[0])[1]\n",
    "    (x,y,w,h) = cv2.boundingRect(biggest_contour)\n",
    "    b_box = [x,y,x+w,y+h]\n",
    "    \n",
    "    max_distance = 0\n",
    "    farthest_points = []\n",
    "\n",
    "    for contour in contours:\n",
    "        for point1 in contour:\n",
    "            for point2 in contour:\n",
    "                distance = np.linalg.norm(point1 - point2)\n",
    "                if distance > max_distance:\n",
    "                    max_distance = distance\n",
    "                    farthest_points = [point1[0], point2[0]]\n",
    "\n",
    "    return biggest_contour,b_box, farthest_points\n",
    "\n",
    "\n",
    "\n",
    "def crop_marked_region(img, source_img):\n",
    "    org = img.copy()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret,thresh = cv2.threshold(gray,2,255,0)\n",
    "    not_img = cv2.bitwise_not(thresh)\n",
    "    contour, box, far_points = get_contour(not_img, change_color=False)\n",
    "    \n",
    "    dx = far_points[1][0] -  far_points[0][0]\n",
    "    dy = far_points[1][1] -  far_points[0][1]\n",
    "    radians = math.atan2(dy, dx)\n",
    "    angle = math.degrees(radians)\n",
    "\n",
    "    rotated = imutils.rotate(source_img, angle)    \n",
    "    center = (rotated.shape[1] // 2, rotated.shape[0] // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated_far_points = cv2.transform(np.array([far_points]), rotation_matrix)[0]\n",
    "\n",
    "    length_line = abs(rotated_far_points[0][0] - rotated_far_points[1][0])    \n",
    "    x1 =  rotated_far_points[0][0] -50\n",
    "    x2 = rotated_far_points[1][0]\n",
    "    y1 = rotated_far_points[0][1]- int( length_line/2) \n",
    "    y2 = rotated_far_points[1][1] -10\n",
    "    y1 = y2 - int(length_line*0.5)\n",
    "    cropped = rotated[y1:y2 ,x1:x2]\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p7obnKzFQnJd",
    "outputId": "4b09cff0-8de4-47c7-9482-971f3a5279d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/U-2-Net\n",
      "N1a Number of capilaries: 5\n",
      "N1b Number of capilaries: 7\n",
      "N1c Number of capilaries: 9\n",
      "N1d Number of capilaries: 8\n",
      "N1e Number of capilaries: 8\n",
      "N1f Number of capilaries: 8\n",
      "N1g Number of capilaries: 8\n",
      "N1h Number of capilaries: 7\n",
      "N1i Number of capilaries: 7\n",
      "N1j Number of capilaries: 8\n",
      "N2b Number of capilaries: 10\n",
      "N2c Number of capilaries: 9\n",
      "N2d Number of capilaries: 6\n",
      "S2a Number of capilaries: 1\n",
      "S2b Number of capilaries: 2\n",
      "S2c Number of capilaries: 2\n",
      "S2d Number of capilaries: 5\n",
      "S2e Number of capilaries: 3\n",
      "S2f Number of capilaries: 1\n",
      "S2g Number of capilaries: 4\n",
      "S3a Number of capilaries: 5\n",
      "S3b Number of capilaries: 4\n",
      "S3c Number of capilaries: 2\n",
      "S3d Number of capilaries: 4\n",
      "S3e Number of capilaries: 5\n",
      "S3f Number of capilaries: 4\n",
      "S3g Number of capilaries: 5\n",
      "S3h Number of capilaries: 4\n",
      "S3i Number of capilaries: 2\n",
      "S3j Number of capilaries: 5\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/U-2-Net/\n",
    "import os\n",
    "import imutils\n",
    "from skimage import io, transform\n",
    "import torch\n",
    "import torchvision\n",
    "import math \n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms#, utils\n",
    "# import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random \n",
    "\n",
    "from data_loader import RescaleT\n",
    "from data_loader import ToTensor\n",
    "from data_loader import ToTensorLab\n",
    "from data_loader import SalObjDataset\n",
    "\n",
    "from model import U2NET # full size version 173.6 MB\n",
    "from model import U2NETP # small version u2net 4.7 MB\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import cv2\n",
    "# normalize the predicted SOD probability map\n",
    "\n",
    "def get_random_rgb():\n",
    "    r = random.randrange(256)  # Random value for red (0-255)\n",
    "    g = random.randrange(256)  # Random value for green (0-255)\n",
    "    b = random.randrange(256)  # Random value for blue (0-255)\n",
    "    return (r, g, b)  # Return the RGB color tuple\n",
    "\n",
    "\n",
    "transform_=transforms.Compose([RescaleT(320), ToTensorLab(flag=0)])\n",
    "\n",
    "def image_loader(image):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    " \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    sample = {'imidx':np.array([0]), 'image':image, 'label':image}\n",
    "    shape= image.shape\n",
    "    sample = transform_(sample)   # Perform rescale and color  format conversion RGB to tensorlab\n",
    "    image = sample['image']\n",
    "    image  = torch.unsqueeze(image,0)\n",
    "\n",
    "    return image,[shape[1],shape[0],3] #assumes that you're using GPU\n",
    "\n",
    "\n",
    "def normPRED(d):\n",
    "    ma = torch.max(d)\n",
    "    mi = torch.min(d)\n",
    "\n",
    "    dn = (d-mi)/(ma-mi)\n",
    "\n",
    "    return dn\n",
    "\n",
    "\n",
    "def main(net, img_original):\n",
    "    original = img_original.copy()\n",
    "    imgx = img_original.copy()\n",
    "    inputs_test, im_size = image_loader(imgx)\n",
    "    bk = np.full(img_original.shape, 255, dtype=np.uint8)  # white bk, same size and type of image\n",
    "\n",
    "    inputs_test = inputs_test.type(torch.FloatTensor)\n",
    "    \n",
    "    inputs_test = Variable(inputs_test)\n",
    "\n",
    "    d1,d2,d3,d4,d5,d6,d7= net(inputs_test)\n",
    "\n",
    "    pred = d4[:,0,:,:]\n",
    "    pred = normPRED(pred)\n",
    "\n",
    "    predict = pred.squeeze()\n",
    "    predict_np = predict.cpu().data.numpy()\n",
    "\n",
    "    im = Image.fromarray(predict_np*255).convert('RGB')\n",
    "\n",
    "    img = im.resize((im_size[0],im_size[1]),resample=Image.BILINEAR)\n",
    "\n",
    "    img = np.array(img)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, mask = cv2.threshold(gray,1, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    return mask \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model_name='u2net'\n",
    "\n",
    "    model_dir = \"/content/drive/MyDrive/U-2-Net/trained_models/capillaries_segmentation.pth\"\n",
    "\n",
    "    net = U2NET(3,1)\n",
    "   \n",
    "    #net.load_state_dict(torch.load(model_dir))\n",
    "    device = torch.device('cpu')\n",
    "    net.load_state_dict(torch.load(model_dir,map_location=device))\n",
    "    #if torch.cuda.is_available():\n",
    "        #net.cuda()\n",
    "    net.eval()\n",
    "\n",
    "    output_folder =  \"/content/drive/MyDrive/U-2-Net/capilaeries_output/\"\n",
    "    if not os.path.exists(output_folder):\n",
    "      os.makedirs(output_folder)\n",
    "    img_folder = \"/content/drive/MyDrive/U-2-Net/test_images/\"\n",
    "    overlayed_images = []\n",
    "    masks = []\n",
    "    eroded_images = []\n",
    "    final_masks = []\n",
    "    for file in sorted(os.listdir(img_folder)):\n",
    "        if \"Natif\" not in file:\n",
    "          filepath = img_folder+file\n",
    "          filename, ext = os.path.splitext(file)\n",
    "          outfile = output_folder+filename+\"_mask\"+\".jpg\"          \n",
    "          outfile_eroded = output_folder+filename+\"_eroded_mask\"+\".jpg\"\n",
    "          outfile_preprocessed = output_folder+filename+\"_preprocessed\"+\".jpg\"\n",
    "          outfile_canvas = output_folder+filename+\"_canvas\"+\".jpg\"\n",
    "\n",
    "          img = cv2.imread(filepath)\n",
    "\n",
    "          source_img_file = filepath.replace(\".jpg\",\"_Natif.jpg\")\n",
    "\n",
    "          source_img = cv2.imread(source_img_file)\n",
    "          cropped_image = crop_marked_region(img.copy(), source_img)\n",
    "          outfile_cropped = output_folder+filename+\"_cropped\"+\".jpg\"\n",
    "          cv2.imwrite(outfile_cropped, cropped_image)\n",
    "\n",
    "          green_channel = cropped_image[:,:,1]\n",
    "          blurred = cv2.GaussianBlur(green_channel.copy(), (5, 5), 0)\n",
    "          filtered_median  = cv2.medianBlur(blurred, 3)\n",
    "          cv2.imwrite(outfile_preprocessed,filtered_median)\n",
    "\n",
    "          \n",
    "          canvas = cv2.cvtColor(filtered_median, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "\n",
    "          mask = main(net, filtered_median)\n",
    "          cv2.imwrite(outfile,mask)\n",
    "\n",
    "          masks.append(cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR))\n",
    "\n",
    "          kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "          eroded = cv2.erode(mask, kernel, iterations=2)\n",
    "\n",
    "          cv2.imwrite(outfile_eroded,eroded)\n",
    "\n",
    "          eroded_images.append(eroded)\n",
    "\n",
    "\n",
    "\n",
    "          contours, _ = cv2.findContours(eroded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "          contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "          c= 0\n",
    "\n",
    "          colors = [(0, 255, 0), (255, 0, 0),(0, 0, 255), [255, 255, 0], [255, 0, 255], [0, 255, 255]]  # Green color for the overlay\n",
    "          alpha = 1 # Overlay transparency (adjust as needed)\n",
    "\n",
    "          final_mask =  np.zeros_like(mask)\n",
    "\n",
    "\n",
    "          threshold_area = 100\n",
    "          for contour in contours:\n",
    "              area = cv2.contourArea(contour)\n",
    "              if area > threshold_area:\n",
    "                  overlay = canvas.copy()\n",
    "                  mask_ = np.zeros_like(mask)\n",
    "                  cv2.drawContours(mask_, [contour], 0, (255, 255, 255), -1)  # Red color is used here, but you can change it\n",
    "                  cv2.drawContours(final_mask, [contour], 0, (255, 255, 255), -1)  # Red color is used here, but you can change it\n",
    "\n",
    "                  overlay[mask_ > 0] = get_random_rgb()\n",
    "\n",
    "                  canvas = cv2.addWeighted(canvas, 1 - alpha, overlay, alpha, 0)\n",
    "\n",
    "\n",
    "                  c +=1\n",
    "\n",
    "          cv2.imwrite(outfile_canvas, canvas)\n",
    "          print(f'{filename} Number of capilaries: {c}')\n",
    "          overlayed_images.append(canvas)\n",
    "          final_masks.append(cv2.cvtColor(final_mask, cv2.COLOR_GRAY2BGR))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k74Q7el9Cp7p"
   },
   "source": [
    "**Visualize the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "id": "uVivq5hl0k--",
    "outputId": "b9527424-d7d8-4e88-dae6-96343589d382"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n",
    "\n",
    "images = [masks[1], masks[6], masks[24],\n",
    "          final_masks[1], final_masks[6], final_masks[24],\n",
    "          overlayed_images[1], overlayed_images[6], overlayed_images[24]]\n",
    "# Iterate over the subplot grid, display the images, and add labels\n",
    "row_labels = [\"Mask Generated by U-Net\",\n",
    "              \"Post processed masks\",\n",
    "              \"Final segmentation results\"]\n",
    "\n",
    "# Iterate over the subplot grid, display the images, and add row labels\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(images[i])\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Add row label only to the first image of each row\n",
    "    if i % 3 == 0:\n",
    "        ax.set_title(row_labels[i // 3], fontsize=12, fontweight='bold')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the grid of images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrpquU_d1HcS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
